\documentclass[12pt]{article}

\usepackage{ishn}

\makeindex[intoc]

\begin{document}

\hypersetup{pageanchor=false}
\begin{titlepage}
	\begin{center}
		\vspace*{1em}
		\Huge
		\textbf{IB Methods}

		\vspace{1em}
		\large
		Ishan Nath, Michaelmas 2022

		\vspace{1.5em}

		\Large

		Based on Lectures by Prof. Edward Shellard

		\vspace{1em}

		\large
		\today
	\end{center}
	
\end{titlepage}
\hypersetup{pageanchor=true}

\tableofcontents

\newpage

\part{Self-Adjoint ODE'S}%
\label{prt:self_adjoint_ode_s}

\section{Fourier Series}%
\label{sec:fourier_series}

\subsection{Periodic Functions}%
\label{sub:periodic_functions}

A function $f(x)$ is \textbf{periodic}\index{periodic function} if
\[
	f(x + T) = f(x)
,\]
where $T$ is the period.

\begin{exbox}
	Consider simple harmonic motion. We have
	\[
	y = A \sin \omega t
	,\]
	where $A$ is the amplitude and the period $T = 2 \pi / \omega$, with angular frequency $\omega$.
\end{exbox}

Consider the set of functions
\[
	g_n(x) = \cos \frac{n \pi x}{L}, \quad h_n(x) = \sin \frac{n \pi x}{L}
,\]
which are periodic on the interval $0 \leq x < 2L$. Recall the identities
\begin{align*}
	\cos A \cos B &= \frac{1}{2} \left(\cos(A - B) + \cos (A + B) \right), \\
	\sin A \sin B &= \frac{1}{2} \left( \cos(A - B) - \cos(A + B) \right), \\
	\sin A \cos B &=  \frac{1}{2} \left( \sin(A - B) + \sin(A + B) \right).
\end{align*}

Define the \textbf{inner product}\index{inner product of periodic functions} for two periodic functions $f, g$ on the interval $[0, 2L)$ 
\[
	\langle f, g \rangle = \int_{0}^{2L}f(x) g(x)\diff x
.\]
I claim that the functions $g_n, h_m$ are \textbf{mutually orthogonal}. Indeed,
\begin{align*}
	\langle h_n, h_m \rangle &= \int_{0}^{2L} \sin \frac{n \pi x}{L} \sin \frac{m \pi x}{L}\diff x \\
				 &= \frac{1}{2} \int_{0}^{2L} \left( \cos \frac{(n - m)\pi x}{L} - \cos \frac{(n + m)\pi x}{L}\right)\diff x \\
				 &= \frac{1}{2} \frac{L}{\pi} \left[ \frac{\sin (n - m) \pi x/L}{n - m} - \frac{\sin (n + m) \pi x/L}{n + m} \right]_{0}^{2L} = 0.
\end{align*}
This works for $n \neq m$. For $n = m$,
\begin{align*}
	\langle h_n, h_n \rangle &= \int_{0}^{2L} \sin^2 \frac{n \pi x}{L}\diff x \\
				 &= \frac{1}{2} \int_{0}^{2L} \left( 1 - \cos \frac{2 \pi n x}{L} \right)\diff x \\
				 &= L \;\; (n \neq 0).
\end{align*}
Hence, we can put these together to get
\[
	\langle h_n, h_m \rangle =
	\begin{cases}
		L \delta_{nm}, & \forall\,\! n, m \neq 0, \\
		0, & n = 0.
	\end{cases}
\] 
Similarly, we can show
\[
	\langle g_n, g_m \rangle =
	\begin{cases}
		L \delta_{nm}, & \forall\,\! n, m \neq 0, \\
		2L \delta_{0n}, &m = 0.
	\end{cases}
	\quad \text{ and } \; \langle h_n, g_m \rangle = 0
.\]

\subsection{Definition of Fourier series}%
\label{sub:definition_of_fourier_series}\index{Fourier series}

We can express any `well-behaved' periodic function $f(x)$ with period $2L$ as
\[
	f(x) = \frac{1}{2}a_0  + \sum_{n = 1}^{\infty} a_n \cos \frac{n \pi x}{L} + \sum_{n = 1}^{\infty}b_n \sin \frac{n \pi x}{L}
,\]

where $a_n, b_n$ are constant such that the right hand side is convergent for all $x$ where $f$ is continuous. At a discontinuity $x$, the Fourier series approaches the midpoint
\[
	\frac{1}{2} \left( f(x_{+}) + f(x_{-}) \right)
.\]
\subsubsection{Fourier Coefficients}%
\label{subsub:fourier_coefficients}\index{Fourier coefficients}

Consider the inner product
\[
	\langle h_m(x), f(x) \rangle = \int_{0}^{2L} \sin \frac{m \pi x}{L} f(x)\diff x = L b_m
,\]
by the orthogonality relations. Hence we find that
\begin{align*}
	b_n &= \frac{1}{L} \int_{0}^{2L}f(x) \sin \frac{n \pi x}{L}\diff x, \\
	a_n &= \frac{1}{L} \int_{0}^{2L}f(x) \cos \frac{n \pi x}{L}\diff x.
\end{align*}
\begin{remark}
	\begin{enumerate}[(i)]
		\item[]
		\item $a_n$ includes $n = 0$, since $\frac{1}{2} a_0$ is the \textbf{average}
			\[
				\langle f(x) \rangle = \frac{1}{2L} \int_{0}^{2L} f(x)\diff x
			.\]
		\item The range of integration is over one period, so we may take the integral over $[0, 2L)$ or $[-L, L)$.
		\item We can think of the Fourier series as a decomposition into harmonics. The simplest Fourier series are the sine and cosine functions.
	\end{enumerate}	
\end{remark}

\begin{exbox}[Sawtooth wave]
	Consider the function $f(x) = x$ for $-L \leq x < L$, periodic with period $T = 2L$. The cosine coefficients are
	\[
	a_n = \frac{1}{L} \int_{-L}^{L} x \cos \frac{n \pi x}{L}\diff x = 0
	,\]
	as $x \cos \omega x$ is odd. The sine coefficients are
	\begin{align*}
		b_n &= \frac{2}{L} \int_{0}^{L} x \sin \frac{n \pi x}{L}\diff x \\
		    &= -\frac{2}{n \pi} \left[ x \cos \frac{n \pi x}{L} \right]_{0}^{L} + \frac{2}{n \pi} \int_{0}^{L} \cos \frac{n \pi x}{L}\diff x \\
		    &= - \frac{2L}{n \pi} \cos n \pi + \frac{2L}{(n \pi)^2} \sin n \pi = \frac{2L}{n \pi }(-1)^{n+1}.
	\end{align*}
	So the sawtooth Fourier series is
	\begin{align*}
		f(x) &= \frac{2L}{\pi} \sum_{n = 1}^{\infty} \frac{(-1)^{n+1}}{n} \sin \frac{n \pi x}{L} \\
		     &= \frac{2L}{\pi} \left( \sin \frac{\pi x}{L} - \frac{1}{2} \sin \frac{2 \pi x}{L} + \frac{1}{3} \sin \frac{3 \pi x}{L} - \cdots \right).
	\end{align*}
\end{exbox}

With Fourier series, we can construct functions with only finitely many discontinuities, the topologist's sine curve, and the Weierstrass function.

\subsection{The Dirichlet Conditions (Fourier's theorem)}%
\label{sub:the_dirichlet_conditions_fourier_s_theorem_}\index{Dirichlet conditions}

These are sufficiency conditions for a ``well-behaved'' function to have a unique Fourier series:

\begin{proposition}
	If $f(x)$ is a bounded periodic function (period $2L$) with a finite number of minima, maxima and discontinuities in $0 \leq x < 2L$, then the Fourier series converges to  $f(x)$ at all points where $f$ is continuous; at discontinuities the series converges to the midpoint.
\end{proposition}

\begin{remark}
	\begin{enumerate}[(i)]
		\item[]
		\item These are weak conditions (in contrast to Taylor series), but pathological functions are excluded, such as
			\[
				f(x) = \frac{1}{x}, \quad f(x) = \sin \frac{1}{x}, \quad f(x) =
				\begin{cases}
					0 & x \in \mathbb{Q},\\
					1 & x \not \in \mathbb{Q}.
				\end{cases}
			\]
		\item The converse is not true.
		\item The proof is difficult.
	\end{enumerate}
	
\end{remark}

\subsubsection{Convergence of Fourier Series}%
\label{subsub:convergence_of_fourier_series}

\begin{theorem}
	If $f(x)$ has continuous derivatives up to the $p$'th derivative, which is discontinuous, then the Fourier series converges as $\mathcal{O}(n^{-(p+1)})$.
\end{theorem}

\begin{exbox}
	Take the square wave\index{square wave}, with $p = 0$.
	\[
		f(x) =
		\begin{cases}
			1 & 0 \leq x < 1, \\
			-1 & -1 \leq x < 0.
		\end{cases}
	\]
	The Fourier series is
	\[
		f(x) = 4 \sum_{m = 1}^{\infty} \frac{\sin (2m - 1) \pi x}{(2m - 1)\pi}
	.\]
	We now look at the general ``see-saw'' wave\index{see-saw wave}, with $p = 1$. Here
	\[
		f(x) =
		\begin{cases}
			x(1 - \xi) & 0 \leq x < \xi, \\
			\xi(1 - x) & \xi \leq x < 1
		\end{cases}
		\text{ on } 0 \leq x < 1,
	\]
	and odd for $-1 \leq x < 0$. The Fourier series is
	\[
		f(x) = 2 \sum_{n = 1}^{\infty} \frac{\sin n \pi \xi \sin n \pi x}{(n \pi)^2}
	.\]
	For $\xi = 1/2$, we have
	\[
		f(x) = 2 \sum_{m = 1}^{\infty}(-1)^{m+1} \frac{\sin (2m - 1) \pi x}{((2m - 1)\pi)^2}
	.\]
	For $p = 2$, take $f(x) = x(1-x)/2$ on $0 \leq x < 1$, and odd for $-1 \leq x < 0$. The Fourier series is
	\[
		f(x) = 4 \sum_{m = 1}^{\infty} \frac{\sin (2m - 1) \pi x}{((2m - 1)\pi)^3}
	.\]
	Consider $f(x) = (1 - x^2)^2$, for $p = 3$. Then $a_n = \mathcal{O}(n^{-4})$.
\end{exbox}

\subsubsection{Integration of Fourier Series}%
\label{subsub:integration_of_fourier_series}

It is always valid to integrate the Fourier series of $f(x)$ term-by-term to obtain
\[
	F(x) = \int_{-L}^{x} f(x)\diff x
,\]
because $F(x)$ satisfies the Dirichlet conditions if $f(x)$ does.

\subsubsection{Differentiation of Fourier Series}%
\label{subsub:differentiation_of_fourier_series}

Differentiation needs to be done with great care. Consider the square wave. We differentiate it to get
\[
	f'(x) = 4 \sum_{m = 1}^{\infty} \cos (2m - 1) \pi x
.\]
But this is unbounded.

\begin{theorem}
	If $f(x)$ is continuous and satisfies the Dirichlet conditions, and $f'(x)$ satisfies the Dirichlet conditions, then $f'(x)$ can be found by term-by-term differentiation of the Fourier series of $f(x)$.
\end{theorem}

\begin{exbox}
	If we differentiate the see-saw with $\xi = 1/2$, then we get an offset square wave.
\end{exbox}

\subsection{Parseval's Theorem}%
\label{sub:parseval_s_theorem}\index{Parseval's theorem}

This gives the relation between the integral of the square of a function and the sum of the squares of the Fourier coefficients:

\begin{align*}
	\int_{0}^{2L}[f(x)]^2\diff x &= \int_{0}^{2L}\diff x \left[ \frac{1}{2} a_0 + \sum_{n}a_n \cos \frac{n \pi x}{L} + \sum_{n}b_n \sin \frac{n \pi x}{L} \right]^2 \\
				   &= \int_{0}^{2L}\diff x \left[ \frac{1}{4} a_0^2 + \sum_{n} a_n^2 \cos^2 \frac{n \pi x}{L} + \sum_{n} b_n^2 \sin^2 \frac{n \pi x}{L} \right]\\
				   &= L \left[ \frac{1}{2} a_0^2 + \sum_{n = 1}^{\infty} (a_n^2 + b_n^2) \right].
\end{align*}

This is also called the \textbf{completeness relation}\index{completeness relation} because the left hand side is always greater than equal to the right hand side if any basis is missing.

\begin{exbox}
	Take the sawtooth wave. We have
	\[
	LHS = \int_{-L}^{L} x^2\diff x = \frac{2}{3}L^3
	,\]
	\[
	RHS = L \sum_{n = 1}^{\infty} \frac{4L^2}{n^2 \pi^2} = \frac{4L^3}{\pi ^2}\sum_{n = 1}^{\infty} \frac{1}{n^2}
	.\]
	Therefore, we obtain
	\[
	\sum_{n = 1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}
	.\]
\end{exbox}

\subsection{Alternative Fourier Series}%
\label{sub:alternative_fourier_series}

\subsubsection{Half-range Series}%
\label{subsub:half_range_series}\index{half-range series}

Consider $f(x)$ defined only on $0 \leq x < L$. Then we can extend its range over $- L \leq x < L$ in two simple ways:
\begin{enumerate}[(i)]
	\item Require it to be odd, so $f(-x) = -f(x)$. Then $a_n = 0$, and
		\[
			b_n = \frac{2}{L} \int_{0}^{L} \sin \frac{n \pi x}{L}\diff x
		.\]
		This is a Fourier sine series.
	\item Require it to be even, so $f(-x) = f(x)$. Then $b_n = 0$,
		\[
			a_n = \frac{2}{L} \int_{0}^{L}f(x) \cos \frac{n \pi x}{L}\diff x
		.\]
		This is a Fourier cosine series.
\end{enumerate}

\subsubsection{Complex Representation}%
\label{subsub:complex_representation}

Recall that
\[
	\cos \frac{n \pi x}{L} = \frac{1}{2} \left( e^{i n \pi x/L} + e^{-i n \pi x/L}\right), \quad \sin \frac{n \pi x}{L} = \frac{1}{2i} \left(e^{i n \pi x/L} - e^{-i n \pi x/L}\right)
.\]
So our Fourier series becomes
\begin{align*}
	f(x) &= \frac{1}{2} a_0 + \sum_{n = 1}^{\infty}a_n \cos \frac{n \pi x}{L} + \sum_{n = 1}^{\infty} b_n \sin \frac{n \pi x}{L} \\
	     &= \frac{1}{2} a_0 + \frac{1}{2} \sum_{n = 1}^{\infty}(a_n - i b_n) e^{i n \pi x/L} + \frac{1}{2} \sum_{n = 1}^{\infty}(a_n + i b_n) e^{-i n \pi x/L} \\
	     &= \sum_{m = -\infty}^{\infty} c_m e^{i m \pi x/L}.
\end{align*}
The coefficients $c_m$ satisfy
\[
c_m =
\begin{cases}
	\frac{1}{2}(a_{m} - ib_{m}) & m > 0, \\
	\frac{1}{2} a_0 & m = 0, \\
	\frac{1}{2}(a_{-m} + i b_{-m}) & m < 0.
\end{cases}
\]
Equivalently,
\[
	c_m = \frac{1}{2L}\int_{-L}^{L}f(x) e^{-i m \pi x/L}\diff x
.\]
Our inner product in the complex representation is
\[
	\langle f, g \rangle = \int f^{\ast}g \diff x
.\]
This is orthogonal, as
\[
\int_{-L}^{L} e^{-i m \pi x/L}e^{i n \pi x/L} \diff x = 2L \delta_{mn}
,\]
and satisfies Parseval's theorem as a result:
\[
	\int_{-L}^{L}|f(x)|^2\diff x = 2L \sum_{m = -\infty}^{\infty}|c_m|^2
.\]

\subsection{Fourier Series Motivations}%
\label{sub:fourier_series_motivations}

\subsubsection{Self-adjoint matrices}%
\label{subsub:self_adjoint_matrices}

Suppose $\mathbf{u}, \mathbf{v}$ are complex $N$-vectors with inner product $\langle \mathbf{u}, \mathbf{v} \rangle = \mathbf{u}^{\dagger} \mathbf{v}$. Then matrix $A$ is self-adjoint\index{self-adjoint matrix} (or Hermitian\index{Hermitian matrix}) if
\[
	\langle A \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{u}, A \mathbf{v} \rangle \implies A^{\dagger} = A
.\]
The eigenvalues $\lambda_1, \ldots, \lambda_N$ of $A$ satisfy the following properties:
\begin{enumerate}[(i)]
	\item The eigenvalues are real: $\lambda_n^{\ast} = \lambda_n$.
	\item If $\lambda_n \neq \lambda_m$, then their respective eigenvectors are orthogonal: $\langle \mathbf{v}_n, \mathbf{v}_m \rangle = 0$.
	\item If we rescale our eigenvectors then $\{\mathbf{v}_1, \ldots, \mathbf{v}_N\}$ form an orthonormal basis.
\end{enumerate}

Given $\mathbf{b}$, we can try to solve for $\mathbf{x}$ in $A \mathbf{x} = \mathbf{b}$. Express 
\[
	\mathbf{b} = \sum_{n = 1}^{N} b_n \mathbf{v}_n, \quad \mathbf{x} = \sum_{n = 1}^{N} c_n \mathbf{v}_n
.\]
Substituting into the equation,
\begin{align*}
	A \mathbf{x} &= \sum_{n = 1}^{N} A c_n \mathbf{v}_n = \sum_{n = 1}^{N} c_n \lambda_n \mathbf{v}_n, \\
	\mathbf{b} &= \sum_{n = 1}^{N} b_n \mathbf{v}_n.
\end{align*}
Equating and using orthogonality,
\[
c_n \lambda_n = b_n \implies c_n = \frac{b_n}{\lambda_n}
.\]
Hence the solution is
\[
\mathbf{x} = \sum_{n = 1}^{N}\frac{b_n}{\lambda_n} \mathbf{v}_n
.\]

\subsubsection{Solving inhomogeneous ODE with Fourier series}%
\label{subsub:solving_inhomogeneous_ode_with_fourier_series}

Take the following problem: We wish to find $y(x)$ given $f(x)$ for which
\[
	\mathcal{L}(y) = - \frac{\Diff2 y}{\diff x^2} = f(x)
,\]
subject to the boundary conditions $y(0) = y(L) = 0$. The related eigenvalue problem is 
\[
	\mathcal{L}y_n = \lambda_n y_n, \quad y_n(0) = y_n(L) = 0
.\]
This has eigenfunctions and eigenvalues
\[
	y_n(x) = \sin \frac{n \pi x}{L}, \quad \lambda_n = \left( \frac{n \pi}{L} \right)^2
.\]
Note that $\mathcal{L}$ is a self-adjoint ODE with orthogonal eigenfunctions. Thus we seek solutions as a half-range sine series. We try
\[
	y(x) = \sum_{n = 1}^{\infty} c_n \sin \frac{n \pi x}{L},
\]
and expand
\[
	f(x) = \sum_{n = 1}^{\infty} b_n \sin \frac{n \pi x}{L}
.\]
Substituting this in,
\begin{align*}
	\mathcal{L}y &= - \frac{\Diff2}{\diff x^2} \left( \sum_{n} c_n \sin \frac{n \pi x}{L} \right) = \sum_{n = 1}^{\infty} c_n \left(\frac{n \pi}{L} \right)^2 \sin \frac{n \pi x}{L} \\
		     &= \sum_{n = 1}^{\infty} b_n \sin \frac{n \pi x}{L}.
\end{align*}
By orthogonality, we have
\[
	c_n \left( \frac{n \pi}{L}\right)^2 = b_n \implies c_n = \left( \frac{L}{n \pi} \right)^2
.\]
Thus the solution is
\[
	y(x) = \sum_{n = 1}^{\infty} \left( \frac{L}{n \pi}\right)^2 b_n \sin \frac{n \pi x}{L} = \sum_{n = 1}^{\infty} \frac{b_n}{\lambda_n} y_n
.\]
This is similar to a self-adjoint matrix.

\begin{exbox}
	Consider the square wave on $L = 1$, as an odd function. This has Fourier series
	\[
		f(x) = 4 \sum_{m} \frac{\sin (2m - 1)\pi x}{(2m - 1)\pi}
	.\]
	So the solution should be
	\[
		y(x) = \sum \frac{b_n}{\lambda_n} y_n = 4 \sum_{m} \frac{\sin (2m - 1)\pi x}{((2m -1)\pi)^3}
	.\]
	This is the Fourier series for $y(x) = x(1-x)/2$.
\end{exbox}

\newpage

\section{Sturm-Liouville theory}%
\label{sec:sturm_liouville_theory}

\subsection{Second-order linear ODEs}%
\label{sub:second_order_linear_odes}

We wish to solve a general inhomogeneous ODE
\[
	\mathcal{L}y = \alpha(x) y'' + \beta(x) y' + \gamma(x) y = f(x)
.\]
\begin{itemize}
	\item The \textbf{homogeneous} equation $\mathcal{L} y = 0$ has two independent solutions $y_1(x)$, $y_2(x)$. The \textbf{complementary function}\index{complementary function} $y_c(x)$ is the general solution of
		\[
			y_c(x) = A y_1(x) +B y_2(x)
		,\]
		where $A, B$ are constants.
	\item The \textbf{inhomogeneous} equation $\mathcal{L}y = f(x)$ has a special solution, the \textbf{particular integral}\index{particular integral} $y_p(x)$. The general solution is then
		\[
			y(x) = y_p(x) + Ay_1(x) + By_2(x)
		.\]
	\item Two \textbf{boundary} or \textbf{initial} conditions are required to determine $A, B$:
		\begin{enumerate}[(a)]
			\item \textbf{Boundary conditions}\index{boundary conditions} require us to solve the equation on $a < x < b$ given $y$ at $x = a, b$ (Dirichlet conditions), or given $y'$ at $x = a, b$ (Neumann conditions), or given a mixed value $y + ky'$. Boundary conditions are often assumed to be $y(a) = y(b)$, to admit the trivial solution $y \equiv 0$. This can be done by adding complementary functions
				\[
				\tilde y = y + A_1 y_1 + B y_2
				.\]
			\item \textbf{Initial condition}\index{initial conditions} require us to solve the equation for $x \geq a$, given $y$ and $y'$ at $x = a$.
		\end{enumerate}
		
\end{itemize}

\subsubsection{General eigenvalue problem}%
\label{subsub:general_eigenvalue_problem}

To solve the equation employing eigenfunction expansion, we are required to solve the related eigenvalue problem
\[
	\alpha(x) y'' + \beta(x) y' + \gamma(x) y = - \lambda \rho(x) y
,\]
with specified boundary conditions. This forms often occurs in higher dimensions, after separation of variables.

\subsection{Self-adjoint operators}%
\label{sub:self_adjoint_operators}

For two complex-valued functions $f, g$ on $a \leq x \leq b$, we can define the \textbf{inner product}\index{inner product of complex-valued functions}
\[
	\langle f, g \rangle = \int_{a}^{b} f^{\ast}(x) g(x)\diff x
.\]
The norm is then $\|f\| = \langle f, f \rangle^{1/2}$.

\subsubsection{Sturm-Liouville equation}%
\label{subsub:sturm_liouville_equation}

The eigenvalue problem greatly simplifies if $\mathcal{L}$ is \textbf{self-adjoint}, that is, it can be expressed in \textbf{Sturm-Liouville form}\index{Sturm-Liouville form}
\[
	\mathcal{L} y \equiv - (\rho y')' + q y = \lambda \omega y
,\]
where the \textbf{weight function}\index{weight function} $\omega(x)$ is non-negative. We can convert to Sturm-Liouville form by multiplying by an integrating factor $F(x)$ to find
\[
F \alpha y'' + F \beta y' + F \gamma y = - \lambda F \rho y
.\]
This gives
\[
	\frac{\diff}{\diff x} (F \alpha y') - F' \alpha y' - F \alpha' y' + F \beta y' + F \gamma y = - \lambda F \rho y
.\]
Eliminating $y'$ terms, we require
\[
	F' \alpha = F(\beta - \alpha') \implies \frac{F'}{F} = \frac{\beta - \alpha'}{\alpha}
.\]
Solving, we get
\[
	F(x) = \exp \left( \int^{x} \frac{(\beta - \alpha')}{\alpha} \diff x \right)
,\]
and $(F \alpha y')' + F \gamma y = - \lambda F \rho y$. So $\rho(x) = F(x) \alpha (x)$, $q(x) = - F(x) \gamma(x)$, and $\omega(x) = F(x) \rho(x)$. This is non-negative as $F(x) > 0$.

\begin{exbox}
	Take the Hermite equation
	\[
	y'' - 2xy' + 2n y = 0
	.\]
	Putting this into Sturm-Liouville form, we have $\alpha = 1$, $\beta - 2x$, $\gamma = 0$ and $\lambda \rho = 2n$. Thus we take
	\[
		F = \exp \left( \int^{x} \frac{-2x}{2} \diff x \right) = e^{-x^2}
	.\]
	Hence
	\[
		\mathcal{L} y \equiv -(e^{-x^2} y')' = 2n e^{-x^2}y
	.\]
\end{exbox}

\subsubsection{Self-adjoint definition}%
\label{subsub:self_adjoint_definition}

A linear operator $\mathcal{L}$ is \textbf{self-adjoint}\index{self-adjoint operator} on $a \leq x \leq b$ for all pairs of functions $y_1, y_2$ satisfying boundary conditions, if
\[
	\langle y_1, \mathcal{L}y_2 \rangle = \langle \mathcal{L}y_1, y_2 \rangle
,\]
or
\[
	\int_{a}^{b} y^{\ast}_1(x) \mathcal{L}y_2(x)\diff x = \int_{a}^{b} (\mathcal{L}y_1(x))^{\ast} y_2(x) \diff x
.\]

Substituting the Sturm-Liouville form into this equation gives
\begin{align*}
	\langle y_1, \mathcal{L}y_2 \rangle - \langle \mathcal{L}y_1, y_2 \rangle &= \int_{a}^{b} [-y_1(\rho y_2')' + y_1qy_2 + y_2(\rho y_1')' - y_2 qy_1]\diff x \\
										  &= \int_{a}^{b} [-(\rho y_1 y_2')' + (\rho y_1' y_2)']\diff x \\
										  &= \left[ - \rho y_1 y_2' + \rho y_1' y_2 \right]_{a}^{b} = 0.
\end{align*}
for given boundary conditions at $x = a, b$. Suitable boundary conditions include:
\begin{itemize}
	\item $y(a) = y(b) = 0$, $y'(a) = y'(b) = 0$, or mixed boundary condition $y + ky' = 0$;
	\item Periodic functions $y(a) = y(b)$;
	\item Singular points of the ODE $\rho(a) = \rho(b) = 0$;
	\item Combinations of the above.
\end{itemize}

\subsection{Properties of self-adjoint operators}%
\label{sub:properties_of_self_adjoint_operators}

Self-adjoint operators satisfy many similar properties to self-adjoint matrices:
\begin{enumerate}[1.]
	\item The eigenvalues $\lambda_n$ are real.
	\item The eigenfunctions $y_n$ are orthogonal.
	\item The eigenfunctions $y_n$ form a complete set.
\end{enumerate}

\begin{proofbox}
\begin{enumerate}[1.]
	\item Given $\mathcal{L}y_n = \lambda_n \omega y_n$, we take the complex conjugate $\mathcal{L}y_n^{\ast} = \lambda_n^{\ast} \omega y_n^{\ast}$. Then,
		\[
			0 = \int_{a}^{b} (y_n^{\ast} \mathcal{L}y_n - (\mathcal{L}y_n^{\ast}) y_n)\diff x = (\lambda_n - \lambda_n^{\ast}) \int_{a}^{b} \omega y_n y_n^{\ast}\diff x
		.\]
		But the right hand side is non-zero, unless $\lambda_n = \lambda_n^{\ast}$, so the eigenvalues are real.
	\item Consider two eigenfunctions $\mathcal{L}y_m = \lambda_m \omega y_m$, $\mathcal{L}y_n = \lambda_n \omega y_n$. Then
		\[
			0 = \int_{a}^{b}(y_m \mathcal{L}y_n - y_n \mathcal{L}y_m) \diff x = (\lambda_n - \lambda_n) \int_{a}^{b} \omega y_n y_m \diff x
		.\]
		Since $\lambda_m \neq \lambda_n$, we get
		\[
		\int_{a}^{b} \omega y_n y_m \diff x = 0
		.\]
		We say $y_n, y_m$ are orthogonal with respect to the weight function $\omega(x)$ on the interval $a \leq x \leq b$. Define the inner product with respect to the weight $\omega(x)$ as
		\[
			\langle f, g \rangle_{\omega} = \int_{a}^{b} \omega(x) f^{\ast}(x) g(x) \diff x = \langle \omega f, g \rangle = \langle f, \omega g\rangle
		.\]
	\item Completeness implies we can approximate any well-behaved function $f(x)$ on $a \leq x \leq b$ by the series
		\[
			f(x) = \sum_{n = 1}^{\infty} a_n y_n(x)
		.\]
	To find the expansion coefficients we consider
		\[
			\int_{a}^{b} \omega(x) y_m(x) f(x) \diff x = \sum_{n = 1}^{\infty} a_n \int_{a}^{b} \omega y_n y_m \diff x = a_m \int_{a}^{b} \omega y_m^2\diff x
		.\]
		Hence
		\[
			a_n = \frac{\int_{a}^{b} \omega(x) y_n(x) f(x) \diff x}{\int_{a}^{b}\omega(x) y_n^2(x)\diff x}
		.\]
		Normally, we have normalized eigenfunctions\index{normalized eigenfunction}, where we take
		\[
			Y_n(x) = \frac{y_n(x)}{\left( \int_{a}^{b}\omega y_n^2 \diff x \right)^{1/2}}
		.\]
		This gives $\langle Y_n, Y_m \rangle_{\omega} = \delta_{nm}$, so
		\[
			f(x) = \sum_{n = 1}^{\infty} A_n Y_n(x)
		, \text{ where }
		A_n = \int_{a}^{b} \omega Y_n f \diff x
		.\]
\end{enumerate}
\end{proofbox}

\begin{exbox}
	Recall the Fourier Series in Sturm-Liouville form
	\[
	\mathcal{L}y_n = -\frac{\Diff2 y_n}{\diff x^2} = \lambda_n y_n
	,\]
	with $\lambda_n = (n\pi/L)^2$ by orthogonality relations.
\end{exbox}

\subsection{Completeness and Parseval's Identity}%
\label{sub:completeness_and_parseval_s_identity}

Consider
\begin{align*}
	\int_{a}^{b}\left[ f(x) - \sum_{n = 1}^{\infty} a_n y_n \right]^2\omega \diff x &= \int_{a}^{b} \left[f^2 - 2f \sum_{n}a_n y_n + \sum_{n}a_n^2 y_n^2\right] \omega \diff x \\
											&= \int_{a}^{b}\omega f^2 \diff x - \sum_{n = 1}^{\infty}a_n^2 \int_{a}^{b}\omega y_n^2 \diff x,
\end{align*}
which follows from
\[
\int_{a}^{b} f y_n \omega \diff x = a_n \int_{a}^{b} \omega y_n^2 \diff x
.\]
Hence if the eigenfunctions are \textbf{complete} then the series converges, and we get
\[
\int_{a}^{b} \omega f^2 \diff x = \sum_{n = 1}^{\infty} a_n^2 \int_{a}^{b} \omega y_n^2 \diff x = \sum_{n = 1}^{\infty} A_n^2
.\]
We also get \textbf{Bessel's inequality}\index{Bessel's inequality}, by looking at what happens if some eigenfunctions are missing:
\[
\int_{a}^{b} \omega f^2 \diff x \geq \sum_{n = 1}^{\infty} A_n^2
.\]
We define the partial sums
\[
	S_N(x) = \sum_{n = 1}^{N}a_n y_n
.\]
The error in the partial sum
\[
	\epsilon_N = \int_{a}^{b} \omega[f(x) - S_N(x)]^2\diff x \to 0
.\]
is minimized by the sequence defined as above, as
\begin{align*}
	\frac{\partial \epsilon_N}{\partial a_n} &= \frac{\partial}{\partial a_n} \left[ \int_{a}^{b} \omega \left[f(x) - \sum_{n = 1}^{N}a_n y_n\right]^2 \diff x \right] \\
						 &= - 2 \int_{a}^{b} y_n \omega \left[f - \sum_{n = 1}^{N} a_n y_n\right]\diff x \\
						 &= -2 \int_{a}^{b}(\omega f y_n - a_n \omega y_n^2)\diff x = 0.
\end{align*}

\subsection{Legendre Polynomials}%
\label{sub:legendre_polynomials}

Consider Legendre's equation\index{Legendre's equation} arising from spherical polar coordinates
\[
	(1 - x^2)y'' - 2xy' + \lambda y = 0
\]
on the interval $-1 \leq x \leq 1$ with $y$ finite at $x = \pm 1$. This is in Sturm-Liouville form with $\rho = 1-x^2$, $q = 0$, $\omega = 1$. To solve, we seek a power series about $x = 0$. Let
\[
y = \sum_{n = 0}^{\infty} c_n x^{n}
.\]
Then substituting,
\[
	(1 - x^2)\sum_{n = 0}^{\infty} n(n-1) c_n x^{n-2} - 2x \sum_{n = 0}^{\infty} n c_n x^{n-1} + \lambda \sum_{n = 0}^{\infty} c_n x^{n} = 0
.\]
Equating powers of $x^{n}$, we get
\[
	(n+2)(n+1)c_{n+2} - n(n-1)c_n - 2n c_n + \lambda c_n = 0
,\]
\[
	\implies c_{n+2} = \frac{n(n+1) - \lambda}{(n+1)(n+2)} c_n
.\]
So specifying $c_0, c_1$ gives two independent solutions,
\begin{align*}
	y_{even} &= c_0 \left[ 1 + \frac{(-\lambda)}{2!}x^2 + \frac{(6 - \lambda)(-\lambda)}{4!}x^{4} + \cdots \right], \\
	y_{odd} &= c_1 \left[x + \frac{(2 - \lambda)}{3!}x^3 + \frac{(12 - \lambda)(2 - \lambda)}{5!}x^{5} + \cdots \right].
\end{align*}
But as $n \to \infty$, the ratio of terms tends to $1$, so the radius of convergence is $|x| < 1$. This means this series is divergent at $x = \pm 1$.

However, we can use finiteness to our advantage. Take $\lambda = l(l+1)$ with $l$ an integer. Then one of the series terminates. These \textbf{Legendre polynomials}\index{Legendre polynomials} $P_l(x)$ are eigenfunctions on $-1 \leq x \leq 1$ with normalization convention $P_l(1) = 1$. The first few values are
\begin{align*}
	l &= 0, & \lambda &= 0, & P_0(x) &= 1, \\
	l &= 1, & \lambda &= 2, & P_1(x) &= x, \\
	l &= 2, & \lambda &= 6, & P_2(x) &= (3x^2-1)/2, \\
	l &= 3, & \lambda &= 12, & P_3(x) &= (5x^3 - 3x)/2.
\end{align*}
As these are in Sturm-Liouville form, we get
\[
\int_{-1}^{1}P_n P_m\diff x = 0, \quad \int_{-1}^{1}P_n^2\diff x = \frac{2}{2n+1}
.\]
The normalization can be proven with Rodrigues' formula
\[
	P_n(x) = \frac{1}{2^{n}x!} \left( \frac{\diff}{\diff x} \right)^{n} (x^2 - 1)^{n}
.\]
We can also take the generating function
\[
	\sum_{n = 0}^{\infty} P_n(x)t^{n} = \frac{1}{\sqrt{1 - 2xt + t^2}}
,\]
then using the binomial expansion gives $P_n$. We also have recursive formulas
\begin{align*}
	(l+1)P_{l+1}(x) &= (2l+1)xP_l(x) - lP_{l-1}(x), \\
	(2l+1)P_{l}(x) &= \frac{d}{dx} (P_{l+1}(x) - P_{l-1}(x)).
\end{align*}
The Legendre polynomials are complete, so any function on $-1 \leq x \leq 1$ can be expressed as
\[
	f(x) = \sum_{l = 0}^{\infty} a_l P_l(x)
,\]
where
\[
	a_l = \frac{2l+1}{2} \int_{-1}^{1}f(x) P_n(x)\diff x
.\]

\subsection{Sturm-Liouville Theory and Inhomogeneous ODEs}%
\label{sub:sturm_liouville_theory_and_inhomogeneous_odes}

Consider the inhomogeneous ODE on $a \leq x \leq b$:
\[
	\mathcal{L}y = f(x) - \omega(x) F(x)
.\]
Given eigenfunctions $y_n(x)$ satisfying
\begin{align*}
	\mathcal{L}y_n & \lambda_n \omega y_n, \\
	y(x) &= \sum_{n} c_n y_n(x), \\
	F(x) &= \sum_{n}a_n y_n(x),
\end{align*}
we can find the coefficients
\[
a_n = \int_{a}^{b} \omega F y_n \diff x / \int_{a}^{b} \omega y_n^2\diff x
.\]
Substituting this, we have
\[
\mathcal{L}y = \mathcal{L}\sum_{n}c_n y_n = \sum_{n} c_n \lambda_n \omega y_n = \omega \sum_{n} a_n y_n
.\]
Hence, by orthogonality, $c_n \lambda_n = a_n$, giving
\[
	y(x) = \sum_{n = 1}^{\infty} \frac{a_n}{\lambda_n} y_n(x)
.\]
This assumes $\lambda_n \neq 0$.

Generalizing, if we have a linear response term, as is often induced by a driving force,
\[
	\mathcal{L}y - \tilde \lambda \omega y = f(x)
.\]
The solution becomes
\[
	y(x) = \sum_{n = 1}^{\infty} \frac{a_n}{\lambda_n - \tilde \lambda} y_n(x)
.\]
This assumes $\tilde \lambda \neq \lambda_n$.

\subsubsection{Integral solution and Green's function}%
\label{subsub:integral_solution_and_green_s_function}

Recall that
\[
	y(x) = \sum_{n = 1}^{\infty}\frac{a_n}{\lambda_n} y_n(x) = \sum_{n} \frac{y_n(x)}{\lambda_n \mathcal{N}} \int_{a}^{b} \omega(\xi) F(\xi) y_n(\xi)\diff \xi
,\]
where $\mathcal{N} = \int \omega y_n^2 \diff x$. Then, we can continue rewriting as
\[
	\int_{a}^{b} \sum_{n = 1}^{\infty} \frac{y_n(x) y_n(\xi)}{\lambda_n \mathcal{N}_n} \omega(\xi) F(\xi)\diff \xi = \int_{a}^{b}G(x, \xi) f(\xi)\diff \xi
,\]
where
\[
	G(x, \xi) = \sum_{n = 1}^{\infty} \frac{y_n(x) y_n(\xi)}{\lambda_n \mathcal{N}_n}
\]
is the eigenfunction expansion of the Green's function\index{Green's function}. Note that $G(x, \xi)$ depends only on $\mathcal{L}$ and the boundary conditions, not on the forcing term $f(x)$: it acts like $\mathcal{L}^{-1}$.

\newpage

\part{PDEs on Bounded Domains}%
\label{prt:pdes_on_bounded_domains}

\section{The Wave Equation}%
\label{sec:the_wave_equation}

\subsection{Waves on an elastic string}%
\label{sub:waves_on_an_elastic_string}

Consider small displacements on a stretched string with fixed ends at $x = 0$ and $x = L$, with boundary conditions $y(0, t) = y(L, t) = 0$, and initial conditions
\[
	y(x, 0) = p(x), \quad \frac{\partial y}{\partial t}(x, 0) = q(x)
.\]
We derive the equation of motion by balancing forces on the segment $(x, x + \delta x)$, and taking $\delta x \to 0$. Then the boundary of the string on the segment induces forces $T_1, T_2$ at angles $\theta_1, \theta_2$ to the horizontal.

Assume that $|\partial y/\partial x| \ll 1$, so $\theta_1$, $\theta_2$ are small.

\begin{itemize}
	\item Resolving in the $x$-direction, $T_1 \cos \theta_1 = T_2 \cos \theta_2$, so $T_1 \approx T_2 = T$. Hence, tension $T$ is a constant independent of $x$, up to $\mathcal{O}(|\partial y/\partial x|^2)$.
	\item Resolving in the $y$-direction,
		\begin{align*}
			F_T &= T_2 \sin \theta_2 - T_1 \sin \theta_1 \approx T \left( \left. \frac{\partial y}{\partial x}\right|_{x + \delta x} \!\!\!\!\!\! - \left. \frac{\partial y}{\partial x} \right|_{x}\right) \\
			    &= T \frac{\partial^2 y}{\partial x^2} \delta x.
		\end{align*}
\end{itemize}

Hence, by Newton's law,
\begin{align*}
	F &= ma = (\mu \delta x) \frac{\partial^2 y}{\partial t^2} = F_T + F_g \\
	&= T \frac{\partial^2 y}{\partial x^2} \delta x - g \mu \delta x,
\end{align*}
where $\mu$ is the mass per unit length. define the wave speed as $c = \sqrt{T/\mu}$, and we find
\[
\frac{\partial^2 y}{\partial t^2} = \frac{T}{\mu} \frac{\partial^2 y}{\partial x^2} - g = c^2 \frac{\partial^2 y}{\partial x^2} - g
.\] 
Assume gravity is negligible. Then we have the 1-dimensional wave equation
\[
\frac{1}{c^2} \frac{\partial^2 y}{\partial t^2} = \frac{\partial^2 y}{\partial x^2}
.\]

\subsection{Separation of Variables}%
\label{sub:separation_of_variables}

We wish to solve the wave equation subject to boundary conditions and initial conditions. Consider possible solution of separable form
\[
	y(x, t) = X(x) T(t)
.\]
Substitute in the wave equation
\[
\frac{1}{c^2} X \ddot T = X'' T \implies \frac{1}{c^2} \frac{\ddot T}{T} = \frac{X''}{X}
.\]
But the left hand side depends only on $t$, while the right hand side depends only on $x$. This means both sides must be equal to a constant $\lambda$. Hence
\begin{align*}
	X'' + \lambda X &= 0, \\
	\ddot T + \lambda c^2 T &= 0.
\end{align*}

\subsection{Boundary Conditions and Normal Modes}%
\label{sub:boundary_conditions_and_normal_modes}

We have three possibilities for $\lambda$.
\begin{enumerate}[(i)]
	\item $\lambda < 0$. We have $\chi^2 = - \lambda$ for the characteristic polynomial, so
		\[
			X(x) = A e^{\chi x} + Be^{-\chi x} = \tilde A \cosh \chi x + \tilde B \sinh \chi x
		.\]
		But the boundary conditions $X(0) = X(L) = 0$ imply $\tilde A = \tilde B = 0$, giving the trivial solution.
	\item $\lambda = 0$. Then $X(x) = Ax + B$, again giving $A = B = 0$.
	\item $\lambda > 0$. Then $X(x) = A \cos \sqrt \lambda x + B \sin \sqrt \lambda L = 0$. Since $X(0) = 0$, $A = 0$, and $X(L) = 0$ gives $\sqrt \lambda L = n \pi$, so
		\[
			X_n(x) = B_n \sin \frac{n \pi x}{L}, \quad \lambda_n = \left( \frac{n \pi}{L}\right)^2
		.\]
		These are the normal modes\index{normal modes} because the spatial shape in $x$ does not change in time.
\end{enumerate}

\subsection{Initial Conditions and Temporal Solutions}%
\label{sub:initial_conditions_and_temporal_solutions}

Substituting the eigenvalues $\lambda_n$ into the time ODE:
\[
\ddot T + \frac{n^2 \pi^2 c^2}{L^2} T = 0
.\]
This gives
\[
	T_n(t) = C_n \cos \frac{n \pi ct}{L} + D_n \sin \frac{n \pi ct}{L}
.\]
Thus a specific solution to the wave equation is
\[
	y_n(x, t) = T_n(t)X_n(x) = \left(c_n \cos \frac{n \pi ct}{L} + D_n \sin \frac{n \pi ct}{L} \right) \sin \frac{n \pi x}{L}
.\]
Since the wave equation and boundary conditions are linear, we can add the solutions together to find the general string solution
\[
	y(x, t) = \sum_{n = 1}^{\infty} \left( c_n \cos \frac{n \pi ct}{L} + D_n \sin \frac{n \pi ct}{L} \right) \sin \frac{n \pi x}{L}
.\]
By construction, this satisfies boundary conditions, so now we need to impose the initial conditions. For $t = 0$, we have
\[
	y(x, 0) = p(x) = \sum_{n = 1}^{\infty} c_n \sin \frac{n \pi x}{L}
,\]
\[
	\frac{\partial y}{\partial t}(x, 0) = q(x) = \sum_{n = 1}^{\infty} \frac{n \pi c}{L} D_n \sin \frac{n \pi x}{L}
.\]
Hence the coefficients are given by a Fourier sine series
\begin{align*}
	C_n &= \frac{2}{L} \int_{0}^{L}p(x) \sin \frac{n \pi x}{L} \diff x, \\
	D_n &= \frac{2}{n \pi c} \int_{0}^{L}q(x) \sin \frac{n \pi x}{L} \diff x.
\end{align*}

\begin{exbox}
	Pluck a string at $x = \xi$, drawing it back as
	\[
		y(x, 0) = \rho(x) =
		\begin{cases}
			x(1 - \xi) & 0 \leq x \leq \xi, \\
			\xi(1 - x) & \xi \leq x \leq 1,
		\end{cases}
	\]
	\[
		\frac{\partial y}{\partial t} (x, 0) = q(x) = 0
	.\]
	Then by Fourier series, $C_n = (2 \sin n \pi \xi)/(n \pi)^2$, $D_n = 0$. Thus we have the solution
	\[
		y(x, t) = \sum_{n = 1}^{\infty} \frac{2}{(n \pi)^2} \sin n \pi \xi \sin n \pi x \cos n \pi c t
	.\]
	Taking $\xi = 1/2$, we get $c_{2m} = 0$, $c_{2m-1} = 2(-1)^{m+1}/((2m-1)\pi)^2$. For a guitar, we typically have $1/4 \leq \xi \leq 1/3$, and for a violin we have $\xi \approx 1/7$.
\end{exbox}

Note, if we recall the sine and cosine summation identities, we can rewrite our solution as
\begin{align*}
	y(x, t) &= \frac{1}{2} \sum_{n = 1}^{\infty}\biggl[ c_n \sin \frac{n \pi}{L} (x - ct) + D_n \cos \frac{n \pi}{L} (x - ct) \\
		&+ C_n \sin \frac{n \pi}{L} (x + ct) + D_n \cos \frac{n \pi}{L} (x + ct) \biggr] = f(x - ct) + g(x + ct)
\end{align*}
The standing wave solution is made up of a right-moving wave and a left-moving wave.

\subsection{Oscillation Energy}%
\label{sub:oscillation_energy}

A vibrating string has kinetic energy due to its motion:
\[
	KE = \frac{1}{2} \mu \int_{0}^{L} \left( \frac{\partial y}{\partial t}\right)^2 \diff x
\]
and potential energy due to stretching
\[
	PE = T \Delta X = T \int_{0}^{L} \left( \sqrt{1 + \left( \frac{\partial y}{\partial x} \right)^2} - 1 \right) \diff x \approx \frac{1}{2} T \int_{0}^{L} \left( \frac{\partial y}{\partial x}\right)^2 \diff x
.\]
The total summed energy becomes (using $c^2 = T/\mu$)
\[
	E = \frac{1}{2} \mu \int_{0}^{L} \left[ \left( \frac{\partial y}{\partial t} \right)^2 + c^2 \left( \frac{\partial y}{\partial x} \right)^2 \right] \diff x
.\]
Substituting and using orthogonality,
\begin{align*}
	E =& \frac{1}{2} \mu \sum_{n = 1}^{\infty} \int_{0}^{L} \biggl[ \left( \frac{- n \pi c}{L} C_n \sin \frac{n \pi c t}{L} + \frac{n \pi c}{L}D_n \cos \frac{n \pi c t}{L} \right)^2 \sin^2 \frac{n \pi x}{L} \\
	  & \quad + c^2 \left( C_n \cos \frac{n \pi c t}{L} + D_n \sin \frac{n \pi c t}{L} \right)^2 \frac{n^2 \pi^2}{L^2} \cos^2 \frac{n \pi x}{L} \biggr]\diff x \\
		=& \frac{1}{4} \mu \sum_{n = 1}^{\infty} \frac{n^2 \pi^2 c^2}{L}(C_n^2 + D_n^2).
\end{align*}
This is the sum of the energy of the normal modes, and is constant, hence energy is conserved in time.

\subsection{Wave Reflection and Transmission}%
\label{sub:wave_reflection_and_transmission}

Recall the travelling wave solution. A simple harmonic traversing wave is
\[
	y = \Re \bigl[ A e^{iw (t - x/c)} \bigr] = |A| \cos \left( w - \left( t - \frac{x}{c} \right) + \phi \right)
,\]
where the phase is $\phi = \arg A$, and the wavelength is $2 \pi c / \omega$. Consider a density discontinuity on a string at $x = 0$, with
\[
\mu =
\begin{cases}
	\mu_{-} & x < 0 \implies c_{-} = \sqrt{T/\mu_{-}}, \\
	\mu_{+} & x > 0 \implies c_{+} = \sqrt{T/\mu_{+}}.
\end{cases}
\]
We will assume constant tension. Consider the incident wave on the junction. $Ae^{i \omega(t - x/c_{-})}$. Then it will split into a reflected wave $Be^{i \omega(t + x/c_{-})}$ and a transmitted wave $De^{i\omega(t - x/c_{+})}$. 

The boundary conditions at $x = 0$ give:
\begin{itemize}
	\item The string does not break, so $y$ is continuous, implying $A + B = D$.
	\item The forces balance, so
		\[
			T \left. \frac{\partial y}{\partial x}\right|_{x = 0_{-}} = T \left. \frac{\partial y}{\partial x} \right|_{x = 0_{+}}
		,\]
		so $\partial y/\partial x$ is continuous for all $t$. Solving, this gives
		\begin{align*}
			- \frac{i \omega A}{c_{-}} + \frac{i \omega B}{c_{-}} &= - \frac{i \omega D}{c_{+}}, \\
			\implies 2A = D + D\frac{c_{-}}{c_{+}} &= \frac{D}{c_{+}}(c_{+} + c_{-}).
		\end{align*}
\end{itemize}
Hence, we have
\[
D = \frac{2 c_{+}}{c_{-} + c_{+}} A, \quad B = \frac{c_{+} - c_{-}}{C_{+} + c_{-}} A
.\]
In general, a different phase shift $\phi$ is possible. We consider the following limiting cases:
\begin{enumerate}[1)]
	\item If the string is continuous, so $c_{-} = c_{+}$, then $D = A$, $B = 0$.
	\item If we have Dirichlet boundary conditions $u_{+}/u_{-} \to \infty$, then $c_{+}/c_{-} \to 0$. This gives $D = 0$, $B = -A$, so total reflection with opposite phase.
	\item If we have Neumann boundary conditions $u_{+}/u_{-} \to 0$, then $c_{+}/c_{-} \to \infty$. This gives $D = 2A$, $B = A$, so total reflection with the same phase.
\end{enumerate}

\subsection{Wave Equation in 2D Plane Polars}%
\label{sub:wave_equation_in_2d_plane_polars}


The 2D wave equation for $u(r, \theta, t)$ is
\[
\frac{1}{c^2} \frac{\partial^2 u}{\partial t^2} = \nabla^2 u
,\]
with boundary conditions at $r = 1$ on a unit disc, often $u(1, \theta, t) = 0$, and initial conditions for $t = 0$:
\[
	u(r, \theta, 0) = \phi(r, \theta), \quad \frac{\partial u}{\partial t}(r, \theta, 0) = \psi (r, \theta)
.\]
First, we try temporal separation. Substitute
\[
	u(r, \theta, t) = T(t) V(r, \theta)
.\]
This gives
\[
\ddot T + \lambda c^2 T = 0, \quad \nabla^2 V + \lambda V = 0
.\]
In polars, this gives
\[
\frac{\partial^2 V}{\partial r^2} + \frac{1}{r} \frac{\partial V}{\partial r} + \frac{1}{r^2} \frac{\partial^2 V}{\partial \theta^2} + \lambda V
.\]
Now we can try spacial separation:
\[
	V(r, \theta) = R(r) \Theta(\theta)
.\]
This gives equations
\[
	\Theta'' + \mu \Theta = 0, \quad r^2 R'' + r R' + (\lambda r^2 - \mu)R = 0
,\]
where $\lambda, \mu$ are separation constants.

The configuration implies periodic boundary conditions, so $\Theta(0) = \Theta(2 \pi)$ with $\mu > 0$, so the eigenvalues are $\mu = m^2$ with solutions
\[
	\Theta_m(\theta) = A_m \cos m \theta + B_m \sin m \theta
.\]
Divide the radial equation by $r$ to bring it into Sturm-Liouville form with $\mu = m^2$:
\[
	\frac{\diff}{\diff r} (r R') - \frac{m^2}{r}R = - \lambda r R
,\]
where $p(r) = R$, $q(r) = m^2/r$, and weight $w(r) = r$. This has self-adjoint boundary conditions with $R(1) = 0$ and bounded at $R(0)$, since $p(0) = 0$ at a regular singular point.

This is known as Bessel's equation\index{Bessel's equation}. Substituting $z = \sqrt{\lambda} r$, we find
\[
	z^2 \frac{\Diff2 R}{\diff z^2} + z \frac{\diff R}{\diff z} + (z^2 - m^2)R = 0
.\]
We can look at a Frobenius solution by substituting a power series
\[
R = z^{p} \sum_{n = 0}^{\infty} a_n z^{n}
,\]
which gives
\[
	\sum_{n} a_n[(n+p)^2 z^{n+p} + z^{n + p + 2} - m^2z^{n+p}] = c
.\]
The indicial equation is $p^2 - m^2 = 0$, so $p = m$ or $p = -m$. We choose $p = m$ to get the regular solution, with recursion relation
\[
	(n + m)^2 a_n + a_{n-2} - m^2a_n = 0 \implies a_n = \frac{-1}{n(n+2m)}a_{n-2}
.\]
Putting $n \to 2n'$, we have
\[
	a_{2n'} = \frac{-1}{4n'(n' + m)}a_{2n'-2}
,\]
so stepping up from $a_0$, we have
\[
	a_{2n} = \frac{(-1)^{n}}{2^{2n}n!(n+m)(n+m-1)\cdots(m+1)}a_0
.\]
Take $a_0 = (2^{m}m!)^{-1}$, to find the Bessel function of the first kind\index{Bessel function of the first kind}
\[
	J_m(z) = \biggl(\frac{z}{2}\biggr)^{m} \sum_{n = 0}^{\infty} \frac{(-1)^{n}}{n!(n+m)!} \biggl( \frac{z}{2} \biggr)^{2n}
.\]

\begin{exbox}
	Using $y = \sqrt z R$ in the Bessel equation, we find
	\[
		y'' + y \biggl( 1 + \frac{1}{4z} - \frac{m^2}{z^2} \biggr) = 0
	.\]
	So as $z \to \infty$, $y'' = -y$, giving solutions
	\[
		R = \frac{1}{\sqrt z}(A \cos z + B \sin z)
	.\]
\end{exbox}

These also work for $m = \mu$ if we replace $(n+m)!$ with $\Gamma(n+m+1)$. We can get a second solution with $p = -m$, which are the Neumann functions\index{Neumann functions}
\[
	Y_m(z) = \lim_{\gamma \to m} \frac{J_{\gamma}(z) \cos (\gamma \pi) - J_{-\gamma}(z)}{\sin \gamma \pi}
.\]

We can show that
\[
	\frac{\diff}{\diff z} (z^{m} J_m(z)) = z^{m}J_{m-1}(z)
,\]
and hence
\[
	J_m'(z) + \frac{m}{z}J_m(z) = J_{m-1}(z)
.\]
Repeating with $z^{-m}$ we can find the recursion relations
\begin{align*}
	J_{m-1}(z) + J_{m+1}(z) &= \frac{2m}{z} J_m(z), \\
	J_{m-1}(z) - J_{m+1}(z) &= 2 J_m'(z).
\end{align*}
For small $z \to 0$, we have
\[
	J_0(z) \to 1, \quad J_m(z) \to \frac{1}{m!} \biggl( \frac{z}{2} \biggr)^{m}
,\]
\[
	Y_0(z) \to \frac{2}{\pi} \log \biggl( \frac{z}{2} \biggr), \quad Y_m(z) \to - \frac{(m-1)!}{\pi} \biggl( \frac{2}{z} \biggr)^{m}
.\]
For large $z \to \infty$, we have oscillatory solutions
\begin{align*}
	J_m(z) &\approx \sqrt{\frac{2}{\pi z}} \cos \biggl( z - \frac{m \pi}{2} - \frac{\pi}{4} \biggr), \\
	Y_m(z) &\approx \sqrt{\frac{2}{\pi z}} \sin \biggl( z - \frac{m \pi}{2} - \frac{\pi}{4} \biggr).
\end{align*}

Define $j_{mn}$ to be the $n$'th solution to the Bessel function $J_m(z)$, so $J_m(j_{mn}) = 0$. Approximating $J_m$, we get
\[
	\cos \biggl( z - \frac{m \pi}{2} - \frac{\pi}{4} \biggr) = 0
\implies z = n \pi + \frac{m \pi}{2} - \frac{\pi}{4} \approx \tilde j_{mn}
.\]
This is accurate within $10\%$.

\subsection{2D Wave Equation Solution}%
\label{sub:2d_wave_equation_solution}

We know the radial solutions to the 2D wave equation are
\[
	R_m(z) = R_m(\sqrt \lambda r) = A J_m(\sqrt \lambda r) + B Y_m(\sqrt \lambda r)
.\]
By imposing regularity at $r = 0$, we get $B = 0$. Moreover, the boundary condition $R = 0$ at $r = 1$ gives $J_m(\sqrt \lambda) = 0$. But then we must get $\lambda_{mn} = j_{mn}^{2}$.

Thus, with the polar mode, the spatial solution is
\[
	V_{mn}(r, \theta) = \Theta_m(\theta) R_{mn}(\sqrt \lambda_{mn} r) = (A_{mn} \cos m \theta + B_{mn} \sin m \theta) J_m(j_{mn} r)
.\]
The temporal solution is $\ddot T = - \lambda c^2 T$, or $T_{mn}(t) = \cos(j_{mn} ct)$ and $\sin (j_{mn} ct)$. Hence we can sum together to obtain our general solution:
\begin{align*}
	u(r, \theta, t) &= \sum_{n = 1}^{\infty}J_0(j_{0n} r) (A_{0n}\cos (j_{0n} ct) + C_{0n} \sin (j_{0n} ct)) \\
			&+ \sum_{m = 1}^{\infty} \sum_{n = 1}^{\infty} J_m(j_{nm}t)(A_{nm} \cos m \theta+ B_{mn} \sin m \theta) \cos (j_{nm} ct) \\
			&+ \sum_{m = 1}^{\infty} \sum_{n = 1}^{\infty} J_m(j_{nm}r)(C_{mn} \cos m \theta + D_{mn} \sin m \theta) \sin (j_{nm} ct).
\end{align*}
Now we impose the initial conditions at $t = 0$:
\begin{align*}
	u(r, \theta, 0) &= \phi(r, \theta) = \sum_{m = 0}^{\infty} \sum_{n = 1}^{\infty} J_m(j_{mn}r) (A_{mn} \cos m \theta + B_{mn} \sin m \theta), \\
	\frac{\partial u}{\partial t}(r, \theta, 0) &= \psi(r, \theta) = \sum_{m = 0}^{\infty} \sum_{n = 1}^{\infty} j_{mn}c J_m(j_{mn} r) (C_{mn} \cos m \theta + D_{mn} \sin m \theta).
\end{align*}
We can find the coefficients by multiplying by $J_m$, $\cos$ and $\sin$, and exploiting orthogonality. The Bessel functions satisfy
\[
	\int_{0}^{1}J_m(j_{mn}r)J_m(j_{mk}r)r \diff r = \frac{1}{2} [J_n'(j_{mn})]^2 \delta_{nk} = \frac{1}{2} [J_{m+1}(j_{mn})]^2\delta_{nk}
.\]
For example,
\[
	\int_{0}^{2\pi}\diff \theta \cos p \theta \int_{0}^{1} r \diff r J_{pq}(j_{pq} r) \phi(r, \theta) = \frac{\pi}{2} [J_{p+1}(j_{pq})]^2 A_{pq}
.\]

\begin{exbox}
	Consider the initial radial profile $u(r, \theta, 0) = \phi(r) = 1 - r^2$, giving $B_{mn} = 0$ and $A_{mn} = 0$ for $m \neq 0$ and $\frac{\partial u}{\partial t} (r, \theta, 0) = 0$. This gives $C_{mn} = D_{mn} = 0$, and we need to find
	\begin{align*}
		A_{0n} &= \frac{2}{[J_1(j_{0n})]^2} \int_{0}^{1}J_0(j_{0n}r)(1 - r^2)r \diff r \\
		       &= \frac{2}{[J_1(j_{0n})]^2} \frac{J_2(j_{0n})}{j_{0n}^2} \approx \frac{J_2(j_{0n})}{n}.
	\end{align*}

\end{exbox}

\newpage

\section{The Diffusion Equation}%
\label{sec:the_diffusion_equation}

\subsection{Physical Origin of Heat Equation}%
\label{sub:physical_origin_of_heat_equation}

This applies to processes that diffuse due to spatial gradients. An early example was Fick's law\index{Fick's law} with flux $J = - D \nabla c$, with concentration $c$ and diffusion coefficient $D$. For heat flow, we have Fourier's law
\[
q = - k \nabla \theta
,\]
where $q$ is the heat flux, $k$ is the thermal conductivity, and $\theta = T$ is the temperature. In a volume $V$, the overall heat energy $Q$ is
\[
Q = \int c_v \rho \theta \diff V
.\]
The rate of change due to heat flow is
\[
\frac{\diff Q}{\diff t} = \int c_v \rho \frac{\partial \theta}{\partial t} \diff V
.\]
Integrating over the surface $S$ enclosing the volume $V$ gives
\[
	-\frac{\diff Q}{\diff t} = \int_{S} q \cdot \hat n \diff S = \int_{S}(k \nabla \theta) \cdot \hat n \diff S = \int (-k \nabla^2 \theta) \diff V
.\]
Equating these, we find
\[
	\int \biggl( c_v \rho \frac{\partial \theta}{\partial t} - k \nabla^2 \theta \biggr) \diff V = 0
.\]
Since this is true for all $V$, the integrand must vanish. This gives
\[
\frac{\partial \theta}{\partial t} - \frac{k}{c_v \rho} \nabla^2 \theta = 0
,\]
so letting $D = k/(c_v \rho)$,
\[
\frac{\partial \theta}{\partial t} = D \nabla^2 \theta
.\]
Einstein also derived this equation in a different manner, using Brownian motion. Gas particles are diffusing by scattering every $\Delta t$, with probability PDF $p(\xi)$ of moving distance $\xi$ with
\[
	\langle \xi \rangle = \int p(\xi) \xi \diff \xi = 0
.\]
Suppose the PDF after $N \Delta t$ steps is $P_{N \Delta t}(x)$, then for the $(N+1)\Delta t$ steps,
\begin{align*}
	P_{(N+1)\Delta t}(x) &= \int_{-\infty}^{\infty}p(\xi) P_{N \Delta t}(x - \xi) \diff \xi \\
			     &\approx \int_{-\infty}^{\infty}p(\xi) \biggl[ P_{N \Delta t}(x) + P_{N \Delta t}'(x)(-\xi) + P_{N \Delta t}''(x) \frac{\xi^2}{2} \biggr] \diff \xi \\
			     &\approx P_{N \Delta t}(x) - P_{N \Delta t}'(x)\langle \xi \rangle + P_{N \Delta t}''(x) \frac{\langle \xi^2 \rangle}{2}.
\end{align*}
Identifying $P_{N \Delta t}(x) = P(x, N \Delta t)$, we have
\[
	P(x, (N + 1) \Delta t) - P(x, N \Delta t) = \frac{\partial ^2}{\partial x^2} P(x, N \Delta t) \frac{\langle \xi^2 \rangle}{2}
.\]
If we assume $\langle \xi^2 \rangle/2 = D \Delta t$, then $\Delta t \to 0$ gives
\[
\frac{\partial P}{\partial t} = D \frac{\partial^2 P}{\partial x^2}
.\]
\subsection{Similarity Solutions}%
\label{sub:similarity_solutions}

The characteristic relation between variance and time suggest seeking solutions with dimensionless parameters:
\[
	\eta \equiv \frac{x}{2 \sqrt{Dt}}
.\]
So we want to find solution $\theta(x, t) = \theta(\eta)$. Changing variable,
\[
	\frac{\partial \theta}{\partial t} = \frac{\partial \eta}{\partial t} \frac{\partial \theta}{\partial \eta} = -\frac{1}{2}\frac{x}{\sqrt D t^{3/2}} \theta' = - \frac{\eta}{2t} \theta'
,\]
\[
	D \frac{\partial^2 \theta}{\partial x^2} = D \frac{\partial}{\partial x} \biggl( \frac{\partial \eta}{\partial x} \frac{\partial \theta}{\partial \eta} \biggr) = D \frac{\partial}{\partial x} \biggl( \frac{1}{2 \sqrt{Dt}} \theta' \biggr) = \frac{1}{4t} \theta''
.\]
Equating, we get
\[
\theta'' = - 2 \eta \theta'
.\]
Take $\psi = \theta'$, then
\[
\frac{\psi'}{\psi} = - 2 \eta \implies \log \psi = - \eta^2 + C
,\]
\[
\implies \psi = \theta' = C e^{-\eta^2}
.\]
Integrating, we find
\[
	\theta = C \frac{2}{\sqrt \pi} \int_{0}^{\eta} e^{-u^2} \diff u = C \erf \biggl( \frac{x}{2 \sqrt{Dt}} \biggr)
,\]
where we define the error function\index{error function} as
\[
\erf z = \frac{2}{\sqrt \pi} \int_{0}^{z} e^{-u^2}\diff u
.\]
This describes discontinuous initial conditions that spread over time.

\subsection{Heat Conduction in a Finite Bar}%
\label{sub:heat_conduction_in_a_finite_bar}

Suppose we have a uniform bar of length $2L$, with $-L \leq x \leq L$ and initial temperature
\[
	\theta(x, 0) = H(x) =
	\begin{cases}
		1 & 0 \leq x \leq L, \\
		0 & -L \leq x < 0.
	\end{cases}
,\]
and boundary conditions $\theta(L, t) = 1$ and $\theta(-L, t) = 0$.

To apply Sturm-Liouville theory, we need homogeneous boundary conditions. Thus we want to transform the boundary condition. THe problem is then to identify a steady state solution which reflects late-time behaviour.

We try $\theta_s(x) = Ax + B$, which satisfies the heat equation. To satisfy the boundary conditions, we take $A = 1/2L$, $B = 1/2$, to get
\[
	\theta_s(x) = \frac{(x + L)}{2L}
.\]
Transforming, we can solve for
\[
	\hat \theta(x, t) = \theta(x, t) - \theta_s(x)
,\]
with homogeneous boundary conditions $\hat \theta(-L, t) = \hat \theta(L, t) = 0$, and boundary conditions $\hat \theta(x, 0) = H(x) - (x+L)/2L$.

We try $\hat \theta(x, t) = X(x)T(t)$, which gives
\[
X'' = - \lambda X, \quad \dot T = - D \lambda T
.\]
The boundary conditions imply $\lambda > $ 0, with
\[
	X(x) = A \cos \sqrt \lambda x + B \sin \sqrt \lambda x
.\]
For the even solutions,
\[
	\cos \sqrt \lambda L = 0 \implies \sqrt{\lambda_m} = \frac{m \pi}{2L}, \; m = 1, 3, 5,\ldots
\]
and the odd solutions give
\[
	\sin \lambda L = 0 \implies \sqrt{\lambda_n} = \frac{n \pi}{L}, \; n = 1, 2, 3, \ldots
\]
but the initial conditions are odd, so we take
\[
X_n = B_n \sin \frac{n \pi x}{L}, \quad \lambda_n = \frac{n^2 \pi^2}{L^2}
.\]
Putting $\lambda_n$ into the equation for time, we find
\[
	T_n(t) = C_n \exp \biggl( - \frac{Dn^2\pi^2}{L^2}t \biggr)
.\]
Hence we can write the general solution with homogeneous boundary conditions as
\[
	\hat \theta(x, t) = \sum_{n = 1}^{\infty}b_n \sin \frac{n \pi x}{L} \exp \biggl( - \frac{D n^2 \pi^2}{L^2} t \biggr)
.\]
Impose the initial conditions at $t = 0$. We get the Fourier coefficients
\begin{align*}
	b_n &= \frac{1}{L} \int_{-L}^{L} \hat \theta(x, 0) \sin \frac{n \pi x}{L}\diff x \\
	    &= \frac{2}{L} \int_{0}^{L} \underbrace{\biggl(H(x) - \frac{1}{2}\biggr)}_{=1/2}\sin \frac{n \pi x}{L} \diff x - \frac{2}{L} \int_{0}^{L} \frac{x}{2l}\sin \frac{n \pi x}{L} \diff x \\
	    &= \underbrace{\frac{2}{(2m - 1)\pi}}_{\text{if }n = 2m - 1} - \frac{(-1)^{n+1}}{n \pi} = \frac{1}{n \pi}.
\end{align*}
So the solution we get is
\[
	\hat \theta(x, t) = \sum_{n = 1}^{\infty} \frac{1}{n \pi} \sin \frac{n \pi x}{L} \exp \biggl(- \frac{Dn^2\pi^2}{L^2}t \biggr)
,\]
or with the original boundary conditions,
\[
	\theta(x, t) = \frac{x+L}{2L} + \hat \theta(x, t)
.\]
The approximate solution is an excellent fit for $\theta$ if $t \ll 1$.

\newpage

\section{The Laplace Equation}%
\label{sec:the_laplace_equation}

We will now look at Laplace's equation
\[
\nabla^2 \phi = 0
.\]
This has wide applications in mathematical physics, applied and pure mathematics.

Laplace's equation is used in
\begin{itemize}
	\item steady state heat flows,
	\item potential theory $\mathbf{F} = - \nabla \phi$,
	\item incompressible fluid flow $\mathbf{v} = \nabla \phi$.
\end{itemize}

We will solve Laplace's equation in a domain subject to two boundary conditions:
\begin{itemize}
	\item[Dirichlet:] $\phi$ is given on the boundary surface $\partial D$.
	\item[Neumann:] $\mathbf{\hat n} \cdot \nabla \phi$ is given on the boundary surface $\partial D$.
\end{itemize}

\subsection{3D Cartesian Coordinates}%
\label{sub:3d_cartesian_coordinates}

In three dimensions, the equation becomes
\[
\frac{\partial^2 \phi}{\partial x^2} + \frac{\partial^2 \phi}{\partial y^2} + \frac{\partial^2 \phi}{\partial z^2} = 0
.\]
We seek separable solutions $\phi(x, y, z) = X(x)Y(y)Z(z)$. Then substituting,
\[
X''YZ + XY''Z + XYZ'' = 0 \implies \frac{X''}{X} = - \frac{Y''}{Y} - \frac{Z''}{Z} = - \lambda_{\ell}
,\]
and similarly we get
\[
\frac{Y''}{Y} = -\lambda_m, \quad \frac{Z''}{Z} = - \lambda_n = \lambda_{\ell} + \lambda_m
.\]
The general solution from the eigenmodes is
\[
	\phi(x, y, z) = \sum_{\ell, m, n} a_{\ell m n} X_{\ell}(x)Y_m(y)Z_n(z)
.\]

\begin{exbox}[Steady heat conduction]
	Setting $\partial \theta/\partial t = 0$ in the heat equation, we get Laplace's equation.

	Consider a semi-infinite rectangular bar with boundary conditions $\phi = 0$ at $x = 0, a$ and $y = 0, b$. Then we can assume $\phi$ stays `hot' near the origin, with $\phi = 1$ at $z = 0$, and gets cold further away, so $\phi \to 0$ as $z \to \infty$.

	We successively solve for the eigenmodes:
	\begin{itemize}
		\item $X'' = - \lambda_{\ell}X$, with $X(0) = X(a) = 0$. Then the solution is
			\[X_{\ell} = \sin \frac{\ell \pi x}{a}, \quad  \lambda_{\ell} = \frac{\ell^2 \pi^2}{a^2}.
			\]
		\item $Y'' = - \lambda_m Y$, with $Y(0) = Y(b) = 0$. Then the solution is
			\[Y_m = \sin \frac{m \pi y}{b}, \quad \lambda_m = \frac{m^2 \pi^2}{b^2}.\]

		\item $Z'' = -\lambda_nZ = (\lambda_\ell + \lambda_m)Z = \pi^2(\frac{\ell^2}{a^2} + \frac{m^2}{b^2})Z$, with boundary conditions $Z \to 0$ as $z \to \infty$. This implies we must have the negative exponential, so
			\[
				Z_{\ell m} = \exp \biggl[ - \biggl( \frac{\ell^2}{a^2} + \frac{m^2}{b^2} \biggr)^{1/2} \pi z \biggr]
			.\]
	\end{itemize}
	Therefore, the general solution becomes
	\[
		\phi(x, y, z) = \sum_{\ell, m}^{\infty}a_{lm} \sin \frac{\ell \pi x}{L} \sin \frac{m \pi x}{L} \exp \biggl[ - \biggl( \frac{\ell^2}{a^2} \biggr)^{1/2}\pi z \biggr]
	.\]
	Finally, we fix $a_{\ell m}$ by using $\phi(x, y, 0) = 1$. Then
	\begin{align*}
		a_{\ell m} &= \frac{2}{b} \int_{0}^{b} \frac{2}{a} \int_{0}^{a} \sin \frac{\ell \pi x}{a} \sin \frac{m \pi y}{b} \diff x \diff y \\
			   &= \underbrace{\frac{4a}{a(2k-1)\pi}}_{\text{if }l = 2k-1} \underbrace{\frac{4b}{b(2p-1)\pi}}_{\text{if }m = 2p-1} = \frac{16}{\pi^2\ell m}.\quad (2 \nmid lm)
	\end{align*}
	Asymptotically, only the small eigenmodes survive.
\end{exbox}

\subsection{2D Plane Polar Coordinates}%
\label{sub:2d_plane_polar_coordinates}

Recall in 2D plane polars,
\[
	\nabla^2 \phi = \frac{1}{r} \frac{\partial}{\partial r} \biggl( r \frac{\partial \phi}{\partial r} \biggr) + \frac{1}{r^2} \frac{\partial^2 \phi}{\partial \theta^2} = 0
.\]
We try to find separable solutions $\phi(r, \theta) = R(r) \Theta(\theta)$. Substituting, we get
\[
	\Theta'' + \mu \Theta = 0, \quad r(rR')' - \mu R = 0
.\]
\begin{itemize}
	\item The polar equation has periodic boundary conditions, hence if $\mu = m^2$,
		\[
			\Theta_m(\theta) = \cos m \theta, \sin m \theta
		.\]
	\item The radial equation is $r(rR')' - m^2R = 0$. Trying $R =  r^{\beta}$, we get
		\[
			r(\beta r^{\beta})' - m^2 r^{\beta} = 0 \implies \beta^2 - m^2 = 0 \implies \beta = \pm m
		.\]
		Hence $R_m = r^{m}$ and $-r^{m}$, $m \neq 0$. If $m = 0$, we have
		\[
			(r R')' = 0 \implies r R' = C \implies R = C \log r + D
		.\]
\end{itemize}
Hence the general solution is
\[
	\phi(r, \theta) = \frac{a_0}{2} + c_0 \log r + \sum_{m = 1}^{\infty}[(a_m \cos m \theta + b_m \sin m \theta)r^{m} + (c_m \cos m \theta + d_m \sin m \theta) r^{-m}]
.\]

\begin{exbox}[Soap film on a unit disk]
	We solve Laplace's equation with a distorted circular wire of radius $r = 1$, and given boundary conditions $\phi(1, \theta) = f(\theta)$, to find $\phi(r, \theta)$ for $r < 1$.

	First, regularity at $r = 0$ implies $c_m = d_m = 0$ for all $m$ (including $0$). So the equation becomes
	\[
		\phi(r, \theta) = \frac{1}{2}a_0 \sum_{m = 1}^{\infty}(a_m \cos m \theta + b_m \sin m \theta)r^{m}
	.\]
	At $r = 1$, we get
	\[
		\phi(1, \theta) = f(\theta) = \frac{1}{2}a_0 + \sum_{m = 1}^{\infty}(a_m \cos m \theta + b_m \sin m \theta)
	.\]
	Therefore the coefficients are the Fourier coefficients,
	\[
		a_m = \frac{1}{\pi} \int_{0}^{2 \pi}f(\theta) \cos m \theta \diff \theta, \quad b_m = \frac{1}{\pi} \int_{0}^{2 \pi}f(\theta) \sin m \theta \diff \theta
	.\]
	Due to the $r^{m}$ term, only the low Fourier modes survive to the center of the disk.
\end{exbox}

\subsection{3D Cylindrical Polar Coordinates}%
\label{sub:3d_cylindrical_polar_coordinates}

In this case, our Laplacian is
\[
	\nabla^2 \phi = \frac{1}{r} \frac{\partial}{\partial r} \biggl( r \frac{\partial \phi}{\partial r} \biggr) + \frac{1}{r^2} \frac{\partial^2 \phi}{\partial \theta^2} + \frac{\partial^2 \phi}{\partial z^2} = 0
.\]
With $\phi = R(r) \Theta(\theta) Z(z)$, we have
\[
	\Theta'' = - \mu \Theta, \quad Z'' = \lambda Z,\quad r(rR')' + (\lambda r^2 - \mu)R = 0
.\]
\begin{itemize}
	\item The polar equation is $\mu_m = m^2, \Theta_m(\theta) = \cos m \theta$ and $\sin m \theta$.
	\item The radial equation is Bessel's equation, with $R = J_m(kr)$ and $Y_m(kr)$. Setting the boundary conditions, $R = 0$ at $r = a$, means $J_m(ka) = 0$. Hence $k = j_{mn}a$. The radial eigenfunction is $R_{mn}(r) = J_m(j_{mn}r/a)$ (we can eliminate the Neumann solutions as they diverge as $r \to 0$).
	\item The $Z$ equation obeys $Z'' = k^2 Z$, implying $Z = e^{-kz}$ and $Z = e^{kz}$ (we usually eliminate $e^{kz}$ with $Z \to 0$ as $z \to \infty)$.
\end{itemize}
Hence the general solution is
\[
	\phi(r, \theta, z) = \sum_{m = 0}^{\infty} \sum_{n = 1}^{\infty} (a_{mn} \cos m \theta + b_{mn} \sin m \theta) J_{m} \biggl( \frac{j_{mn}}{a} r \biggr) e^{-j_{mn}r/a}
.\]

\begin{exbox}
	We describe steady-state heat flow in a semi-infinite circular wire with boundary conditions $\phi = 0$ at $r = a$, $\phi = T_0$ at $z = 0$ and $\phi \to 0$ as $z \to \infty$. The solution is
	\[
		\phi(r, \theta, z) = \sum_{n = 1}^{\infty} \frac{2T_0}{j_{0n}J_1(j_{0n})}J_0\biggl(\frac{j_{0n}}{a}r\biggr)e^{-j_{0n}z/a}
	.\]
\end{exbox}

\subsection{3D Spherical Polar Coordinates}%
\label{sub:3d_spherical_polar_coordinates}

Recall that in spherical polars, $x = r \sin \theta \cos \phi$, $y = r \sin \theta \sin \phi$, $z = r \cos \theta$ and $\diff V = r^2 \sin \theta \diff r \diff \theta \diff \phi$. Laplace's equation becomes
\[
	\frac{1}{r^2}\frac{\partial}{\partial r}\biggl(r^2 \frac{\partial \Phi}{\partial r}\biggr) + \frac{1}{r^2 \sin \theta} \frac{\partial}{\partial \theta} \biggl( \sin \theta \frac{\partial \Phi}{\partial \theta}\biggr) + \frac{1}{r^2\sin^2\theta} \frac{\partial^2\Phi}{\partial \phi^2} = 0
.\]
We look only at the axisymmetric case, where there is no $\phi$ dependence. So we seek separable solutions $\Phi(r, \theta) = R(r) \Theta(\theta)$. Substituting,
\[
	(\sin \theta \Theta)' + \lambda \sin \theta = 0, \quad (r^2R')' - \lambda R = 0
.\]
\begin{itemize}
	\item The polar equation is Legendre's equation. Substitute $x = \cos \theta$, and note $-1 \leq x \leq 1$. Then, since
		\[
		\frac{\diff x}{\diff \theta} = - \sin \theta \implies \frac{\diff \Theta}{\diff \theta} = - \sin \theta \frac{\diff \Theta}{\diff x}
		,\]
		our equation becomes
		\[
			\frac{\diff}{\diff x} \biggl[(1- x^2) \frac{\diff \Theta}{\diff x} \biggr] + \lambda \Theta = 0
		.\]
		This is Legendre's equation with eigenvalue $\lambda_{\ell} = \ell(\ell + 1)$, and eigenfunctions $\Theta_l(\theta) = P_{\ell}(x) = P_{\ell}(\cos \theta)$.
	\item The radial equation is $(r^2R')' - \ell(\ell+1)R = 0$. We seek solution of the form $R = \alpha r^{\beta}$. Then we get $\beta(\beta + 1) - \ell(\ell + 1) = 0$. This has two solution $\beta = \ell$ or $\beta = - \ell - 1$, giving $R_\ell = r^{\ell}$ and $r^{- \ell - 1}$.
\end{itemize}

Hence the general axisymmetric solution is
\[
	\Phi(r, \theta) = \sum_{\ell = 0}^{\infty} (a_{\ell}r^{\ell} + b_{\ell} r^{-\ell - 1}) P_{\ell}(\cos \theta)
,\]
where $a_{\ell}, b_{\ell}$ are determined by boundary conditions, usually at fixed $r = r_0$. Then the orthogonality of $P_{\ell}$ can be used to obtain coefficients.

\begin{exbox}
	We solve $\nabla^2 \Phi = 0$ with axisymmetric boundary conditions at $r = 1$, $\Phi(1, \theta) = f(\theta)$. Regularity implies $b_{\ell} = 0$, so we have
	\[
		f(\theta) = \sum_{\ell = 0}^{\infty} a_{\ell} P_{\ell}(\cos \theta)
	,\]
	or writing $f(\theta) = F(\cos \theta)$,
	\[
		F(x) = \sum_{\ell = 0}^{\infty}a_{\ell} P_{\ell}(x)
	.\]
	This gives
	\[
		a_{\ell} = \frac{(2 \ell + 1)}{2}\int_{-1}^{1}F(x)P_{\ell}(x)\diff x
	.\]
	If $f(\theta) = \sin^2 \theta$, we get the solution
	\[
		\Phi(r, \theta) = \frac{2}{3}(1 - P_2(\cos \theta)r^2)
	.\]
\end{exbox}

\subsection{Generating functions for Legendre Polynomials}%
\label{sub:generating_functions_for_legendre_polynomials}

Consider a charge on the $z$-axis at $\mathbf{r}_0 = (0, 0, 1)$, then the potential at $P$ becomes
\begin{align*}
	\Phi(\mathbf{r}) &= \frac{1}{|\mathbf{r} - \mathbf{r}_0|}= \frac{1}{(x^2 + y^2 + (z - 1)^2)^{1/2}} \\
			 &= \frac{1}{(r^2 \sin^2 \theta + r^2 \cos^2\theta - 2 r \cos \theta + 1)^{1/2}} \\ 
			 &= \frac{1}{\sqrt{r^2 - 2r \cos \theta + 1}} = \frac{1}{\sqrt{r^2 - 2 r \bar x + 1}}.
\end{align*}

Moreover, this $\Phi$ satisfies $\nabla^2 \Phi = 0$, where $\mathbf{r} \neq \mathbf{r}_0$.

We can represent any axisymmetric solution as a sum of Legendre polynomials:
\[
	\frac{1}{\sqrt{r^2 - 2rx + 1}} = \sum_{\ell = 0}^{\infty} a_{\ell} P_{\ell}(x) r^{\ell}
,\]
with normalisation $P_\ell(1) = 1$ at $x = 1$. Therefore
\[
\frac{1}{1 - r} = \sum_{\ell = 0}^{\infty}a_{\ell}r^{\ell}
,\]
which gives $a_{\ell} = 1$. Thus we have found the generating polynomial
\[
	\frac{1}{\sqrt{1 - 2rx + r^2}} = \sum_{\ell = 0}^{\infty} P_{\ell}(x) r^{\ell}
.\]
Then, expanding the left hand side with binomial theorem, we can find $P_{\ell}(x)$. Using this, we can obtain the normalisation condition. Using this, we can obtain the normalisation condition.

\newpage

\part{Inhomogeneous ODEs; Fourier Transforms}%
\label{prt:inhomogeneous_odes_fourier_transforms}

\section{The Dirac Delta Function}%
\label{sec:the_dirac_delta_function}\index{delta function}

\subsection{Definition of \texorpdfstring{$\delta(x)$}{Delta function}}%
\label{sub:definition_of_delta_function}

Define a generalized function $\delta(x - \xi)$ with the following properties:
\begin{align*}
	\delta(x - \xi) = 0\; \forall x \neq \xi, \\
	\int_{-\infty}^{\infty}\delta(x - \xi)\diff x = 1.
\end{align*}
This acts as a linear operator on an arbitrary function $f(x)$ to produce a number $f(\xi)$, that is,
\[
	\int_{-\infty}^{\infty} \diff x \delta(x - \xi) f(x) = f(\xi)
,\]
provided $f(x)$ is well-behaved at $x = \xi$ and as $x \to \pm \infty$.

\begin{remark}
	\begin{itemize}
		\item[]
		\item The delta function $\delta(x)$ is classified as a distribution. 
		\item As such, $\delta(x)$ always appears in an integrand as a linear operator, where it is well-defined.
		\item It represents a unit point source or an impulse.
	\end{itemize}
\end{remark}

\subsubsection{Limiting Distributions}%
\label{subsub:limiting_distributions}

We can define a discrete approximation to the delta function as the limit as $n \to \infty$ of
\[
	\delta_n(x) =
	\begin{cases}
		0 & |x| > \frac{1}{n}, \\
		n/2 & |x| \leq \frac{1}{n}.
	\end{cases}
.\]
However this is not that good of an approximation. Instead, we can try the continuous approximation as $\eps \to 0$ of
\[
	\delta_{\eps}(x) = \frac{1}{\eps \sqrt \pi} e^{-x^2/\eps^2}
.\]
We can verify the properties of this approximation:
\begin{align*}
	\int_{-\infty}^{\infty}f(x) \delta(x) \diff x &= \lim_{\eps \to 0} \int_{-\infty}^{\infty} \frac{1}{\eps \sqrt \pi} e^{-x^2/\eps^2} f(x) \diff x \\
						      &= \lim_{\eps \to 0}\int_{-\infty}^{\infty}\frac{1}{\sqrt \pi} e^{-y^2}f(\eps y) \diff y \\
						      &= \lim_{\eps \to 0} \int_{-\infty}^{\infty} \diff y \frac{1}{\sqrt \pi} e^{-y^2} [f(0) + \eps y f'(0) + \cdots] \\
						      &= f(0),
\end{align*}
for well-behaved $f$ at $x = 0$ and $x = \pm \infty$.

We can consider further examples:
\[
	\delta_n(x) = \frac{\sin n x}{\pi x} = \frac{1}{2 \pi} \int_{-n}^{n} e^{ikx}\diff k,
\]
\[
	\delta_n(x) = \frac{n}{2} \mathrm{sech}^2 nx
.\]

\subsection{Properties of \texorpdfstring{$\delta(x)$}{Delta function}}%
\label{sub:properties_of_delta_function}

The unit step function or Heaviside function\index{Heaviside function} is
\[
	H(x) =
	\begin{cases}
		1 & x \geq 0,\\
		0 & x < 0,
	\end{cases}
\]
which is the integral of $\delta(x)$, and therefore we can identify $H'(x) = \delta(x)$. We can verify this using limiting distributions of $\delta(x)$.

Define $\delta'(x)$ using integration by parts:
\[
	\int_{-\infty}^{\infty}\delta'(x -\xi)f(x)\diff x = [\delta(x-\xi)f(x)]_{-\infty}^{\infty} - \int_{-\infty}^{\infty}\delta(x - \xi)f'(x)\diff x = - f'(\xi)
,\]
for all $f(x)$ smooth at $x = \xi$. We can verify this by considering the Gaussian approximation, then
\[
	\delta_{\eps}'(x) = \frac{-2x}{\eps^3 \sqrt \pi}e^{-x^2/\eps^2}
.\]

The sampling property\index{sampling property} says that
\[
	\int_{a}^{b}f(x) \delta(x - \xi)\diff x =
	\begin{cases}
		f(\xi) & a < \xi < b, \\
		0 & \text{otherwise}.
	\end{cases}
\]
Moreover, the even property\index{even property} says that
\[
	\int_{-\infty}^{\infty}f(x) \delta(-(x-\xi))\diff x = \int_{-\infty}^{\infty}f(x) \delta(x - \xi) \diff x
,\]
through a change of variables. The scaling property\index{scaling property} says
\[
	\int_{-\infty}^{\infty}f(x) \delta(a(x - \xi)) \diff x = \frac{1}{|a|}f(\xi)
,\]
again shown through a change of variables. In fact, we can extend this definition: Suppose $g(x)$ has $n$ isolated zeroes at $x_1, x_2, \ldots, x_n$ with $g'(x_i) \neq 0$. Then,
\[
	\delta(g(x)) = \sum_{i = 1}^{n} \frac{\delta(x - x_i)}{|g'(x_i)|}
.\]
Finally, the isolation property says that if $g(x)$ is continuous at $x = 0$, then $g(x) \delta(x) = g(0) \delta(x)$.

\subsection{Fourier Series Expansion of Delta Function}%
\label{sub:fourier_series_expansion_of_delta_function}

Consider a complex Fourier series expansion
\[
	\delta(x) = \sum_{n = -\infty}^{\infty}c_n e^{in\pi x/L}, \quad c_n = \frac{1}{2L} \int_{-L}^{L} \delta(x) e^{-in\pi x/L} \diff x = \frac{1}{2L}
.\]
Hence, we can express the delta function
\[
	\delta(x) = \frac{1}{2L} \sum_{n = -\infty}^{\infty}e^{in \pi x/L}
.\]
Letting $f(x)$ be an arbitrary function, we can expand $f(x)$ as $f(x) = \sum d_n e^{in \pi x/L}$. The inner product of $f$ and $\delta$ is given by
\[
	\int_{-L}^{L}f^{\ast}(x) \delta(x) \diff x = \frac{1}{2L} \sum_{n = -\infty}^{\infty}d_n \int_{-L}^{L} e^{in \pi x/L}e^{in \pi x/L}\diff x = \sum_{n = -\infty}^{\infty} d_n = f(0)
.\]
The Fourier expansion of the $\delta$ function can be extended periodically to the whole real line. This infinite set of $\delta$ functions is known as the \textit{Dirac comb}\index{Dirac comb}, given by
\[
	\sum_{m = -\infty}^{\infty} \delta(x - 2mL) = \sum_{n = -\infty}^{\infty} e^{in \pi x/L}
.\]

\subsection{Arbitrary Eigenfunction Expansion of Delta Function}%
\label{sub:arbitrary_eigenfunction_expansion_of_delta_function}

In general, suppose $\delta(x - \zeta) = \sum a_n y_n$, with coefficients
\[
	a_n = \frac{\int_{a}^{n} w(x) y_n(x) \delta(x - \zeta)\diff x}{\int_{a}^{b} w(x) y_n(x)^2 \diff x} = \frac{w(\zeta)y_n(\zeta)}{\int_{a}^{b} w(x) y_n(x)^2\diff x} = w_n(\zeta) Y_n(\zeta)
.\]
Then, the expansion of $\delta$ is
\[
	\delta(x - \zeta) = w(\zeta) \sum_{n = 1}^{\infty} Y_n(\zeta) Y_n(x) = w(x) \sum_{n = 1}^{\infty} Y_n(\zeta)y_n(x)
,\]
from the isolation property. Hence,
\[
	\delta(x - \zeta) = w(x) \sum_{n = 1}^{\infty}\frac{y_n(\zeta)y_n(x)}{N_n}
,\]
where $N_n = \int w y_n^2 \diff x$ is the normalisation factor.

\begin{exbox}
	Consider a Fourier series for $y(0) = y(1) = 0$, with $y_n(x) = \sin n \pi x$. From the sine series coefficient expansion,
	\[
		\delta(x - \zeta) = 2 \sum_{n = 1}^{\infty} \sin n \pi \zeta \sin n \pi x
	,\]
	for $0 < \zeta < 1$.
\end{exbox}

\newpage

\section{Green's Functions}%
\label{sec:green_s_functions}

\subsection{Motivation for Green's Functions}%
\label{sub:motivation_for_green_s_functions}

Consider a massive static string with tension $T$ and linear mass density $\mu$, suspended between fixed ends $y(0) = y(1) = 0$. By resolving forces, we have the time independent form
\[
T \frac{\Diff 2 y}{\diff x^2} - \mu g = 0
.\]
Integrating directly, we find that
\[
y = \frac{\mu g}{2T}x^2 + k_1x + k_2
.\]
Imposing boundary conditions,
\[
	y(x) = \biggl( - \frac{\mu g}{T} \biggr) \cdot \frac{1}{2} x(1 - x)
.\]
That was one way to obtain the solution. Alternatively, we may solve the equation for a single point mass, and superimpose the resulting solution to find the overall solution.

Our single point has mass $\delta m = \mu \delta x$, and is at position $x = \zeta$. The solution to the ODE will then be two straight lines, joining $(0, 0), (1, 0)$ and the mass $(\zeta_i, y_i(\zeta_i))$. If the angle of these straight lines from the horizontal are $\theta_1, \theta_2$, we can resolve the vertical forces:
\begin{align*}
	0 &= T(\sin \theta_1 + \sin \theta_2) - \delta mg = T \biggl( \frac{-y_i}{\zeta_i} + \frac{-y_i}{1 - \zeta_i} \biggr) - \delta mg, \\
	\implies y_i(\zeta_i) &= \frac{- \delta mg}{T} \zeta_i(1 - \zeta_i).
\end{align*}
This is a generalised sawtooth. Alternatively, we can write this as $f_i(\zeta)G(x, \zeta)$, where $f_i$ is a source term, and $G(x, \zeta)$ is the Green's function, which is the solution for a unit point source. As the differential equation is linear, we may sum the solution to find
\[
	y(x) = \sum_{i = 1}^{N} f_i(\zeta)G(x, \zeta_i)
.\]
Taking the limit,
\[
	f_i(\zeta) = \frac{-\delta mg}{T} = \frac{- \mu \delta x g}{T} = f(x) \diff x \implies f(x) = \frac{- \mu g}{T}
.\]
We can thus write
\[
	y(x) = \int_{0}^{1}f(\zeta)G(x, \zeta) \diff \zeta
.\]
If we substitute our calculated values,
\begin{align*}
	y(x) &= \biggl( \frac{- \mu g}{T} \biggr) \biggl[ \int_{0}^{x} \zeta(1 - x)\diff \zeta + \int_{x}^{1}x(1 - \zeta)\diff \zeta \biggr] \\
	     &= \biggl( \frac{- \mu g}{T} \biggr) \left\{ \left[\frac{\zeta^2}{2}(1 - x)\right]_{0}^{x} + \left[x\bigl(\zeta - \frac{\zeta^2}{2} \bigr) \right]_{x}^{1} \right\} \\
	     &= \biggl( \frac{- \mu g}{T} \biggr) \biggl( \frac{x^2}{2} (1 - x) + \frac{x}{2} - x \biggl( x - \frac{x^2}{2} \biggr) \biggr) \\
	     &= \biggl( \frac{-\mu g}{T} \biggr) \cdot \frac{1}{2} x(1 - x),
\end{align*}
which is the same solution as earlier. The benefit of using Green's function is that direct integration may not be possible in all cases, and so Green's functions may have to be used.

\subsection{Definition of Green's Function}%
\label{sub:definition_of_green_s_function}

Suppose we wish to solve the inhomogeneous ODE
\[
	\mathcal{L} y = \alpha(x) '' + \beta(x) y' + \gamma(x) y = f(x)
,\]
on the interval $a \leq x \leq b$, where $\alpha \not \equiv 0$ and $\alpha, \beta, \gamma$ are continuous and bounded, taking homogeneous boundary conditions $y(a) = y(b) = 0$. The Green's function for $\mathcal{L}$ is defined to be the solution for a unit point source at $x = \zeta$. That is, $G(x, \zeta)$ is the function that satisfies the boundary conditions, and
\[
	\mathcal{L}G(x, \zeta) = \delta(x - \zeta)
,\]
with $G(a, \zeta) = G(b, \zeta) = 0$. By linearity, the general solution is
\[
	y(x) = \int_{a}^{b} f(\zeta)G(x, \zeta) \diff \zeta
,\]
where $y(x)$ satisfies the homogeneous boundary conditions. Indeed,
\[
	\mathcal{L}y = \int_{a}^{b}\mathcal{L}G(x, \zeta)f(\zeta)\diff \zeta = \int_{a}^{b}\delta(x - \zeta)f(\zeta)\diff \zeta = f(x)
.\]
Hence the solution is given by the inverse operator $y = \mathcal{L}^{-1} f$, where
\[
	\mathcal{L}^{-1} = \int_{a}^{b} \diff \zeta G(x, \zeta)
.\]
We can split the Green's function into two parts:
\[
	G(x, \zeta) =
	\begin{cases}
		G_1(x, \zeta) & a \leq x < \zeta, \\
		G_2(x, \zeta) & \zeta < x \leq b.
	\end{cases}
\]
For all $x \neq \zeta$, we have $\mathcal{L}G_1 = \mathcal{L}G_2 = 0$, so the parts are homogeneous solutions. Since $G$ satisfies the homogeneous boundary conditions $G_1(a, \zeta) = G_2(b, \zeta) = 0$. Moreover, $G$ must be continuous at $x = \zeta$, so $G_1(\zeta, \zeta) = G_2(\zeta, \zeta)$.

Since $\mathcal{L}G = \delta(x - \zeta)$, $G$ must have a jump condition. For a second order ODE, this implies the derivative of $G$ is discontinuous at $x = \zeta$. Thus, we must have
	\[
		[G']_{\zeta_{-}}^{\zeta_{+}} = \left. \frac{\diff G_2}{\diff x} \right|_{x = \zeta_{+}} - \left.\frac{\diff G_1}{\diff x} \right|_{x = \zeta_{+}} = \frac{1}{\alpha(\zeta)}
	.\]

Hence, solving
\[
	\mathcal{L}G(x, \zeta) = \delta(x - \zeta)
\]
on $a \leq x \leq b$, we have functions $G_1, G_2$ which satisfy the homogeneous equation, so $\mathcal{L}G_i = 0$. Suppose there are two independent homogeneous solutions $y_1(x), y_2(x)$ to $\mathcal{L}y = 0$. Then $G_1 = Ay_1 + By_2$ satisfies $Ay_1(a) + By_2(a) = 0$, constraining $A$ and $B$. Thus there is one complementary function $y_{-}(x)$ such that $y_{-}(a) = 0$. Similarly, we can define $y_{+}$ as a linear combination of $y_1, y_2$ with $y_{+}(b) = 0$. Letting
\[
G_1 = Cy_{-}, \quad G_2 = Dy_{+}
,\]
we impose our other boundary conditions. Since $G_1(\zeta, \zeta) = G_2(\zeta, \zeta)$, we have
\[
	Cy_{-}(\zeta) = Dy_{+}(\zeta)
.\]
Moreover, the jump condition implies
\[
	Dy_{+}'(\zeta) - Cy_{-}'(\zeta) = \frac{1}{\alpha(\zeta)}
.\]
Solving these equation, we find
\[
	C(\zeta) = \frac{y_{+}(\zeta)}{\alpha(\zeta) W(\zeta)}, \quad D(\zeta) = \frac{y_{-}(\zeta)}{\alpha(\zeta)W(\zeta)}
,\]
where $W(\zeta)$ is the Wronskian\index{Wronskian}
\[
	W(\zeta) = y_{-}(\zeta)y_{+}'(\zeta) - y_{+}(\zeta)y'_{-}(\zeta)
,\]
and is non-zero	if $y_{-}$, $y_{+}$ are linearly independent. Hence,
\[
	G(x, \zeta) =
	\begin{dcases}
		\frac{y_{-}(x)y_{+}(\zeta)}{\alpha(\zeta)W(\zeta)} & a \leq x \leq \zeta, \\
		\frac{y_{-}(\zeta)y_{+}(x)}{\alpha(\zeta)W(\zeta)} & \zeta \leq x \leq b.
	\end{dcases}
\]

By linearity, the solution of $\mathcal{L}y = f$ is
\[
	y(x) = \int_{a}^{b} G(x, \zeta)f(\zeta) \diff \zeta
.\]
Split this into two intervals such that $G = G_1$ for $\zeta > x$ and $G = G_2$ for $\zeta < x$: this allows us to compute
\begin{align*}
	y(x) &= \int_{a}^{x} G_2(x, \zeta) f(\zeta) \diff \zeta + \int_{x}^{b}G_1(x, \zeta)f(\zeta) \diff \zeta \\
	     &= y_{+}(x) \int_{a}^{x} \frac{y_{-}(\zeta) f(\zeta)}{\alpha(\zeta)W(\zeta)}\diff \zeta + y_{-}(x) \int_{x}^{b} \frac{y_{+}(\zeta) f(\zeta)}{\alpha(\zeta)W(\zeta)} \diff \zeta.
\end{align*}
If $\mathcal{L}$ is in Sturm-Liouville form, $\beta = \alpha'$. Then, the denominator $\alpha(\zeta)W(\zeta)$ is constant. Notice also $G$ is symmetric, so $G(x, \zeta) = G(\zeta, x)$. Often we take $\alpha = 1$.

\begin{exbox}
	Consider $y'' - y = f(x)$ with $y(0) = y(1) = 0$. The homogeneous solutions are $y_1 = e^{x}$ and $y_2 = e^{-x}$. Imposing boundary conditions,
	\[
	G =
	\begin{cases}
		C \sinh x & 0 \leq x < \zeta, \\
		D \sinh (1 - x) & \zeta < x \leq b.
	\end{cases}
	\]
	Continuity at $x = \zeta$ implies
	\[
		C \sinh \zeta - D \sinh (1 - \zeta) \implies C = D \frac{\sinh (1 - \zeta)}{\sinh \zeta}
	.\]
	The jump condition gives
	\[
		-D \cosh (1 - \zeta) - C \cosh \zeta = 1
	.\]
	Solving simultaneously, we get
	\[
		C = \frac{- \sinh(1 - \zeta)}{\sinh 1}, \quad D = \frac{\sinh \zeta}{\sinh 1}
	.\]
	Therefore,
	\[
		y(x) = \frac{-\sinh(1 - x)}{\sinh 1} \int_{0}^{x} \sinh \zeta f(\zeta) \diff \zeta - \frac{\sinh x}{\sinh 1} \int_{x}^{1} \sinh (1 - \zeta)f(\zeta) \diff \zeta
	.\]
\end{exbox}

If we are given inhomogeneous boundary conditions, we wish to find a particular integral $y_p$ that is homogeneous and which solves the inhomogeneous boundary conditions. Then subtracting this solution from the original equation, we are reduced to solving for homogeneous boundary conditions.

For example, in the above $y_p=  \frac{\sinh x}{\sinh 1}$ is a homogeneous solution to $y(0) = 0$, $y(1) = 1$.

\subsection{Higher-order ODEs}%
\label{sub:higher_order_odes}

Suppose $\mathcal{L}y = f(x)$, where $\mathcal{L}$ is an $n$'th order linear differential operator, and $\alpha(x)$ is the coefficient for the highest derivative. Suppose that we are given homogeneous boundary conditions. Then we can define the Green's function to be the function that solves
\[
	\mathcal{L}G(x, \zeta) = \delta(x - \zeta)
,\]
and which has properties:
\begin{enumerate}[(i)]
	\item $G_1$, $G_2$ are homogeneous solutions satisfying the homogeneous boundary conditions;
	\item $G_1^{(k)}(\zeta) = G_2^{(k)}(\zeta)$ for all $k \in \{0, 1, \ldots, n-2\}$;
	\item $G_2^{(n-1)}(\zeta_{+}) - G_1^{(n-1)}(\zeta_{-}) = \frac{1}{\alpha(z)}$.
\end{enumerate}

\subsection{Eigenfunction Expansion of Green's Functions}%
\label{sub:eigenfunction_expansion_of_green_s_functions}

Suppose $\mathcal{L}$ is in Sturm-Liouville form, with eigenfunctions $y_n(x)$ and eigenvalues $\lambda_n$. We seek an eigenfunction expansion
\[
	G(x, \xi) = \sum_{n = 1}^{\infty}A_n y_n(x),
\]
where $\mathcal{L}G = \delta(x - \xi)$. Evaluating $\mathcal{L}G$ directly,
\begin{align*}
	\mathcal{L}G &= \sum_{n = 1}^{\infty} A_n \mathcal{L}y_n(x) = \sum_{n = 1}^{\infty} A_n \lambda_n w(x) y_n(x) \\
		     &= \delta(x = \xi) = \omega(x) \sum_{n = 1}^{\infty} \frac{y_n(\xi) y_n(x)}{\mathcal{N}_n}.
\end{align*}
Hence we have $A_n(\xi) = y_n(\xi)/(\lambda_n \mathcal{N}_n)$. Thus,
\[
	G(x, \xi) = \sum_{n = 1}^{\infty}\frac{y_n(\xi)y_n(x)}{\lambda_n \mathcal{N}_n} = \sum_{n = 1}^{\infty} \frac{Y_n(\xi) Y_n(x)}{\lambda_n}
.\]

\subsection{Green's Functions for Initial Value Problems}%
\label{sub:green_s_functions_for_initial_value_problems}

Suppose we want to solve $\mathcal{L}(y) = f(t)$, a second-order ODE, for $t \geq a$, subject to boundary conditions $y(a) = y'(a) = 0$.

Again, we can use Green's functions $G(t, \tau)$ satisfying $\mathcal{L}G = \delta(t - \tau)$.

\begin{itemize}
	\item For $t < \tau$, $G_1 = Ay_1(t) + By_2(t)$, with boundary conditions
		\begin{align*}
			Ay_1(a) + By_2(a) &= 0, \\
			Ay_1'(a) + By_2'(a) &= 0.
		\end{align*}
		This corresponds to $W(a) = 0$, so if $y_1, y_2$ are not linearly dependent, then $A = B = 0$ and $G_1(t, \tau) = 0$.
	\item For $t > \tau$, by continuity, $G_2(\tau, \tau) = 0$. Hence we can choose $G_2 = D y_{+}(t)$, with $y_{+}(t) = Ay_1(t) + By_2(t)$ satisfying $y_{+}(t) = 0$.
\end{itemize}

But by the jump condition, we must have
\[
	[G']_{\tau_{-}}^{\tau_{+}} = G_2'(\tau, \tau) - G_1'(\tau, \tau) = D y_{+}'(\tau) = \frac{1}{\alpha(\tau)}
.\]
Hence we get $D(\tau) = (\alpha(\tau)y_{+}'(\tau))^{-1}$. This gives solution
\[
	G(t, \tau) =
	\begin{dcases}
		0 & t < \tau, \\
		\frac{y_{+}(t)}{\alpha(\tau) y_{+}'(t)} & t \geq \tau.
	\end{dcases}
\]

The IVP is
\[
	y(t) = \int_{a}^{t}G(t, \tau) f(\tau) \diff \tau = \int_{a}^{t} \frac{y_{+}(t) f(\tau)}{\alpha(\tau) y_{+}'(\tau)} \diff \tau
.\]

As we can see, the causality in this solution is ``built-in'': only forces which begin acting prior to a time $t$ can impact the solution at time $t$.

\begin{exbox}
	Suppose we want to solve $y'' - y = f(t)$ with $y(0) = y'(0) = 0$. Then, solving for $G(t, \tau)$:
	\begin{itemize}
		\item At $t < \tau$, $G_1 \equiv 0$.
		\item At $t > \tau$, $G_2 = Ae^{t} + Be^{-t}$.
	\end{itemize}
	By continuity, $G_2 = D \sinh(t - \tau)$, and by the jump condition,
	\[
		[G']_{\tau_{-}}^{\tau_{+}} = \frac{1}{\alpha} = 1 \implies G_2'(\tau, \tau) = D \cosh(0) = 1
	.\]
	So $D = 1$, and the general solution is
	\[
		y(t) = \int_{0}^{t}f(\tau) \sinh (t - \tau) \diff \tau
	.\]
\end{exbox}

\newpage

\section{Fourier Transforms}%
\label{sec:fourier_transforms}

\subsection{Introduction}%
\label{sub:introduction}

\begin{definition}
	The \textit{Fourier transform}\index{Fourier transform} (FT) of a function $f(x)$ is
	\[
		\tilde f(k) = \mathcal{F}(f)(k) = \int_{-\infty}^{\infty}f(x) e^{-ikx}\diff x
	,\]
	and the \textit{inverse Fourier transform} is
	\[
		f(x) = \mathcal{F}^{-1}(\tilde f)(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde f(k) e^{ikx}\diff k
	.\]
\end{definition}

The \textit{Fourier inversion theorem}\index{Fourier inversion theorem} states that
\[
	\mathcal{F}^{-1}(\mathcal{F}(f))(x) = f(x)
,\]
with the sufficient condition that $f$ and $\tilde f$ are \textit{absolutely integrable}\index{absolutely integrable}
\[
	\int_{-\infty}^{\infty}|f(x)| \diff x = M < \infty
.\]

\begin{exbox}[Fourier Transform of Gaussian]
	Take a Gaussian $f(x) = \frac{1}{\sigma \sqrt \pi} e^{-x^2/\sigma^2}$. Then,
	\[
		\tilde f(k) = \frac{1}{\sigma \sqrt \pi} \int_{-\infty}^{\infty} e^{-x^2/\sigma^2}e^{-ikx} \diff x = \frac{1}{\sigma \sqrt \pi} \int_{-\infty}^{\infty} e^{-x^2/\sigma^2} \cos kx \diff x
	,\]
	since the odd part $i \sin kx$ disappears. Consider the derivative of $\tilde f$:
	\begin{align*}
		\tilde f'(k) &= \frac{-1}{\sigma \sqrt \pi} \int_{-\infty}^{\infty} x e^{-x^2/\sigma^2} \sin kx \diff x \\
			     &= \frac{1}{\sigma \sqrt \pi} \biggl[ \frac{\sigma^2}{2} e^{-x^2/\sigma^2} \sin kx \biggr]_{-\infty}^{\infty} - \frac{1}{\sigma \sqrt \pi} \int_{-\infty}^{\infty} \biggl( \frac{k \sigma^2}{2} e^{-x^2/\sigma^2}\biggr) \cos kx \diff x \\
			     &= - \frac{k \sigma^2}{2} \tilde f(k).
	\end{align*}
	This gives $\tilde f(k) = C e^{- k^2 \sigma^2/4}$. But $k = 0$ gives $\tilde f (0) = 1$, so $C = 1$ and so
	\[
		\tilde f(k) = \exp \biggl( - \frac{k^2\sigma^2}{4} \biggr)
	.\]
	We can then show that $\mathcal{F}^{-1} \tilde f = f$.
\end{exbox}

\begin{exbox}
	We can show that $f(x) = e^{-a|x|}$ for $a > 0$ has Fourier transform $\tilde f(k) = 2a/(a^2 + k^2)$ in two ways:
	\begin{enumerate}[(i)]
		\item Integrate by parts
			\[
			2 \int_{0}^{\infty} e^{-ax} \cos k x \diff x
			.\]
		\item Integrate directly
			\[
				\int_{0}^{\infty} e^{-(a-ik)x}\diff x + \int_{-\infty}^{0}e^{(a + ik)x}\diff x
			.\]
	\end{enumerate}
	If $f(x)=  e^{-ax}$ for $x > 0$, and $0$ for $x \leq 0$, then we can show that $\tilde f(k) = (ik+a)^{-1}$.
\end{exbox}

\subsection{Fourier Transforms and Fourier Series}%
\label{sub:fourier_transforms_and_fourier_series}

We can write a Fourier series as
\[
	f(x) = \sum_{n = -\infty}^{\infty} c_n e^{i k_n x}
,\]
where $k_n = (n\pi)/2 = n \Delta k$ where $\Delta k = \pi/2$. Then,
\[
	c_n = \frac{1}{2L} \int_{-L}^{L} f(x) e^{-ik_n x}\diff x = \frac{\Delta k}{2 \pi} \int_{-L}^{L} f(x) e^{-ik_nx}\diff x
.\]

\newpage

\printindex

\end{document}
