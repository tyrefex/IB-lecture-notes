\documentclass[12pt]{article}

\usepackage{ishn}

\makeindex[intoc]

\begin{document}

\hypersetup{pageanchor=false}
\begin{titlepage}
	\begin{center}
		\vspace*{1em}
		\Huge
		\textbf{IB Methods}

		\vspace{1em}
		\large
		Ishan Nath, Michaelmas 2022

		\vspace{1.5em}

		\Large

		Based on Lectures by Prof. Edward Shellard

		\vspace{1em}

		\large
		\today
	\end{center}
	
\end{titlepage}
\hypersetup{pageanchor=true}

\tableofcontents

\newpage

\part{Self-Adjoint ODE'S}%
\label{prt:self_adjoint_ode_s}

\section{Fourier Series}%
\label{sec:fourier_series}

\subsection{Periodic Functions}%
\label{sub:periodic_functions}

A function $f(x)$ is \textbf{periodic}\index{periodic function} if
\[
	f(x + T) = f(x)
,\]
where $T$ is the period.

\begin{exbox}
	Consider simple harmonic motion. We have
	\[
	y = A \sin \omega t
	,\]
	where $A$ is the amplitude and the period $T = 2 \pi / \omega$, with angular frequency $\omega$.
\end{exbox}

Consider the set of functions
\[
	g_n(x) = \cos \frac{n \pi x}{L}, \quad h_n(x) = \sin \frac{n \pi x}{L}
,\]
which are periodic on the interval $0 \leq x < 2L$. Recall the identities
\begin{align*}
	\cos A \cos B &= \frac{1}{2} \left(\cos(A - B) + \cos (A + B) \right), \\
	\sin A \sin B &= \frac{1}{2} \left( \cos(A - B) - \cos(A + B) \right), \\
	\sin A \cos B &=  \frac{1}{2} \left( \sin(A - B) + \sin(A + B) \right).
\end{align*}

Define the \textbf{inner product}\index{inner product of periodic functions} for two periodic functions $f, g$ on the interval $[0, 2L)$ 
\[
	\langle f, g \rangle = \int_{0}^{2L}f(x) g(x)\diff x
.\]
I claim that the functions $g_n, h_m$ are \textbf{mutually orthogonal}. Indeed,
\begin{align*}
	\langle h_n, h_m \rangle &= \int_{0}^{2L} \sin \frac{n \pi x}{L} \sin \frac{m \pi x}{L}\diff x \\
				 &= \frac{1}{2} \int_{0}^{2L} \left( \cos \frac{(n - m)\pi x}{L} - \cos \frac{(n + m)\pi x}{L}\right)\diff x \\
				 &= \frac{1}{2} \frac{L}{\pi} \left[ \frac{\sin (n - m) \pi x/L}{n - m} - \frac{\sin (n + m) \pi x/L}{n + m} \right]_{0}^{2L} = 0.
\end{align*}
This works for $n \neq m$. For $n = m$,
\begin{align*}
	\langle h_n, h_n \rangle &= \int_{0}^{2L} \sin^2 \frac{n \pi x}{L}\diff x \\
				 &= \frac{1}{2} \int_{0}^{2L} \left( 1 - \cos \frac{2 \pi n x}{L} \right)\diff x \\
				 &= L \;\; (n \neq 0).
\end{align*}
Hence, we can put these together to get
\[
	\langle h_n, h_m \rangle =
	\begin{cases}
		L \delta_{nm}, & \forall\,\! n, m \neq 0, \\
		0, & n = 0.
	\end{cases}
\] 
Similarly, we can show
\[
	\langle g_n, g_m \rangle =
	\begin{cases}
		L \delta_{nm}, & \forall\,\! n, m \neq 0, \\
		2L \delta_{0n}, &m = 0.
	\end{cases}
	\quad \text{ and } \; \langle h_n, g_m \rangle = 0
.\]

\subsection{Definition of Fourier series}%
\label{sub:definition_of_fourier_series}\index{Fourier series}

We can express any `well-behaved' periodic function $f(x)$ with period $2L$ as
\[
	f(x) = \frac{1}{2}a_0  + \sum_{n = 1}^{\infty} a_n \cos \frac{n \pi x}{L} + \sum_{n = 1}^{\infty}b_n \sin \frac{n \pi x}{L}
,\]

where $a_n, b_n$ are constant such that the right hand side is convergent for all $x$ where $f$ is continuous. At a discontinuity $x$, the Fourier series approaches the midpoint
\[
	\frac{1}{2} \left( f(x_{+}) + f(x_{-}) \right)
.\]
\subsubsection{Fourier Coefficients}%
\label{subsub:fourier_coefficients}\index{Fourier coefficients}

Consider the inner product
\[
	\langle h_m(x), f(x) \rangle = \int_{0}^{2L} \sin \frac{m \pi x}{L} f(x)\diff x = L b_m
,\]
by the orthogonality relations. Hence we find that
\begin{align*}
	b_n &= \frac{1}{L} \int_{0}^{2L}f(x) \sin \frac{n \pi x}{L}\diff x, \\
	a_n &= \frac{1}{L} \int_{0}^{2L}f(x) \cos \frac{n \pi x}{L}\diff x.
\end{align*}
\begin{remark}
	\begin{enumerate}[(i)]
		\item[]
		\item $a_n$ includes $n = 0$, since $\frac{1}{2} a_0$ is the \textbf{average}
			\[
				\langle f(x) \rangle = \frac{1}{2L} \int_{0}^{2L} f(x)\diff x
			.\]
		\item The range of integration is over one period, so we may take the integral over $[0, 2L)$ or $[-L, L)$.
		\item We can think of the Fourier series as a decomposition into harmonics. The simplest Fourier series are the sine and cosine functions.
	\end{enumerate}	
\end{remark}

\begin{exbox}[Sawtooth wave]
	Consider the function $f(x) = x$ for $-L \leq x < L$, periodic with period $T = 2L$. The cosine coefficients are
	\[
	a_n = \frac{1}{L} \int_{-L}^{L} x \cos \frac{n \pi x}{L}\diff x = 0
	,\]
	as $x \cos \omega x$ is odd. The sine coefficients are
	\begin{align*}
		b_n &= \frac{2}{L} \int_{0}^{L} x \sin \frac{n \pi x}{L}\diff x \\
		    &= -\frac{2}{n \pi} \left[ x \cos \frac{n \pi x}{L} \right]_{0}^{L} + \frac{2}{n \pi} \int_{0}^{L} \cos \frac{n \pi x}{L}\diff x \\
		    &= - \frac{2L}{n \pi} \cos n \pi + \frac{2L}{(n \pi)^2} \sin n \pi = \frac{2L}{n \pi }(-1)^{n+1}.
	\end{align*}
	So the sawtooth Fourier series is
	\begin{align*}
		f(x) &= \frac{2L}{\pi} \sum_{n = 1}^{\infty} \frac{(-1)^{n+1}}{n} \sin \frac{n \pi x}{L} \\
		     &= \frac{2L}{\pi} \left( \sin \frac{\pi x}{L} - \frac{1}{2} \sin \frac{2 \pi x}{L} + \frac{1}{3} \sin \frac{3 \pi x}{L} - \cdots \right).
	\end{align*}
\end{exbox}

With Fourier series, we can construct functions with only finitely many discontinuities, the topologist's sine curve, and the Weierstrass function.

\subsection{The Dirichlet Conditions (Fourier's theorem)}%
\label{sub:the_dirichlet_conditions_fourier_s_theorem_}\index{Dirichlet conditions}

These are sufficiency conditions for a ``well-behaved'' function to have a unique Fourier series:

\begin{proposition}
	If $f(x)$ is a bounded periodic function (period $2L$) with a finite number of minima, maxima and discontinuities in $0 \leq x < 2L$, then the Fourier series converges to  $f(x)$ at all points where $f$ is continuous; at discontinuities the series converges to the midpoint.
\end{proposition}

\begin{remark}
	\begin{enumerate}[(i)]
		\item[]
		\item These are weak conditions (in contrast to Taylor series), but pathological functions are excluded, such as
			\[
				f(x) = \frac{1}{x}, \quad f(x) = \sin \frac{1}{x}, \quad f(x) =
				\begin{cases}
					0 & x \in \mathbb{Q},\\
					1 & x \not \in \mathbb{Q}.
				\end{cases}
			\]
		\item The converse is not true.
		\item The proof is difficult.
	\end{enumerate}
	
\end{remark}

\subsubsection{Convergence of Fourier Series}%
\label{subsub:convergence_of_fourier_series}

\begin{theorem}
	If $f(x)$ has continuous derivatives up to the $p$'th derivative, which is discontinuous, then the Fourier series converges as $\mathcal{O}(n^{-(p+1)})$.
\end{theorem}

\begin{exbox}
	Take the square wave\index{square wave}, with $p = 0$.
	\[
		f(x) =
		\begin{cases}
			1 & 0 \leq x < 1, \\
			-1 & -1 \leq x < 0.
		\end{cases}
	\]
	The Fourier series is
	\[
		f(x) = 4 \sum_{m = 1}^{\infty} \frac{\sin (2m - 1) \pi x}{(2m - 1)\pi}
	.\]
	We now look at the general ``see-saw'' wave\index{see-saw wave}, with $p = 1$. Here
	\[
		f(x) =
		\begin{cases}
			x(1 - \xi) & 0 \leq x < \xi, \\
			\xi(1 - x) & \xi \leq x < 1
		\end{cases}
		\text{ on } 0 \leq x < 1,
	\]
	and odd for $-1 \leq x < 0$. The Fourier series is
	\[
		f(x) = 2 \sum_{n = 1}^{\infty} \frac{\sin n \pi \xi \sin n \pi x}{(n \pi)^2}
	.\]
	For $\xi = 1/2$, we have
	\[
		f(x) = 2 \sum_{m = 1}^{\infty}(-1)^{m+1} \frac{\sin (2m - 1) \pi x}{((2m - 1)\pi)^2}
	.\]
	For $p = 2$, take $f(x) = x(1-x)/2$ on $0 \leq x < 1$, and odd for $-1 \leq x < 0$. The Fourier series is
	\[
		f(x) = 4 \sum_{m = 1}^{\infty} \frac{\sin (2m - 1) \pi x}{((2m - 1)\pi)^3}
	.\]
	Consider $f(x) = (1 - x^2)^2$, for $p = 3$. Then $a_n = \mathcal{O}(n^{-4})$.
\end{exbox}

\subsubsection{Integration of Fourier Series}%
\label{subsub:integration_of_fourier_series}

It is always valid to integrate the Fourier series of $f(x)$ term-by-term to obtain
\[
	F(x) = \int_{-L}^{x} f(x)\diff x
,\]
because $F(x)$ satisfies the Dirichlet conditions if $f(x)$ does.

\subsubsection{Differentiation of Fourier Series}%
\label{subsub:differentiation_of_fourier_series}

Differentiation needs to be done with great care. Consider the square wave. We differentiate it to get
\[
	f'(x) = 4 \sum_{m = 1}^{\infty} \cos (2m - 1) \pi x
.\]
But this is unbounded.

\begin{theorem}
	If $f(x)$ is continuous and satisfies the Dirichlet conditions, and $f'(x)$ satisfies the Dirichlet conditions, then $f'(x)$ can be found by term-by-term differentiation of the Fourier series of $f(x)$.
\end{theorem}

\begin{exbox}
	If we differentiate the see-saw with $\xi = 1/2$, then we get an offset square wave.
\end{exbox}

\subsection{Parseval's Theorem}%
\label{sub:parseval_s_theorem}\index{Parseval's theorem}

This gives the relation between the integral of the square of a function and the sum of the squares of the Fourier coefficients:

\begin{align*}
	\int_{0}^{2L}[f(x)]^2\diff x &= \int_{0}^{2L}\diff x \left[ \frac{1}{2} a_0 + \sum_{n}a_n \cos \frac{n \pi x}{L} + \sum_{n}b_n \sin \frac{n \pi x}{L} \right]^2 \\
				   &= \int_{0}^{2L}\diff x \left[ \frac{1}{4} a_0^2 + \sum_{n} a_n^2 \cos^2 \frac{n \pi x}{L} + \sum_{n} b_n^2 \sin^2 \frac{n \pi x}{L} \right]\\
				   &= L \left[ \frac{1}{2} a_0^2 + \sum_{n = 1}^{\infty} (a_n^2 + b_n^2) \right].
\end{align*}

This is also called the \textbf{completeness relation}\index{completeness relation} because the left hand side is always greater than equal to the right hand side if any basis is missing.

\begin{exbox}
	Take the sawtooth wave. We have
	\[
	LHS = \int_{-L}^{L} x^2\diff x = \frac{2}{3}L^3
	,\]
	\[
	RHS = L \sum_{n = 1}^{\infty} \frac{4L^2}{n^2 \pi^2} = \frac{4L^3}{\pi ^2}\sum_{n = 1}^{\infty} \frac{1}{n^2}
	.\]
	Therefore, we obtain
	\[
	\sum_{n = 1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}
	.\]
\end{exbox}

\subsection{Alternative Fourier Series}%
\label{sub:alternative_fourier_series}

\subsubsection{Half-range Series}%
\label{subsub:half_range_series}\index{half-range series}

Consider $f(x)$ defined only on $0 \leq x < L$. Then we can extend its range over $- L \leq x < L$ in two simple ways:
\begin{enumerate}[(i)]
	\item Require it to be odd, so $f(-x) = -f(x)$. Then $a_n = 0$, and
		\[
			b_n = \frac{2}{L} \int_{0}^{L} \sin \frac{n \pi x}{L}\diff x
		.\]
		This is a Fourier sine series.
	\item Require it to be even, so $f(-x) = f(x)$. Then $b_n = 0$,
		\[
			a_n = \frac{2}{L} \int_{0}^{L}f(x) \cos \frac{n \pi x}{L}\diff x
		.\]
		This is a Fourier cosine series.
\end{enumerate}

\subsubsection{Complex Representation}%
\label{subsub:complex_representation}

Recall that
\[
	\cos \frac{n \pi x}{L} = \frac{1}{2} \left( e^{i n \pi x/L} + e^{-i n \pi x/L}\right), \quad \sin \frac{n \pi x}{L} = \frac{1}{2i} \left(e^{i n \pi x/L} - e^{-i n \pi x/L}\right)
.\]
So our Fourier series becomes
\begin{align*}
	f(x) &= \frac{1}{2} a_0 + \sum_{n = 1}^{\infty}a_n \cos \frac{n \pi x}{L} + \sum_{n = 1}^{\infty} b_n \sin \frac{n \pi x}{L} \\
	     &= \frac{1}{2} a_0 + \frac{1}{2} \sum_{n = 1}^{\infty}(a_n - i b_n) e^{i n \pi x/L} + \frac{1}{2} \sum_{n = 1}^{\infty}(a_n + i b_n) e^{-i n \pi x/L} \\
	     &= \sum_{m = -\infty}^{\infty} c_m e^{i m \pi x/L}.
\end{align*}
The coefficients $c_m$ satisfy
\[
c_m =
\begin{cases}
	\frac{1}{2}(a_{m} - ib_{m}) & m > 0, \\
	\frac{1}{2} a_0 & m = 0, \\
	\frac{1}{2}(a_{-m} + i b_{-m}) & m < 0.
\end{cases}
\]
Equivalently,
\[
	c_m = \frac{1}{2L}\int_{-L}^{L}f(x) e^{-i m \pi x/L}\diff x
.\]
Our inner product in the complex representation is
\[
	\langle f, g \rangle = \int f^{\ast}g \diff x
.\]
This is orthogonal, as
\[
\int_{-L}^{L} e^{-i m \pi x/L}e^{i n \pi x/L} \diff x = 2L \delta_{mn}
,\]
and satisfies Parseval's theorem as a result:
\[
	\int_{-L}^{L}|f(x)|^2\diff x = 2L \sum_{m = -\infty}^{\infty}|c_m|^2
.\]

\subsection{Fourier Series Motivations}%
\label{sub:fourier_series_motivations}

\subsubsection{Self-adjoint matrices}%
\label{subsub:self_adjoint_matrices}

Suppose $\mathbf{u}, \mathbf{v}$ are complex $N$-vectors with inner product $\langle \mathbf{u}, \mathbf{v} \rangle = \mathbf{u}^{\dagger} \mathbf{v}$. Then matrix $A$ is self-adjoint\index{self-adjoint matrix} (or Hermitian\index{Hermitian matrix}) if
\[
	\langle A \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{u}, A \mathbf{v} \rangle \implies A^{\dagger} = A
.\]
The eigenvalues $\lambda_1, \ldots, \lambda_N$ of $A$ satisfy the following properties:
\begin{enumerate}[(i)]
	\item The eigenvalues are real: $\lambda_n^{\ast} = \lambda_n$.
	\item If $\lambda_n \neq \lambda_m$, then their respective eigenvectors are orthogonal: $\langle \mathbf{v}_n, \mathbf{v}_m \rangle = 0$.
	\item If we rescale our eigenvectors then $\{\mathbf{v}_1, \ldots, \mathbf{v}_N\}$ form an orthonormal basis.
\end{enumerate}

Given $\mathbf{b}$, we can try to solve for $\mathbf{x}$ in $A \mathbf{x} = \mathbf{b}$. Express 
\[
	\mathbf{b} = \sum_{n = 1}^{N} b_n \mathbf{v}_n, \quad \mathbf{x} = \sum_{n = 1}^{N} c_n \mathbf{v}_n
.\]
Substituting into the equation,
\begin{align*}
	A \mathbf{x} &= \sum_{n = 1}^{N} A c_n \mathbf{v}_n = \sum_{n = 1}^{N} c_n \lambda_n \mathbf{v}_n, \\
	\mathbf{b} &= \sum_{n = 1}^{N} b_n \mathbf{v}_n.
\end{align*}
Equating and using orthogonality,
\[
c_n \lambda_n = b_n \implies c_n = \frac{b_n}{\lambda_n}
.\]
Hence the solution is
\[
\mathbf{x} = \sum_{n = 1}^{N}\frac{b_n}{\lambda_n} \mathbf{v}_n
.\]

\subsubsection{Solving inhomogeneous ODE with Fourier series}%
\label{subsub:solving_inhomogeneous_ode_with_fourier_series}

Take the following problem: We wish to find $y(x)$ given $f(x)$ for which
\[
	\mathcal{L}(y) = - \frac{\Diff2 y}{\diff x^2} = f(x)
,\]
subject to the boundary conditions $y(0) = y(L) = 0$. The related eigenvalue problem is 
\[
	\mathcal{L}y_n = \lambda_n y_n, \quad y_n(0) = y_n(L) = 0
.\]
This has eigenfunctions and eigenvalues
\[
	y_n(x) = \sin \frac{n \pi x}{L}, \quad \lambda_n = \left( \frac{n \pi}{L} \right)^2
.\]
Note that $\mathcal{L}$ is a self-adjoint ODE with orthogonal eigenfunctions. Thus we seek solutions as a half-range sine series. We try
\[
	y(x) = \sum_{n = 1}^{\infty} c_n \sin \frac{n \pi x}{L},
\]
and expand
\[
	f(x) = \sum_{n = 1}^{\infty} b_n \sin \frac{n \pi x}{L}
.\]
Substituting this in,
\begin{align*}
	\mathcal{L}y &= - \frac{\Diff2}{\diff x^2} \left( \sum_{n} c_n \sin \frac{n \pi x}{L} \right) = \sum_{n = 1}^{\infty} c_n \left(\frac{n \pi}{L} \right)^2 \sin \frac{n \pi x}{L} \\
		     &= \sum_{n = 1}^{\infty} b_n \sin \frac{n \pi x}{L}.
\end{align*}
By orthogonality, we have
\[
	c_n \left( \frac{n \pi}{L}\right)^2 = b_n \implies c_n = \left( \frac{L}{n \pi} \right)^2
.\]
Thus the solution is
\[
	y(x) = \sum_{n = 1}^{\infty} \left( \frac{L}{n \pi}\right)^2 b_n \sin \frac{n \pi x}{L} = \sum_{n = 1}^{\infty} \frac{b_n}{\lambda_n} y_n
.\]
This is similar to a self-adjoint matrix.

\begin{exbox}
	Consider the square wave on $L = 1$, as an odd function. This has Fourier series
	\[
		f(x) = 4 \sum_{m} \frac{\sin (2m - 1)\pi x}{(2m - 1)\pi}
	.\]
	So the solution should be
	\[
		y(x) = \sum \frac{b_n}{\lambda_n} y_n = 4 \sum_{m} \frac{\sin (2m - 1)\pi x}{((2m -1)\pi)^3}
	.\]
	This is the Fourier series for $y(x) = x(1-x)/2$.
\end{exbox}

\newpage

\section{Sturm-Liouville theory}%
\label{sec:sturm_liouville_theory}

\subsection{Second-order linear ODEs}%
\label{sub:second_order_linear_odes}

We wish to solve a general inhomogeneous ODE
\[
	\mathcal{L}y = \alpha(x) y'' + \beta(x) y' + \gamma(x) y = f(x)
.\]
\begin{itemize}
	\item The \textbf{homogeneous} equation $\mathcal{L} y = 0$ has two independent solutions $y_1(x)$, $y_2(x)$. The \textbf{complementary function}\index{complementary function} $y_c(x)$ is the general solution of
		\[
			y_c(x) = A y_1(x) +B y_2(x)
		,\]
		where $A, B$ are constants.
	\item The \textbf{inhomogeneous} equation $\mathcal{L}y = f(x)$ has a special solution, the \textbf{particular integral}\index{particular integral} $y_p(x)$. The general solution is then
		\[
			y(x) = y_p(x) + Ay_1(x) + By_2(x)
		.\]
	\item Two \textbf{boundary} or \textbf{initial} conditions are required to determine $A, B$:
		\begin{enumerate}[(a)]
			\item \textbf{Boundary conditions}\index{boundary conditions} require us to solve the equation on $a < x < b$ given $y$ at $x = a, b$ (Dirichlet conditions), or given $y'$ at $x = a, b$ (Neumann conditions), or given a mixed value $y + ky'$. Boundary conditions are often assumed to be $y(a) = y(b)$, to admit the trivial solution $y \equiv 0$. This can be done by adding complementary functions
				\[
				\tilde y = y + A_1 y_1 + B y_2
				.\]
			\item \textbf{Initial condition}\index{initial conditions} require us to solve the equation for $x \geq a$, given $y$ and $y'$ at $x = a$.
		\end{enumerate}
		
\end{itemize}

\subsubsection{General eigenvalue problem}%
\label{subsub:general_eigenvalue_problem}

To solve the equation employing eigenfunction expansion, we are required to solve the related eigenvalue problem
\[
	\alpha(x) y'' + \beta(x) y' + \gamma(x) y = - \lambda \rho(x) y
,\]
with specified boundary conditions. This forms often occurs in higher dimensions, after separation of variables.

\subsection{Self-adjoint operators}%
\label{sub:self_adjoint_operators}

For two complex-valued functions $f, g$ on $a \leq x \leq b$, we can define the \textbf{inner product}\index{inner product of complex-valued functions}
\[
	\langle f, g \rangle = \int_{a}^{b} f^{\ast}(x) g(x)\diff x
.\]
The norm is then $\|f\| = \langle f, f \rangle^{1/2}$.

\subsubsection{Sturm-Liouville equation}%
\label{subsub:sturm_liouville_equation}

The eigenvalue problem greatly simplifies if $\mathcal{L}$ is \textbf{self-adjoint}, that is, it can be expressed in \textbf{Sturm-Liouville form}\index{Sturm-Liouville form}
\[
	\mathcal{L} y \equiv - (\rho y')' + q y = \lambda \omega y
,\]
where the \textbf{weight function}\index{weight function} $\omega(x)$ is non-negative. We can convert to Sturm-Liouville form by multiplying by an integrating factor $F(x)$ to find
\[
F \alpha y'' + F \beta y' + F \gamma y = - \lambda F \rho y
.\]
This gives
\[
	\frac{\diff}{\diff x} (F \alpha y') - F' \alpha y' - F \alpha' y' + F \beta y' + F \gamma y = - \lambda F \rho y
.\]
Eliminating $y'$ terms, we require
\[
	F' \alpha = F(\beta - \alpha') \implies \frac{F'}{F} = \frac{\beta - \alpha'}{\alpha}
.\]
Solving, we get
\[
	F(x) = \exp \left( \int^{x} \frac{(\beta - \alpha')}{\alpha} \diff x \right)
,\]
and $(F \alpha y')' + F \gamma y = - \lambda F \rho y$. So $\rho(x) = F(x) \alpha (x)$, $q(x) = - F(x) \gamma(x)$, and $\omega(x) = F(x) \rho(x)$. This is non-negative as $F(x) > 0$.

\begin{exbox}
	Take the Hermite equation
	\[
	y'' - 2xy' + 2n y = 0
	.\]
	Putting this into Sturm-Liouville form, we have $\alpha = 1$, $\beta - 2x$, $\gamma = 0$ and $\lambda \rho = 2n$. Thus we take
	\[
		F = \exp \left( \int^{x} \frac{-2x}{2} \diff x \right) = e^{-x^2}
	.\]
	Hence
	\[
		\mathcal{L} y \equiv -(e^{-x^2} y')' = 2n e^{-x^2}y
	.\]
\end{exbox}

\subsubsection{Self-adjoint definition}%
\label{subsub:self_adjoint_definition}

A linear operator $\mathcal{L}$ is \textbf{self-adjoint}\index{self-adjoint operator} on $a \leq x \leq b$ for all pairs of functions $y_1, y_2$ satisfying boundary conditions, if
\[
	\langle y_1, \mathcal{L}y_2 \rangle = \langle \mathcal{L}y_1, y_2 \rangle
,\]
or
\[
	\int_{a}^{b} y^{\ast}_1(x) \mathcal{L}y_2(x)\diff x = \int_{a}^{b} (\mathcal{L}y_1(x))^{\ast} y_2(x) \diff x
.\]

Substituting the Sturm-Liouville form into this equation gives
\begin{align*}
	\langle y_1, \mathcal{L}y_2 \rangle - \langle \mathcal{L}y_1, y_2 \rangle &= \int_{a}^{b} [-y_1(\rho y_2')' + y_1qy_2 + y_2(\rho y_1')' - y_2 qy_1]\diff x \\
										  &= \int_{a}^{b} [-(\rho y_1 y_2')' + (\rho y_1' y_2)']\diff x \\
										  &= \left[ - \rho y_1 y_2' + \rho y_1' y_2 \right]_{a}^{b} = 0.
\end{align*}
for given boundary conditions at $x = a, b$. Suitable boundary conditions include:
\begin{itemize}
	\item $y(a) = y(b) = 0$, $y'(a) = y'(b) = 0$, or mixed boundary condition $y + ky' = 0$;
	\item Periodic functions $y(a) = y(b)$;
	\item Singular points of the ODE $\rho(a) = \rho(b) = 0$;
	\item Combinations of the above.
\end{itemize}

\subsection{Properties of self-adjoint operators}%
\label{sub:properties_of_self_adjoint_operators}

Self-adjoint operators satisfy many similar properties to self-adjoint matrices:
\begin{enumerate}[1.]
	\item The eigenvalues $\lambda_n$ are real.
	\item The eigenfunctions $y_n$ are orthogonal.
	\item The eigenfunctions $y_n$ form a complete set.
\end{enumerate}

\begin{proofbox}
\begin{enumerate}[1.]
	\item Given $\mathcal{L}y_n = \lambda_n \omega y_n$, we take the complex conjugate $\mathcal{L}y_n^{\ast} = \lambda_n^{\ast} \omega y_n^{\ast}$. Then,
		\[
			0 = \int_{a}^{b} (y_n^{\ast} \mathcal{L}y_n - (\mathcal{L}y_n^{\ast}) y_n)\diff x = (\lambda_n - \lambda_n^{\ast}) \int_{a}^{b} \omega y_n y_n^{\ast}\diff x
		.\]
		But the right hand side is non-zero, unless $\lambda_n = \lambda_n^{\ast}$, so the eigenvalues are real.
	\item Consider two eigenfunctions $\mathcal{L}y_m = \lambda_m \omega y_m$, $\mathcal{L}y_n = \lambda_n \omega y_n$. Then
		\[
			0 = \int_{a}^{b}(y_m \mathcal{L}y_n - y_n \mathcal{L}y_m) \diff x = (\lambda_n - \lambda_n) \int_{a}^{b} \omega y_n y_m \diff x
		.\]
		Since $\lambda_m \neq \lambda_n$, we get
		\[
		\int_{a}^{b} \omega y_n y_m \diff x = 0
		.\]
		We say $y_n, y_m$ are orthogonal with respect to the weight function $\omega(x)$ on the interval $a \leq x \leq b$. Define the inner product with respect to the weight $\omega(x)$ as
		\[
			\langle f, g \rangle_{\omega} = \int_{a}^{b} \omega(x) f^{\ast}(x) g(x) \diff x = \langle \omega f, g \rangle = \langle f, \omega g\rangle
		.\]
	\item Completeness implies we can approximate any well-behaved function $f(x)$ on $a \leq x \leq b$ by the series
		\[
			f(x) = \sum_{n = 1}^{\infty} a_n y_n(x)
		.\]
	To find the expansion coefficients we consider
		\[
			\int_{a}^{b} \omega(x) y_m(x) f(x) \diff x = \sum_{n = 1}^{\infty} a_n \int_{a}^{b} \omega y_n y_m \diff x = a_m \int_{a}^{b} \omega y_m^2\diff x
		.\]
		Hence
		\[
			a_n = \frac{\int_{a}^{b} \omega(x) y_n(x) f(x) \diff x}{\int_{a}^{b}\omega(x) y_n^2(x)\diff x}
		.\]
		Normally, we have normalized eigenfunctions\index{normalized eigenfunction}, where we take
		\[
			Y_n(x) = \frac{y_n(x)}{\left( \int_{a}^{b}\omega y_n^2 \diff x \right)^{1/2}}
		.\]
		This gives $\langle Y_n, Y_m \rangle_{\omega} = \delta_{nm}$, so
		\[
			f(x) = \sum_{n = 1}^{\infty} A_n Y_n(x)
		, \text{ where }
		A_n = \int_{a}^{b} \omega Y_n f \diff x
		.\]
\end{enumerate}
\end{proofbox}

\begin{exbox}
	Recall the Fourier Series in Sturm-Liouville form
	\[
	\mathcal{L}y_n = -\frac{\Diff2 y_n}{\diff x^2} = \lambda_n y_n
	,\]
	with $\lambda_n = (n\pi/L)^2$ by orthogonality relations.
\end{exbox}

\subsection{Completeness and Parseval's Identity}%
\label{sub:completeness_and_parseval_s_identity}

Consider
\begin{align*}
	\int_{a}^{b}\left[ f(x) - \sum_{n = 1}^{\infty} a_n y_n \right]^2\omega \diff x &= \int_{a}^{b} \left[f^2 - 2f \sum_{n}a_n y_n + \sum_{n}a_n^2 y_n^2\right] \omega \diff x \\
											&= \int_{a}^{b}\omega f^2 \diff x - \sum_{n = 1}^{\infty}a_n^2 \int_{a}^{b}\omega y_n^2 \diff x,
\end{align*}
which follows from
\[
\int_{a}^{b} f y_n \omega \diff x = a_n \int_{a}^{b} \omega y_n^2 \diff x
.\]
Hence if the eigenfunctions are \textbf{complete} then the series converges, and we get
\[
\int_{a}^{b} \omega f^2 \diff x = \sum_{n = 1}^{\infty} a_n^2 \int_{a}^{b} \omega y_n^2 \diff x = \sum_{n = 1}^{\infty} A_n^2
.\]
We also get \textbf{Bessel's inequality}\index{Bessel's inequality}, by looking at what happens if some eigenfunctions are missing:
\[
\int_{a}^{b} \omega f^2 \diff x \geq \sum_{n = 1}^{\infty} A_n^2
.\]
We define the partial sums
\[
	S_N(x) = \sum_{n = 1}^{N}a_n y_n
.\]
The error in the partial sum
\[
	\epsilon_N = \int_{a}^{b} \omega[f(x) - S_N(x)]^2\diff x \to 0
.\]
is minimized by the sequence defined as above, as
\begin{align*}
	\frac{\partial \epsilon_N}{\partial a_n} &= \frac{\partial}{\partial a_n} \left[ \int_{a}^{b} \omega \left[f(x) - \sum_{n = 1}^{N}a_n y_n\right]^2 \diff x \right] \\
						 &= - 2 \int_{a}^{b} y_n \omega \left[f - \sum_{n = 1}^{N} a_n y_n\right]\diff x \\
						 &= -2 \int_{a}^{b}(\omega f y_n - a_n \omega y_n^2)\diff x = 0.
\end{align*}

\subsection{Legendre Polynomials}%
\label{sub:legendre_polynomials}

Consider Legendre's equation\index{Legendre's equation} arising from spherical polar coordinates
\[
	(1 - x^2)y'' - 2xy' + \lambda y = 0
\]
on the interval $-1 \leq x \leq 1$ with $y$ finite at $x = \pm 1$. This is in Sturm-Liouville form with $\rho = 1-x^2$, $q = 0$, $\omega = 1$. To solve, we seek a power series about $x = 0$. Let
\[
y = \sum_{n = 0}^{\infty} c_n x^{n}
.\]
Then substituting,
\[
	(1 - x^2)\sum_{n = 0}^{\infty} n(n-1) c_n x^{n-2} - 2x \sum_{n = 0}^{\infty} n c_n x^{n-1} + \lambda \sum_{n = 0}^{\infty} c_n x^{n} = 0
.\]
Equating powers of $x^{n}$, we get
\[
	(n+2)(n+1)c_{n+2} - n(n-1)c_n - 2n c_n + \lambda c_n = 0
,\]
\[
	\implies c_{n+2} = \frac{n(n+1) - \lambda}{(n+1)(n+2)} c_n
.\]
So specifying $c_0, c_1$ gives two independent solutions,
\begin{align*}
	y_{even} &= c_0 \left[ 1 + \frac{(-\lambda)}{2!}x^2 + \frac{(6 - \lambda)(-\lambda)}{4!}x^{4} + \cdots \right], \\
	y_{odd} &= c_1 \left[x + \frac{(2 - \lambda)}{3!}x^3 + \frac{(12 - \lambda)(2 - \lambda)}{5!}x^{5} + \cdots \right].
\end{align*}
But as $n \to \infty$, the ratio of terms tends to $1$, so the radius of convergence is $|x| < 1$. This means this series is divergent at $x = \pm 1$.

However, we can use finiteness to our advantage. Take $\lambda = l(l+1)$ with $l$ an integer. Then one of the series terminates. These \textbf{Legendre polynomials}\index{Legendre polynomials} $P_l(x)$ are eigenfunctions on $-1 \leq x \leq 1$ with normalization convention $P_l(1) = 1$. The first few values are
\begin{align*}
	l &= 0, & \lambda &= 0, & P_0(x) &= 1, \\
	l &= 1, & \lambda &= 2, & P_1(x) &= x, \\
	l &= 2, & \lambda &= 6, & P_2(x) &= (3x^2-1)/2, \\
	l &= 3, & \lambda &= 12, & P_3(x) &= (5x^3 - 3x)/2.
\end{align*}
As these are in Sturm-Liouville form, we get
\[
\int_{-1}^{1}P_n P_m\diff x = 0, \quad \int_{-1}^{1}P_n^2\diff x = \frac{2}{2n+1}
.\]
The normalization can be proven with Rodrigues' formula
\[
	P_n(x) = \frac{1}{2^{n}x!} \left( \frac{\diff}{\diff x} \right)^{n} (x^2 - 1)^{n}
.\]
We can also take the generating function
\[
	\sum_{n = 0}^{\infty} P_n(x)t^{n} = \frac{1}{\sqrt{1 - 2xt + t^2}}
,\]
then using the binomial expansion gives $P_n$. We also have recursive formulas
\begin{align*}
	(l+1)P_{l+1}(x) &= (2l+1)xP_l(x) - lP_{l-1}(x), \\
	(2l+1)P_{l}(x) &= \frac{d}{dx} (P_{l+1}(x) - P_{l-1}(x)).
\end{align*}
The Legendre polynomials are complete, so any function on $-1 \leq x \leq 1$ can be expressed as
\[
	f(x) = \sum_{l = 0}^{\infty} a_l P_l(x)
,\]
where
\[
	a_l = \frac{2l+1}{2} \int_{-1}^{1}f(x) P_n(x)\diff x
.\]

\subsection{Sturm-Liouville Theory and Inhomogeneous ODEs}%
\label{sub:sturm_liouville_theory_and_inhomogeneous_odes}

Consider the inhomogeneous ODE on $a \leq x \leq b$:
\[
	\mathcal{L}y = f(x) - \omega(x) F(x)
.\]
Given eigenfunctions $y_n(x)$ satisfying
\begin{align*}
	\mathcal{L}y_n & \lambda_n \omega y_n, \\
	y(x) &= \sum_{n} c_n y_n(x), \\
	F(x) &= \sum_{n}a_n y_n(x),
\end{align*}
we can find the coefficients
\[
a_n = \int_{a}^{b} \omega F y_n \diff x / \int_{a}^{b} \omega y_n^2\diff x
.\]
Substituting this, we have
\[
\mathcal{L}y = \mathcal{L}\sum_{n}c_n y_n = \sum_{n} c_n \lambda_n \omega y_n = \omega \sum_{n} a_n y_n
.\]
Hence, by orthogonality, $c_n \lambda_n = a_n$, giving
\[
	y(x) = \sum_{n = 1}^{\infty} \frac{a_n}{\lambda_n} y_n(x)
.\]
This assumes $\lambda_n \neq 0$.

Generalizing, if we have a linear response term, as is often induced by a driving force,
\[
	\mathcal{L}y - \tilde \lambda \omega y = f(x)
.\]
The solution becomes
\[
	y(x) = \sum_{n = 1}^{\infty} \frac{a_n}{\lambda_n - \tilde \lambda} y_n(x)
.\]
This assumes $\tilde \lambda \neq \lambda_n$.

\subsubsection{Integral solution and Green's function}%
\label{subsub:integral_solution_and_green_s_function}

Recall that
\[
	y(x) = \sum_{n = 1}^{\infty}\frac{a_n}{\lambda_n} y_n(x) = \sum_{n} \frac{y_n(x)}{\lambda_n \mathcal{N}} \int_{a}^{b} \omega(\xi) F(\xi) y_n(\xi)\diff \xi
,\]
where $\mathcal{N} = \int \omega y_n^2 \diff x$. Then, we can continue rewriting as
\[
	\int_{a}^{b} \sum_{n = 1}^{\infty} \frac{y_n(x) y_n(\xi)}{\lambda_n \mathcal{N}_n} \omega(\xi) F(\xi)\diff \xi = \int_{a}^{b}G(x, \xi) f(\xi)\diff \xi
,\]
where
\[
	G(x, \xi) = \sum_{n = 1}^{\infty} \frac{y_n(x) y_n(\xi)}{\lambda_n \mathcal{N}_n}
\]
is the eigenfunction expansion of the Green's function\index{Green's function}. Note that $G(x, \xi)$ depends only on $\mathcal{L}$ and the boundary conditions, not on the forcing term $f(x)$: it acts like $\mathcal{L}^{-1}$.

\newpage

\part{PDEs on Bounded Domains}%
\label{prt:pdes_on_bounded_domains}

\section{The Wave Equation}%
\label{sec:the_wave_equation}

\subsection{Waves on an elastic string}%
\label{sub:waves_on_an_elastic_string}

Consider small displacements on a stretched string with fixed ends at $x = 0$ and $x = L$, with boundary conditions $y(0, t) = y(L, t) = 0$, and initial conditions
\[
	y(x, 0) = p(x), \quad \frac{\partial y}{\partial t}(x, 0) = q(x)
.\]
We derive the equation of motion by balancing forces on the segment $(x, x + \delta x)$, and taking $\delta x \to 0$. Then the boundary of the string on the segment induces forces $T_1, T_2$ at angles $\theta_1, \theta_2$ to the horizontal.

Assume that $|\partial y/\partial x| \ll 1$, so $\theta_1$, $\theta_2$ are small.

\begin{itemize}
	\item Resolving in the $x$-direction, $T_1 \cos \theta_1 = T_2 \cos \theta_2$, so $T_1 \approx T_2 = T$. Hence, tension $T$ is a constant independent of $x$, up to $\mathcal{O}(|\partial y/\partial x|^2)$.
	\item Resolving in the $y$-direction,
		\begin{align*}
			F_T &= T_2 \sin \theta_2 - T_1 \sin \theta_1 \approx T \left( \left. \frac{\partial y}{\partial x}\right|_{x + \delta x} \!\!\!\!\!\! - \left. \frac{\partial y}{\partial x} \right|_{x}\right) \\
			    &= T \frac{\partial^2 y}{\partial x^2} \delta x.
		\end{align*}
\end{itemize}

Hence, by Newton's law,
\begin{align*}
	F &= ma = (\mu \delta x) \frac{\partial^2 y}{\partial t^2} = F_T + F_g \\
	&= T \frac{\partial^2 y}{\partial x^2} \delta x - g \mu \delta x,
\end{align*}
where $\mu$ is the mass per unit length. define the wave speed as $c = \sqrt{T/\mu}$, and we find
\[
\frac{\partial^2 y}{\partial t^2} = \frac{T}{\mu} \frac{\partial^2 y}{\partial x^2} - g = c^2 \frac{\partial^2 y}{\partial x^2} - g
.\] 
Assume gravity is negligible. Then we have the 1-dimensional wave equation
\[
\frac{1}{c^2} \frac{\partial^2 y}{\partial t^2} = \frac{\partial^2 y}{\partial x^2}
.\]

\subsection{Separation of Variables}%
\label{sub:separation_of_variables}

We wish to solve the wave equation subject to boundary conditions and initial conditions. Consider possible solution of separable form
\[
	y(x, t) = X(x) T(t)
.\]
Substitute in the wave equation
\[
\frac{1}{c^2} X \ddot T = X'' T \implies \frac{1}{c^2} \frac{\ddot T}{T} = \frac{X''}{X}
.\]
But the left hand side depends only on $t$, while the right hand side depends only on $x$. This means both sides must be equal to a constant $\lambda$. Hence
\begin{align*}
	X'' + \lambda X &= 0, \\
	\ddot T + \lambda c^2 T &= 0.
\end{align*}

\subsection{Boundary Conditions and Normal Modes}%
\label{sub:boundary_conditions_and_normal_modes}

We have three possibilities for $\lambda$.
\begin{enumerate}[(i)]
	\item $\lambda < 0$. We have $\chi^2 = - \lambda$ for the characteristic polynomial, so
		\[
			X(x) = A e^{\chi x} + Be^{-\chi x} = \tilde A \cosh \chi x + \tilde B \sinh \chi x
		.\]
		But the boundary conditions $X(0) = X(L) = 0$ imply $\tilde A = \tilde B = 0$, giving the trivial solution.
	\item $\lambda = 0$. Then $X(x) = Ax + B$, again giving $A = B = 0$.
	\item $\lambda > 0$. Then $X(x) = A \cos \sqrt \lambda x + B \sin \sqrt \lambda L = 0$. Since $X(0) = 0$, $A = 0$, and $X(L) = 0$ gives $\sqrt \lambda L = n \pi$, so
		\[
			X_n(x) = B_n \sin \frac{n \pi x}{L}, \quad \lambda_n = \left( \frac{n \pi}{L}\right)^2
		.\]
		These are the normal modes\index{normal modes} because the spatial shape in $x$ does not change in time.
\end{enumerate}

\subsection{Initial Conditions and Temporal Solutions}%
\label{sub:initial_conditions_and_temporal_solutions}

Substituting the eigenvalues $\lambda_n$ into the time ODE:
\[
\ddot T + \frac{n^2 \pi^2 c^2}{L^2} T = 0
.\]
This gives
\[
	T_n(t) = C_n \cos \frac{n \pi ct}{L} + D_n \sin \frac{n \pi ct}{L}
.\]
Thus a specific solution to the wave equation is
\[
	y_n(x, t) = T_n(t)X_n(x) = \left(c_n \cos \frac{n \pi ct}{L} + D_n \sin \frac{n \pi ct}{L} \right) \sin \frac{n \pi x}{L}
.\]
Since the wave equation and boundary conditions are linear, we can add the solutions together to find the general string solution
\[
	y(x, t) = \sum_{n = 1}^{\infty} \left( c_n \cos \frac{n \pi ct}{L} + D_n \sin \frac{n \pi ct}{L} \right) \sin \frac{n \pi x}{L}
.\]
By construction, this satisfies boundary conditions, so now we need to impose the initial conditions. For $t = 0$, we have
\[
	y(x, 0) = p(x) = \sum_{n = 1}^{\infty} c_n \sin \frac{n \pi x}{L}
,\]
\[
	\frac{\partial y}{\partial t}(x, 0) = q(x) = \sum_{n = 1}^{\infty} \frac{n \pi c}{L} D_n \sin \frac{n \pi x}{L}
.\]
Hence the coefficients are given by a Fourier sine series
\begin{align*}
	C_n &= \frac{2}{L} \int_{0}^{L}p(x) \sin \frac{n \pi x}{L} \diff x, \\
	D_n &= \frac{2}{n \pi c} \int_{0}^{L}q(x) \sin \frac{n \pi x}{L} \diff x.
\end{align*}

\begin{exbox}
	Pluck a string at $x = \xi$, drawing it back as
	\[
		y(x, 0) = \rho(x) =
		\begin{cases}
			x(1 - \xi) & 0 \leq x \leq \xi, \\
			\xi(1 - x) & \xi \leq x \leq 1,
		\end{cases}
	\]
	\[
		\frac{\partial y}{\partial t} (x, 0) = q(x) = 0
	.\]
	Then by Fourier series, $C_n = (2 \sin n \pi \xi)/(n \pi)^2$, $D_n = 0$. Thus we have the solution
	\[
		y(x, t) = \sum_{n = 1}^{\infty} \frac{2}{(n \pi)^2} \sin n \pi \xi \sin n \pi x \cos n \pi c t
	.\]
	Taking $\xi = 1/2$, we get $c_{2m} = 0$, $c_{2m-1} = 2(-1)^{m+1}/((2m-1)\pi)^2$. For a guitar, we typically have $1/4 \leq \xi \leq 1/3$, and for a violin we have $\xi \approx 1/7$.
\end{exbox}

Note, if we recall the sine and cosine summation identities, we can rewrite our solution as
\begin{align*}
	y(x, t) &= \frac{1}{2} \sum_{n = 1}^{\infty}\biggl[ c_n \sin \frac{n \pi}{L} (x - ct) + D_n \cos \frac{n \pi}{L} (x - ct) \\
		&+ C_n \sin \frac{n \pi}{L} (x + ct) + D_n \cos \frac{n \pi}{L} (x + ct) \biggr] = f(x - ct) + g(x + ct)
\end{align*}
The standing wave solution is made up of a right-moving wave and a left-moving wave.

\subsection{Oscillation Energy}%
\label{sub:oscillation_energy}

A vibrating string has kinetic energy due to its motion:
\[
	KE = \frac{1}{2} \mu \int_{0}^{L} \left( \frac{\partial y}{\partial t}\right)^2 \diff x
\]
and potential energy due to stretching
\[
	PE = T \Delta X = T \int_{0}^{L} \left( \sqrt{1 + \left( \frac{\partial y}{\partial x} \right)^2} - 1 \right) \diff x \approx \frac{1}{2} T \int_{0}^{L} \left( \frac{\partial y}{\partial x}\right)^2 \diff x
.\]
The total summed energy becomes (using $c^2 = T/\mu$)
\[
	E = \frac{1}{2} \mu \int_{0}^{L} \left[ \left( \frac{\partial y}{\partial t} \right)^2 + c^2 \left( \frac{\partial y}{\partial x} \right)^2 \right] \diff x
.\]
Substituting and using orthogonality,
\begin{align*}
	E =& \frac{1}{2} \mu \sum_{n = 1}^{\infty} \int_{0}^{L} \biggl[ \left( \frac{- n \pi c}{L} C_n \sin \frac{n \pi c t}{L} + \frac{n \pi c}{L}D_n \cos \frac{n \pi c t}{L} \right)^2 \sin^2 \frac{n \pi x}{L} \\
	  & \quad + c^2 \left( C_n \cos \frac{n \pi c t}{L} + D_n \sin \frac{n \pi c t}{L} \right)^2 \frac{n^2 \pi^2}{L^2} \cos^2 \frac{n \pi x}{L} \biggr]\diff x \\
		=& \frac{1}{4} \mu \sum_{n = 1}^{\infty} \frac{n^2 \pi^2 c^2}{L}(C_n^2 + D_n^2).
\end{align*}
This is the sum of the energy of the normal modes, and is constant, hence energy is conserved in time.

\subsection{Wave Reflection and Transmission}%
\label{sub:wave_reflection_and_transmission}

Recall the travelling wave solution. A simple harmonic traversing wave is
\[
	y = \Re \bigl[ A e^{iw (t - x/c)} \bigr] = |A| \cos \left( w - \left( t - \frac{x}{c} \right) + \phi \right)
,\]
where the phase is $\phi = \arg A$, and the wavelength is $2 \pi c / \omega$. Consider a density discontinuity on a string at $x = 0$, with
\[
\mu =
\begin{cases}
	\mu_{-} & x < 0 \implies c_{-} = \sqrt{T/\mu_{-}}, \\
	\mu_{+} & x > 0 \implies c_{+} = \sqrt{T/\mu_{+}}.
\end{cases}
\]
We will assume constant tension. Consider the incident wave on the junction. $Ae^{i \omega(t - x/c_{-})}$. Then it will split into a reflected wave $Be^{i \omega(t + x/c_{-})}$ and a transmitted wave $De^{i\omega(t - x/c_{+})}$. 

The boundary conditions at $x = 0$ give:
\begin{itemize}
	\item The string does not break, so $y$ is continuous, implying $A + B = D$.
	\item The forces balance, so
		\[
			T \left. \frac{\partial y}{\partial x}\right|_{x = 0_{-}} = T \left. \frac{\partial y}{\partial x} \right|_{x = 0_{+}}
		,\]
		so $\partial y/\partial x$ is continuous for all $t$. Solving, this gives
		\begin{align*}
			- \frac{i \omega A}{c_{-}} + \frac{i \omega B}{c_{-}} &= - \frac{i \omega D}{c_{+}}, \\
			\implies 2A = D + D\frac{c_{-}}{c_{+}} &= \frac{D}{c_{+}}(c_{+} + c_{-}).
		\end{align*}
\end{itemize}
Hence, we have
\[
D = \frac{2 c_{+}}{c_{-} + c_{+}} A, \quad B = \frac{c_{+} - c_{-}}{C_{+} + c_{-}} A
.\]
In general, a different phase shift $\phi$ is possible. We consider the following limiting cases:
\begin{enumerate}[1)]
	\item If the string is continuous, so $c_{-} = c_{+}$, then $D = A$, $B = 0$.
	\item If we have Dirichlet boundary conditions $u_{+}/u_{-} \to \infty$, then $c_{+}/c_{-} \to 0$. This gives $D = 0$, $B = -A$, so total reflection with opposite phase.
	\item If we have Neumann boundary conditions $u_{+}/u_{-} \to 0$, then $c_{+}/c_{-} \to \infty$. This gives $D = 2A$, $B = A$, so total reflection with the same phase.
\end{enumerate}

\subsection{Wave Equation in 2D Plane Polars}%
\label{sub:wave_equation_in_2d_plane_polars}


The 2D wave equation for $u(r, \theta, t)$ is
\[
\frac{1}{c^2} \frac{\partial^2 u}{\partial t^2} = \nabla^2 u
,\]
with boundary conditions at $r = 1$ on a unit disc, often $u(1, \theta, t) = 0$, and initial conditions for $t = 0$:
\[
	u(r, \theta, 0) = \phi(r, \theta), \quad \frac{\partial u}{\partial t}(r, \theta, 0) = \psi (r, \theta)
.\]
First, we try temporal separation. Substitute
\[
	u(r, \theta, t) = T(t) V(r, \theta)
.\]
This gives
\[
\ddot T + \lambda c^2 T = 0, \quad \nabla^2 V + \lambda V = 0
.\]
In polars, this gives
\[
\frac{\partial^2 V}{\partial r^2} + \frac{1}{r} \frac{\partial V}{\partial r} + \frac{1}{r^2} \frac{\partial^2 V}{\partial \theta^2} + \lambda V
.\]
Now we can try spacial separation:
\[
	V(r, \theta) = R(r) \Theta(\theta)
.\]
This gives equations
\[
	\Theta'' + \mu \Theta = 0, \quad r^2 R'' + r R' + (\lambda r^2 - \mu)R = 0
,\]
where $\lambda, \mu$ are separation constants.

The configuration implies periodic boundary conditions, so $\Theta(0) = \Theta(2 \pi)$ with $\mu > 0$, so the eigenvalues are $\mu = m^2$ with solutions
\[
	\Theta_m(\theta) = A_m \cos m \theta + B_m \sin m \theta
.\]
Divide the radial equation by $r$ to bring it into Sturm-Liouville form with $\mu = m^2$:
\[
	\frac{\diff}{\diff r} (r R') - \frac{m^2}{r}R = - \lambda r R
,\]
where $p(r) = R$, $q(r) = m^2/r$, and weight $w(r) = r$. This has self-adjoint boundary conditions with $R(1) = 0$ and bounded at $R(0)$, since $p(0) = 0$ at a regular singular point.

This is known as Bessel's equation\index{Bessel's equation}. Substituting $z = \sqrt{\lambda} r$, we find
\[
	z^2 \frac{\Diff2 R}{\diff z^2} + z \frac{\diff R}{\diff z} + (z^2 - m^2)R = 0
.\]
We can look at a Frobenius solution by substituting a power series
\[
R = z^{p} \sum_{n = 0}^{\infty} a_n z^{n}
,\]
which gives
\[
	\sum_{n} a_n[(n+p)^2 z^{n+p} + z^{n + p + 2} - m^2z^{n+p}] = c
.\]
The indicial equation is $p^2 - m^2 = 0$, so $p = m$ or $p = -m$. We choose $p = m$ to get the regular solution, with recursion relation
\[
	(n + m)^2 a_n + a_{n-2} - m^2a_n = 0 \implies a_n = \frac{-1}{n(n+2m)}a_{n-2}
.\]
Putting $n \to 2n'$, we have
\[
	a_{2n'} = \frac{-1}{4n'(n' + m)}a_{2n'-2}
,\]
so stepping up from $a_0$, we have
\[
	a_{2n} = \frac{(-1)^{n}}{2^{2n}n!(n+m)(n+m-1)\cdots(m+1)}a_0
.\]
Take $a_0 = (2^{m}m!)^{-1}$, to find the Bessel function of the first kind\index{Bessel function of the first kind}
\[
	J_m(z) = \biggl(\frac{z}{2}\biggr)^{m} \sum_{n = 0}^{\infty} \frac{(-1)^{n}}{n!(n+m)!} \biggl( \frac{z}{2} \biggr)^{2n}
.\]

\begin{exbox}
	Using $y = \sqrt z R$ in the Bessel equation, we find
	\[
		y'' + y \biggl( 1 + \frac{1}{4z} - \frac{m^2}{z^2} \biggr) = 0
	.\]
	So as $z \to \infty$, $y'' = -y$, giving solutions
	\[
		R = \frac{1}{\sqrt z}(A \cos z + B \sin z)
	.\]
\end{exbox}

These also work for $m = \mu$ if we replace $(n+m)!$ with $\Gamma(n+m+1)$. We can get a second solution with $p = -m$, which are the Neumann functions\index{Neumann functions}
\[
	Y_m(z) = \lim_{\gamma \to m} \frac{J_{\gamma}(z) \cos (\gamma \pi) - J_{-\gamma}(z)}{\sin \gamma \pi}
.\]

We can show that
\[
	\frac{\diff}{\diff z} (z^{m} J_m(z)) = z^{m}J_{m-1}(z)
,\]
and hence
\[
	J_m'(z) + \frac{m}{z}J_m(z) = J_{m-1}(z)
.\]
Repeating with $z^{-m}$ we can find the recursion relations
\begin{align*}
	J_{m-1}(z) + J_{m+1}(z) &= \frac{2m}{z} J_m(z), \\
	J_{m-1}(z) - J_{m+1}(z) &= 2 J_m'(z).
\end{align*}
For small $z \to 0$, we have
\[
	J_0(z) \to 1, \quad J_m(z) \to \frac{1}{m!} \biggl( \frac{z}{2} \biggr)^{m}
,\]
\[
	Y_0(z) \to \frac{2}{\pi} \log \biggl( \frac{z}{2} \biggr), \quad Y_m(z) \to - \frac{(m-1)!}{\pi} \biggl( \frac{2}{z} \biggr)^{m}
.\]
For large $z \to \infty$, we have oscillatory solutions
\begin{align*}
	J_m(z) &\approx \sqrt{\frac{2}{\pi z}} \cos \biggl( z - \frac{m \pi}{2} - \frac{\pi}{4} \biggr), \\
	Y_m(z) &\approx \sqrt{\frac{2}{\pi z}} \sin \biggl( z - \frac{m \pi}{2} - \frac{\pi}{4} \biggr).
\end{align*}

Define $j_{mn}$ to be the $n$'th solution to the Bessel function $J_m(z)$, so $J_m(j_{mn}) = 0$. Approximating $J_m$, we get
\[
	\cos \biggl( z - \frac{m \pi}{2} - \frac{\pi}{4} \biggr) = 0
\implies z = n \pi + \frac{m \pi}{2} - \frac{\pi}{4} \approx \tilde j_{mn}
.\]
This is accurate within $10\%$.

\subsection{2D Wave Equation Solution}%
\label{sub:2d_wave_equation_solution}

We know the radial solutions to the 2D wave equation are
\[
	R_m(z) = R_m(\sqrt \lambda r) = A J_m(\sqrt \lambda r) + B Y_m(\sqrt \lambda r)
.\]
By imposing regularity at $r = 0$, we get $B = 0$. Moreover, the boundary condition $R = 0$ at $r = 1$ gives $J_m(\sqrt \lambda) = 0$. But then we must get $\lambda_{mn} = j_{mn}^{2}$.

Thus, with the polar mode, the spatial solution is
\[
	V_{mn}(r, \theta) = \Theta_m(\theta) R_{mn}(\sqrt \lambda_{mn} r) = (A_{mn} \cos m \theta + B_{mn} \sin m \theta) J_m(j_{mn} r)
.\]
The temporal solution is $\ddot T = - \lambda c^2 T$, or $T_{mn}(t) = \cos(j_{mn} ct)$ and $\sin (j_{mn} ct)$. Hence we can sum together to obtain our general solution:
\begin{align*}
	u(r, \theta, t) &= \sum_{n = 1}^{\infty}J_0(j_{0n} r) (A_{0n}\cos (j_{0n} ct) + C_{0n} \sin (j_{0n} ct)) \\
			&+ \sum_{m = 1}^{\infty} \sum_{n = 1}^{\infty} J_m(j_{nm}t)(A_{nm} \cos m \theta+ B_{mn} \sin m \theta) \cos (j_{nm} ct) \\
			&+ \sum_{m = 1}^{\infty} \sum_{n = 1}^{\infty} J_m(j_{nm}r)(C_{mn} \cos m \theta + D_{mn} \sin m \theta) \sin (j_{nm} ct).
\end{align*}
Now we impose the initial conditions at $t = 0$:
\begin{align*}
	u(r, \theta, 0) &= \phi(r, \theta) = \sum_{m = 0}^{\infty} \sum_{n = 1}^{\infty} J_m(j_{mn}r) (A_{mn} \cos m \theta + B_{mn} \sin m \theta), \\
	\frac{\partial u}{\partial t}(r, \theta, 0) &= \psi(r, \theta) = \sum_{m = 0}^{\infty} \sum_{n = 1}^{\infty} j_{mn}c J_m(j_{mn} r) (C_{mn} \cos m \theta + D_{mn} \sin m \theta).
\end{align*}
We can find the coefficients by multiplying by $J_m$, $\cos$ and $\sin$, and exploiting orthogonality. The Bessel functions satisfy
\[
	\int_{0}^{1}J_m(j_{mn}r)J_m(j_{mk}r)r \diff r = \frac{1}{2} [J_n'(j_{mn})]^2 \delta_{nk} = \frac{1}{2} [J_{m+1}(j_{mn})]^2\delta_{nk}
.\]
For example,
\[
	\int_{0}^{2\pi}\diff \theta \cos p \theta \int_{0}^{1} r \diff r J_{pq}(j_{pq} r) \phi(r, \theta) = \frac{\pi}{2} [J_{p+1}(j_{pq})]^2 A_{pq}
.\]

\begin{exbox}
	Consider the initial radial profile $u(r, \theta, 0) = \phi(r) = 1 - r^2$, giving $B_{mn} = 0$ and $A_{mn} = 0$ for $m \neq 0$ and $\frac{\partial u}{\partial t} (r, \theta, 0) = 0$. This gives $C_{mn} = D_{mn} = 0$, and we need to find
	\begin{align*}
		A_{0n} &= \frac{2}{[J_1(j_{0n})]^2} \int_{0}^{1}J_0(j_{0n}r)(1 - r^2)r \diff r \\
		       &= \frac{2}{[J_1(j_{0n})]^2} \frac{J_2(j_{0n})}{j_{0n}^2} \approx \frac{J_2(j_{0n})}{n}.
	\end{align*}

\end{exbox}

\newpage

\section{The Diffusion Equation}%
\label{sec:the_diffusion_equation}

\subsection{Physical Origin of Heat Equation}%
\label{sub:physical_origin_of_heat_equation}

This applies to processes that diffuse due to spatial gradients. An early example was Fick's law\index{Fick's law} with flux $J = - D \nabla c$, with concentration $c$ and diffusion coefficient $D$. For heat flow, we have Fourier's law
\[
q = - k \nabla \theta
,\]
where $q$ is the heat flux, $k$ is the thermal conductivity, and $\theta = T$ is the temperature. In a volume $V$, the overall heat energy $Q$ is
\[
Q = \int c_v \rho \theta \diff V
.\]
The rate of change due to heat flow is
\[
\frac{\diff Q}{\diff t} = \int c_v \rho \frac{\partial \theta}{\partial t} \diff V
.\]
Integrating over the surface $S$ enclosing the volume $V$ gives
\[
	-\frac{\diff Q}{\diff t} = \int_{S} q \cdot \hat n \diff S = \int_{S}(k \nabla \theta) \cdot \hat n \diff S = \int (-k \nabla^2 \theta) \diff V
.\]
Equating these, we find
\[
	\int \biggl( c_v \rho \frac{\partial \theta}{\partial t} - k \nabla^2 \theta \biggr) \diff V = 0
.\]
Since this is true for all $V$, the integrand must vanish. This gives
\[
\frac{\partial \theta}{\partial t} - \frac{k}{c_v \rho} \nabla^2 \theta = 0
,\]
so letting $D = k/(c_v \rho)$,
\[
\frac{\partial \theta}{\partial t} = D \nabla^2 \theta
.\]
Einstein also derived this equation in a different manner, using Brownian motion. Gas particles are diffusing by scattering every $\Delta t$, with probability PDF $p(\xi)$ of moving distance $\xi$ with
\[
	\langle \xi \rangle = \int p(\xi) \xi \diff \xi = 0
.\]
Suppose the PDF after $N \Delta t$ steps is $P_{N \Delta t}(x)$, then for the $(N+1)\Delta t$ steps,
\begin{align*}
	P_{(N+1)\Delta t}(x) &= \int_{-\infty}^{\infty}p(\xi) P_{N \Delta t}(x - \xi) \diff \xi \\
			     &\approx \int_{-\infty}^{\infty}p(\xi) \biggl[ P_{N \Delta t}(x) + P_{N \Delta t}'(x)(-\xi) + P_{N \Delta t}''(x) \frac{\xi^2}{2} \biggr] \diff \xi \\
			     &\approx P_{N \Delta t}(x) - P_{N \Delta t}'(x)\langle \xi \rangle + P_{N \Delta t}''(x) \frac{\langle \xi^2 \rangle}{2}.
\end{align*}
Identifying $P_{N \Delta t}(x) = P(x, N \Delta t)$, we have
\[
	P(x, (N + 1) \Delta t) - P(x, N \Delta t) = \frac{\partial ^2}{\partial x^2} P(x, N \Delta t) \frac{\langle \xi^2 \rangle}{2}
.\]
If we assume $\langle \xi^2 \rangle/2 = D \Delta t$, then $\Delta t \to 0$ gives
\[
\frac{\partial P}{\partial t} = D \frac{\partial^2 P}{\partial x^2}
.\]
\subsection{Similarity Solutions}%
\label{sub:similarity_solutions}

The characteristic relation between variance and time suggest seeking solutions with dimensionless parameters:
\[
	\eta \equiv \frac{x}{2 \sqrt{Dt}}
.\]
So we want to find solution $\theta(x, t) = \theta(\eta)$. Changing variable,
\[
	\frac{\partial \theta}{\partial t} = \frac{\partial \eta}{\partial t} \frac{\partial \theta}{\partial \eta} = -\frac{1}{2}\frac{x}{\sqrt D t^{3/2}} \theta' = - \frac{\eta}{2t} \theta'
,\]
\[
	D \frac{\partial^2 \theta}{\partial x^2} = D \frac{\partial}{\partial x} \biggl( \frac{\partial \eta}{\partial x} \frac{\partial \theta}{\partial \eta} \biggr) = D \frac{\partial}{\partial x} \biggl( \frac{1}{2 \sqrt{Dt}} \theta' \biggr) = \frac{1}{4t} \theta''
.\]
Equating, we get
\[
\theta'' = - 2 \eta \theta'
.\]
Take $\psi = \theta'$, then
\[
\frac{\psi'}{\psi} = - 2 \eta \implies \log \psi = - \eta^2 + C
,\]
\[
\implies \psi = \theta' = C e^{-\eta^2}
.\]
Integrating, we find
\[
	\theta = C \frac{2}{\sqrt \pi} \int_{0}^{\eta} e^{-u^2} \diff u = C \erf \biggl( \frac{x}{2 \sqrt{Dt}} \biggr)
,\]
where we define the error function\index{error function} as
\[
\erf z = \frac{2}{\sqrt \pi} \int_{0}^{z} e^{-u^2}\diff u
.\]
This describes discontinuous initial conditions that spread over time.

\subsection{Heat Conduction in a Finite Bar}%
\label{sub:heat_conduction_in_a_finite_bar}

Suppose we have a uniform bar of length $2L$, with $-L \leq x \leq L$ and initial temperature
\[
	\theta(x, 0) = H(x) =
	\begin{cases}
		1 & 0 \leq x \leq L, \\
		0 & -L \leq x < 0.
	\end{cases}
,\]
and boundary conditions $\theta(L, t) = 1$ and $\theta(-L, t) = 0$.

To apply Sturm-Liouville theory, we need homogeneous boundary conditions. Thus we want to transform the boundary condition. THe problem is then to identify a steady state solution which reflects late-time behaviour.

We try $\theta_s(x) = Ax + B$, which satisfies the heat equation. To satisfy the boundary conditions, we take $A = 1/2L$, $B = 1/2$, to get
\[
	\theta_s(x) = \frac{(x + L)}{2L}
.\]
Transforming, we can solve for
\[
	\hat \theta(x, t) = \theta(x, t) - \theta_s(x)
,\]
with homogeneous boundary conditions $\hat \theta(-L, t) = \hat \theta(L, t) = 0$, and boundary conditions $\hat \theta(x, 0) = H(x) - (x+L)/2L$.

We try $\hat \theta(x, t) = X(x)T(t)$, which gives
\[
X'' = - \lambda X, \quad \dot T = - D \lambda T
.\]
The boundary conditions imply $\lambda > $ 0, with
\[
	X(x) = A \cos \sqrt \lambda x + B \sin \sqrt \lambda x
.\]
For the even solutions,
\[
	\cos \sqrt \lambda L = 0 \implies \sqrt{\lambda_m} = \frac{m \pi}{2L}, \; m = 1, 3, 5,\ldots
\]
and the odd solutions give
\[
	\sin \lambda L = 0 \implies \sqrt{\lambda_n} = \frac{n \pi}{L}, \; n = 1, 2, 3, \ldots
\]
but the initial conditions are odd, so we take
\[
X_n = B_n \sin \frac{n \pi x}{L}, \quad \lambda_n = \frac{n^2 \pi^2}{L^2}
.\]
Putting $\lambda_n$ into the equation for time, we find
\[
	T_n(t) = C_n \exp \biggl( - \frac{Dn^2\pi^2}{L^2}t \biggr)
.\]
Hence we can write the general solution with homogeneous boundary conditions as
\[
	\hat \theta(x, t) = \sum_{n = 1}^{\infty}b_n \sin \frac{n \pi x}{L} \exp \biggl( - \frac{D n^2 \pi^2}{L^2} t \biggr)
.\]
Impose the initial conditions at $t = 0$. We get the Fourier coefficients
\begin{align*}
	b_n &= \frac{1}{L} \int_{-L}^{L} \hat \theta(x, 0) \sin \frac{n \pi x}{L}\diff x \\
	    &= \frac{2}{L} \int_{0}^{L} \underbrace{\biggl(H(x) - \frac{1}{2}\biggr)}_{=1/2}\sin \frac{n \pi x}{L} \diff x - \frac{2}{L} \int_{0}^{L} \frac{x}{2l}\sin \frac{n \pi x}{L} \diff x \\
	    &= \underbrace{\frac{2}{(2m - 1)\pi}}_{\text{if }n = 2m - 1} - \frac{(-1)^{n+1}}{n \pi} = \frac{1}{n \pi}.
\end{align*}
So the solution we get is
\[
	\hat \theta(x, t) = \sum_{n = 1}^{\infty} \frac{1}{n \pi} \sin \frac{n \pi x}{L} \exp \biggl(- \frac{Dn^2\pi^2}{L^2}t \biggr)
,\]
or with the original boundary conditions,
\[
	\theta(x, t) = \frac{x+L}{2L} + \hat \theta(x, t)
.\]
The approximate solution is an excellent fit for $\theta$ if $t \ll 1$.

\newpage

\section{The Laplace Equation}%
\label{sec:the_laplace_equation}

We will now look at Laplace's equation
\[
\nabla^2 \phi = 0
.\]
This has wide applications in mathematical physics, applied and pure mathematics.

Laplace's equation is used in
\begin{itemize}
	\item steady state heat flows,
	\item potential theory $\mathbf{F} = - \nabla \phi$,
	\item incompressible fluid flow $\mathbf{v} = \nabla \phi$.
\end{itemize}

We will solve Laplace's equation in a domain subject to two boundary conditions:
\begin{itemize}
	\item[Dirichlet:] $\phi$ is given on the boundary surface $\partial D$.
	\item[Neumann:] $\mathbf{\hat n} \cdot \nabla \phi$ is given on the boundary surface $\partial D$.
\end{itemize}

\subsection{3D Cartesian Coordinates}%
\label{sub:3d_cartesian_coordinates}

In three dimensions, the equation becomes
\[
\frac{\partial^2 \phi}{\partial x^2} + \frac{\partial^2 \phi}{\partial y^2} + \frac{\partial^2 \phi}{\partial z^2} = 0
.\]
We seek separable solutions $\phi(x, y, z) = X(x)Y(y)Z(z)$. Then substituting,
\[
X''YZ + XY''Z + XYZ'' = 0 \implies \frac{X''}{X} = - \frac{Y''}{Y} - \frac{Z''}{Z} = - \lambda_{\ell}
,\]
and similarly we get
\[
\frac{Y''}{Y} = -\lambda_m, \quad \frac{Z''}{Z} = - \lambda_n = \lambda_{\ell} + \lambda_m
.\]
The general solution from the eigenmodes is
\[
	\phi(x, y, z) = \sum_{\ell, m, n} a_{\ell m n} X_{\ell}(x)Y_m(y)Z_n(z)
.\]

\begin{exbox}[Steady heat conduction]
	Setting $\partial \theta/\partial t = 0$ in the heat equation, we get Laplace's equation.

	Consider a semi-infinite rectangular bar with boundary conditions $\phi = 0$ at $x = 0, a$ and $y = 0, b$. Then we can assume $\phi$ stays `hot' near the origin, with $\phi = 1$ at $z = 0$, and gets cold further away, so $\phi \to 0$ as $z \to \infty$.

	We successively solve for the eigenmodes:
	\begin{itemize}
		\item $X'' = - \lambda_{\ell}X$, with $X(0) = X(a) = 0$. Then the solution is
			\[X_{\ell} = \sin \frac{\ell \pi x}{a}, \quad  \lambda_{\ell} = \frac{\ell^2 \pi^2}{a^2}.
			\]
		\item $Y'' = - \lambda_m Y$, with $Y(0) = Y(b) = 0$. Then the solution is
			\[Y_m = \sin \frac{m \pi y}{b}, \quad \lambda_m = \frac{m^2 \pi^2}{b^2}.\]

		\item $Z'' = -\lambda_nZ = (\lambda_\ell + \lambda_m)Z = \pi^2(\frac{\ell^2}{a^2} + \frac{m^2}{b^2})Z$, with boundary conditions $Z \to 0$ as $z \to \infty$. This implies we must have the negative exponential, so
			\[
				Z_{\ell m} = \exp \biggl[ - \biggl( \frac{\ell^2}{a^2} + \frac{m^2}{b^2} \biggr)^{1/2} \pi z \biggr]
			.\]
	\end{itemize}
	Therefore, the general solution becomes
	\[
		\phi(x, y, z) = \sum_{\ell, m}^{\infty}a_{lm} \sin \frac{\ell \pi x}{L} \sin \frac{m \pi x}{L} \exp \biggl[ - \biggl( \frac{\ell^2}{a^2} \biggr)^{1/2}\pi z \biggr]
	.\]
	Finally, we fix $a_{\ell m}$ by using $\phi(x, y, 0) = 1$. Then
	\begin{align*}
		a_{\ell m} &= \frac{2}{b} \int_{0}^{b} \frac{2}{a} \int_{0}^{a} \sin \frac{\ell \pi x}{a} \sin \frac{m \pi y}{b} \diff x \diff y \\
			   &= \underbrace{\frac{4a}{a(2k-1)\pi}}_{\text{if }l = 2k-1} \underbrace{\frac{4b}{b(2p-1)\pi}}_{\text{if }m = 2p-1} = \frac{16}{\pi^2\ell m}.\quad (2 \nmid lm)
	\end{align*}
	Asymptotically, only the small eigenmodes survive.
\end{exbox}

\subsection{2D Plane Polar Coordinates}%
\label{sub:2d_plane_polar_coordinates}

Recall in 2D plane polars,
\[
	\nabla^2 \phi = \frac{1}{r} \frac{\partial}{\partial r} \biggl( r \frac{\partial \phi}{\partial r} \biggr) + \frac{1}{r^2} \frac{\partial^2 \phi}{\partial \theta^2} = 0
.\]
We try to find separable solutions $\phi(r, \theta) = R(r) \Theta(\theta)$. Substituting, we get
\[
	\Theta'' + \mu \Theta = 0, \quad r(rR')' - \mu R = 0
.\]
\begin{itemize}
	\item The polar equation has periodic boundary conditions, hence if $\mu = m^2$,
		\[
			\Theta_m(\theta) = \cos m \theta, \sin m \theta
		.\]
	\item The radial equation is $r(rR')' - m^2R = 0$. Trying $R =  r^{\beta}$, we get
		\[
			r(\beta r^{\beta})' - m^2 r^{\beta} = 0 \implies \beta^2 - m^2 = 0 \implies \beta = \pm m
		.\]
		Hence $R_m = r^{m}$ and $-r^{m}$, $m \neq 0$. If $m = 0$, we have
		\[
			(r R')' = 0 \implies r R' = C \implies R = C \log r + D
		.\]
\end{itemize}
Hence the general solution is
\[
	\phi(r, \theta) = \frac{a_0}{2} + c_0 \log r + \sum_{m = 1}^{\infty}[(a_m \cos m \theta + b_m \sin m \theta)r^{m} + (c_m \cos m \theta + d_m \sin m \theta) r^{-m}]
.\]

\begin{exbox}[Soap film on a unit disk]
	We solve Laplace's equation with a distorted circular wire of radius $r = 1$, and given boundary conditions $\phi(1, \theta) = f(\theta)$, to find $\phi(r, \theta)$ for $r < 1$.

	First, regularity at $r = 0$ implies $c_m = d_m = 0$ for all $m$ (including $0$). So the equation becomes
	\[
		\phi(r, \theta) = \frac{1}{2}a_0 \sum_{m = 1}^{\infty}(a_m \cos m \theta + b_m \sin m \theta)r^{m}
	.\]
	At $r = 1$, we get
	\[
		\phi(1, \theta) = f(\theta) = \frac{1}{2}a_0 + \sum_{m = 1}^{\infty}(a_m \cos m \theta + b_m \sin m \theta)
	.\]
	Therefore the coefficients are the Fourier coefficients,
	\[
		a_m = \frac{1}{\pi} \int_{0}^{2 \pi}f(\theta) \cos m \theta \diff \theta, \quad b_m = \frac{1}{\pi} \int_{0}^{2 \pi}f(\theta) \sin m \theta \diff \theta
	.\]
	Due to the $r^{m}$ term, only the low Fourier modes survive to the center of the disk.
\end{exbox}

\subsection{3D Cylindrical Polar Coordinates}%
\label{sub:3d_cylindrical_polar_coordinates}

In this case, our Laplacian is
\[
	\nabla^2 \phi = \frac{1}{r} \frac{\partial}{\partial r} \biggl( r \frac{\partial \phi}{\partial r} \biggr) + \frac{1}{r^2} \frac{\partial^2 \phi}{\partial \theta^2} + \frac{\partial^2 \phi}{\partial z^2} = 0
.\]
With $\phi = R(r) \Theta(\theta) Z(z)$, we have
\[
	\Theta'' = - \mu \Theta, \quad Z'' = \lambda Z,\quad r(rR')' + (\lambda r^2 - \mu)R = 0
.\]
\begin{itemize}
	\item The polar equation is $\mu_m = m^2, \Theta_m(\theta) = \cos m \theta$ and $\sin m \theta$.
	\item The radial equation is Bessel's equation, with $R = J_m(kr)$ and $Y_m(kr)$. Setting the boundary conditions, $R = 0$ at $r = a$, means $J_m(ka) = 0$. Hence $k = j_{mn}a$. The radial eigenfunction is $R_{mn}(r) = J_m(j_{mn}r/a)$ (we can eliminate the Neumann solutions as they diverge as $r \to 0$).
	\item The $Z$ equation obeys $Z'' = k^2 Z$, implying $Z = e^{-kz}$ and $Z = e^{kz}$ (we usually eliminate $e^{kz}$ with $Z \to 0$ as $z \to \infty)$.
\end{itemize}
Hence the general solution is
\[
	\phi(r, \theta, z) = \sum_{m = 0}^{\infty} \sum_{n = 1}^{\infty} (a_{mn} \cos m \theta + b_{mn} \sin m \theta) J_{m} \biggl( \frac{j_{mn}}{a} r \biggr) e^{-j_{mn}r/a}
.\]

\begin{exbox}
	We describe steady-state heat flow in a semi-infinite circular wire with boundary conditions $\phi = 0$ at $r = a$, $\phi = T_0$ at $z = 0$ and $\phi \to 0$ as $z \to \infty$. The solution is
	\[
		\phi(r, \theta, z) = \sum_{n = 1}^{\infty} \frac{2T_0}{j_{0n}J_1(j_{0n})}J_0\biggl(\frac{j_{0n}}{a}r\biggr)e^{-j_{0n}z/a}
	.\]
\end{exbox}

\subsection{3D Spherical Polar Coordinates}%
\label{sub:3d_spherical_polar_coordinates}

Recall that in spherical polars, $x = r \sin \theta \cos \phi$, $y = r \sin \theta \sin \phi$, $z = r \cos \theta$ and $\diff V = r^2 \sin \theta \diff r \diff \theta \diff \phi$. Laplace's equation becomes
\[
	\frac{1}{r^2}\frac{\partial}{\partial r}\biggl(r^2 \frac{\partial \Phi}{\partial r}\biggr) + \frac{1}{r^2 \sin \theta} \frac{\partial}{\partial \theta} \biggl( \sin \theta \frac{\partial \Phi}{\partial \theta}\biggr) + \frac{1}{r^2\sin^2\theta} \frac{\partial^2\Phi}{\partial \phi^2} = 0
.\]
We look only at the axisymmetric case, where there is no $\phi$ dependence. So we seek separable solutions $\Phi(r, \theta) = R(r) \Theta(\theta)$. Substituting,
\[
	(\sin \theta \Theta)' + \lambda \sin \theta = 0, \quad (r^2R')' - \lambda R = 0
.\]
\begin{itemize}
	\item The polar equation is Legendre's equation. Substitute $x = \cos \theta$, and note $-1 \leq x \leq 1$. Then, since
		\[
		\frac{\diff x}{\diff \theta} = - \sin \theta \implies \frac{\diff \Theta}{\diff \theta} = - \sin \theta \frac{\diff \Theta}{\diff x}
		,\]
		our equation becomes
		\[
			\frac{\diff}{\diff x} \biggl[(1- x^2) \frac{\diff \Theta}{\diff x} \biggr] + \lambda \Theta = 0
		.\]
		This is Legendre's equation with eigenvalue $\lambda_{\ell} = \ell(\ell + 1)$, and eigenfunctions $\Theta_l(\theta) = P_{\ell}(x) = P_{\ell}(\cos \theta)$.
	\item The radial equation is $(r^2R')' - \ell(\ell+1)R = 0$. We seek solution of the form $R = \alpha r^{\beta}$. Then we get $\beta(\beta + 1) - \ell(\ell + 1) = 0$. This has two solution $\beta = \ell$ or $\beta = - \ell - 1$, giving $R_\ell = r^{\ell}$ and $r^{- \ell - 1}$.
\end{itemize}

Hence the general axisymmetric solution is
\[
	\Phi(r, \theta) = \sum_{\ell = 0}^{\infty} (a_{\ell}r^{\ell} + b_{\ell} r^{-\ell - 1}) P_{\ell}(\cos \theta)
,\]
where $a_{\ell}, b_{\ell}$ are determined by boundary conditions, usually at fixed $r = r_0$. Then the orthogonality of $P_{\ell}$ can be used to obtain coefficients.

\begin{exbox}
	We solve $\nabla^2 \Phi = 0$ with axisymmetric boundary conditions at $r = 1$, $\Phi(1, \theta) = f(\theta)$. Regularity implies $b_{\ell} = 0$, so we have
	\[
		f(\theta) = \sum_{\ell = 0}^{\infty} a_{\ell} P_{\ell}(\cos \theta)
	,\]
	or writing $f(\theta) = F(\cos \theta)$,
	\[
		F(x) = \sum_{\ell = 0}^{\infty}a_{\ell} P_{\ell}(x)
	.\]
	This gives
	\[
		a_{\ell} = \frac{(2 \ell + 1)}{2}\int_{-1}^{1}F(x)P_{\ell}(x)\diff x
	.\]
	If $f(\theta) = \sin^2 \theta$, we get the solution
	\[
		\Phi(r, \theta) = \frac{2}{3}(1 - P_2(\cos \theta)r^2)
	.\]
\end{exbox}

\subsection{Generating functions for Legendre Polynomials}%
\label{sub:generating_functions_for_legendre_polynomials}

Consider a charge on the $z$-axis at $\mathbf{r}_0 = (0, 0, 1)$, then the potential at $P$ becomes
\begin{align*}
	\Phi(\mathbf{r}) &= \frac{1}{|\mathbf{r} - \mathbf{r}_0|}= \frac{1}{(x^2 + y^2 + (z - 1)^2)^{1/2}} \\
			 &= \frac{1}{(r^2 \sin^2 \theta + r^2 \cos^2\theta - 2 r \cos \theta + 1)^{1/2}} \\ 
			 &= \frac{1}{\sqrt{r^2 - 2r \cos \theta + 1}} = \frac{1}{\sqrt{r^2 - 2 r \bar x + 1}}.
\end{align*}

Moreover, this $\Phi$ satisfies $\nabla^2 \Phi = 0$, where $\mathbf{r} \neq \mathbf{r}_0$.

We can represent any axisymmetric solution as a sum of Legendre polynomials:
\[
	\frac{1}{\sqrt{r^2 - 2rx + 1}} = \sum_{\ell = 0}^{\infty} a_{\ell} P_{\ell}(x) r^{\ell}
,\]
with normalisation $P_\ell(1) = 1$ at $x = 1$. Therefore
\[
\frac{1}{1 - r} = \sum_{\ell = 0}^{\infty}a_{\ell}r^{\ell}
,\]
which gives $a_{\ell} = 1$. Thus we have found the generating polynomial
\[
	\frac{1}{\sqrt{1 - 2rx + r^2}} = \sum_{\ell = 0}^{\infty} P_{\ell}(x) r^{\ell}
.\]
Then, expanding the left hand side with binomial theorem, we can find $P_{\ell}(x)$. Using this, we can obtain the normalisation condition. Using this, we can obtain the normalisation condition.

\newpage

\part{Inhomogeneous ODEs; Fourier Transforms}%
\label{prt:inhomogeneous_odes_fourier_transforms}

\section{The Dirac Delta Function}%
\label{sec:the_dirac_delta_function}\index{delta function}

\subsection{Definition of \texorpdfstring{$\delta(x)$}{Delta function}}%
\label{sub:definition_of_delta_function}

Define a generalized function $\delta(x - \xi)$ with the following properties:
\begin{align*}
	\delta(x - \xi) = 0\; \forall x \neq \xi, \\
	\int_{-\infty}^{\infty}\delta(x - \xi)\diff x = 1.
\end{align*}
This acts as a linear operator on an arbitrary function $f(x)$ to produce a number $f(\xi)$, that is,
\[
	\int_{-\infty}^{\infty} \diff x \delta(x - \xi) f(x) = f(\xi)
,\]
provided $f(x)$ is well-behaved at $x = \xi$ and as $x \to \pm \infty$.

\begin{remark}
	\begin{itemize}
		\item[]
		\item The delta function $\delta(x)$ is classified as a distribution. 
		\item As such, $\delta(x)$ always appears in an integrand as a linear operator, where it is well-defined.
		\item It represents a unit point source or an impulse.
	\end{itemize}
\end{remark}

\subsubsection{Limiting Distributions}%
\label{subsub:limiting_distributions}

We can define a discrete approximation to the delta function as the limit as $n \to \infty$ of
\[
	\delta_n(x) =
	\begin{cases}
		0 & |x| > \frac{1}{n}, \\
		n/2 & |x| \leq \frac{1}{n}.
	\end{cases}
.\]
However this is not that good of an approximation. Instead, we can try the continuous approximation as $\eps \to 0$ of
\[
	\delta_{\eps}(x) = \frac{1}{\eps \sqrt \pi} e^{-x^2/\eps^2}
.\]
We can verify the properties of this approximation:
\begin{align*}
	\int_{-\infty}^{\infty}f(x) \delta(x) \diff x &= \lim_{\eps \to 0} \int_{-\infty}^{\infty} \frac{1}{\eps \sqrt \pi} e^{-x^2/\eps^2} f(x) \diff x \\
						      &= \lim_{\eps \to 0}\int_{-\infty}^{\infty}\frac{1}{\sqrt \pi} e^{-y^2}f(\eps y) \diff y \\
						      &= \lim_{\eps \to 0} \int_{-\infty}^{\infty} \diff y \frac{1}{\sqrt \pi} e^{-y^2} [f(0) + \eps y f'(0) + \cdots] \\
						      &= f(0),
\end{align*}
for well-behaved $f$ at $x = 0$ and $x = \pm \infty$.

We can consider further examples:
\[
	\delta_n(x) = \frac{\sin n x}{\pi x} = \frac{1}{2 \pi} \int_{-n}^{n} e^{ikx}\diff k,
\]
\[
	\delta_n(x) = \frac{n}{2} \mathrm{sech}^2 nx
.\]

\subsection{Properties of \texorpdfstring{$\delta(x)$}{Delta function}}%
\label{sub:properties_of_delta_function}

The unit step function or Heaviside function\index{Heaviside function} is
\[
	H(x) =
	\begin{cases}
		1 & x \geq 0,\\
		0 & x < 0,
	\end{cases}
\]
which is the integral of $\delta(x)$, and therefore we can identify $H'(x) = \delta(x)$. We can verify this using limiting distributions of $\delta(x)$.

Define $\delta'(x)$ using integration by parts:
\[
	\int_{-\infty}^{\infty}\delta'(x -\xi)f(x)\diff x = [\delta(x-\xi)f(x)]_{-\infty}^{\infty} - \int_{-\infty}^{\infty}\delta(x - \xi)f'(x)\diff x = - f'(\xi)
,\]
for all $f(x)$ smooth at $x = \xi$. We can verify this by considering the Gaussian approximation, then
\[
	\delta_{\eps}'(x) = \frac{-2x}{\eps^3 \sqrt \pi}e^{-x^2/\eps^2}
.\]

The sampling property\index{sampling property} says that
\[
	\int_{a}^{b}f(x) \delta(x - \xi)\diff x =
	\begin{cases}
		f(\xi) & a < \xi < b, \\
		0 & \text{otherwise}.
	\end{cases}
\]
Moreover, the even property\index{even property} says that
\[
	\int_{-\infty}^{\infty}f(x) \delta(-(x-\xi))\diff x = \int_{-\infty}^{\infty}f(x) \delta(x - \xi) \diff x
,\]
through a change of variables. The scaling property\index{scaling property} says
\[
	\int_{-\infty}^{\infty}f(x) \delta(a(x - \xi)) \diff x = \frac{1}{|a|}f(\xi)
,\]
again shown through a change of variables. In fact, we can extend this definition: Suppose $g(x)$ has $n$ isolated zeroes at $x_1, x_2, \ldots, x_n$ with $g'(x_i) \neq 0$. Then,
\[
	\delta(g(x)) = \sum_{i = 1}^{n} \frac{\delta(x - x_i)}{|g'(x_i)|}
.\]
Finally, the isolation property says that if $g(x)$ is continuous at $x = 0$, then $g(x) \delta(x) = g(0) \delta(x)$.

\subsection{Fourier Series Expansion of Delta Function}%
\label{sub:fourier_series_expansion_of_delta_function}

Consider a complex Fourier series expansion
\[
	\delta(x) = \sum_{n = -\infty}^{\infty}c_n e^{in\pi x/L}, \quad c_n = \frac{1}{2L} \int_{-L}^{L} \delta(x) e^{-in\pi x/L} \diff x = \frac{1}{2L}
.\]
Hence, we can express the delta function
\[
	\delta(x) = \frac{1}{2L} \sum_{n = -\infty}^{\infty}e^{in \pi x/L}
.\]
Letting $f(x)$ be an arbitrary function, we can expand $f(x)$ as $f(x) = \sum d_n e^{in \pi x/L}$. The inner product of $f$ and $\delta$ is given by
\[
	\int_{-L}^{L}f^{\ast}(x) \delta(x) \diff x = \frac{1}{2L} \sum_{n = -\infty}^{\infty}d_n \int_{-L}^{L} e^{in \pi x/L}e^{in \pi x/L}\diff x = \sum_{n = -\infty}^{\infty} d_n = f(0)
.\]
The Fourier expansion of the $\delta$ function can be extended periodically to the whole real line. This infinite set of $\delta$ functions is known as the \textit{Dirac comb}\index{Dirac comb}, given by
\[
	\sum_{m = -\infty}^{\infty} \delta(x - 2mL) = \sum_{n = -\infty}^{\infty} e^{in \pi x/L}
.\]

\subsection{Arbitrary Eigenfunction Expansion of Delta Function}%
\label{sub:arbitrary_eigenfunction_expansion_of_delta_function}

In general, suppose $\delta(x - \zeta) = \sum a_n y_n$, with coefficients
\[
	a_n = \frac{\int_{a}^{n} w(x) y_n(x) \delta(x - \zeta)\diff x}{\int_{a}^{b} w(x) y_n(x)^2 \diff x} = \frac{w(\zeta)y_n(\zeta)}{\int_{a}^{b} w(x) y_n(x)^2\diff x} = w_n(\zeta) Y_n(\zeta)
.\]
Then, the expansion of $\delta$ is
\[
	\delta(x - \zeta) = w(\zeta) \sum_{n = 1}^{\infty} Y_n(\zeta) Y_n(x) = w(x) \sum_{n = 1}^{\infty} Y_n(\zeta)y_n(x)
,\]
from the isolation property. Hence,
\[
	\delta(x - \zeta) = w(x) \sum_{n = 1}^{\infty}\frac{y_n(\zeta)y_n(x)}{N_n}
,\]
where $N_n = \int w y_n^2 \diff x$ is the normalisation factor.

\begin{exbox}
	Consider a Fourier series for $y(0) = y(1) = 0$, with $y_n(x) = \sin n \pi x$. From the sine series coefficient expansion,
	\[
		\delta(x - \zeta) = 2 \sum_{n = 1}^{\infty} \sin n \pi \zeta \sin n \pi x
	,\]
	for $0 < \zeta < 1$.
\end{exbox}

\newpage

\section{Green's Functions}%
\label{sec:green_s_functions}

\subsection{Motivation for Green's Functions}%
\label{sub:motivation_for_green_s_functions}

Consider a massive static string with tension $T$ and linear mass density $\mu$, suspended between fixed ends $y(0) = y(1) = 0$. By resolving forces, we have the time independent form
\[
T \frac{\Diff 2 y}{\diff x^2} - \mu g = 0
.\]
Integrating directly, we find that
\[
y = \frac{\mu g}{2T}x^2 + k_1x + k_2
.\]
Imposing boundary conditions,
\[
	y(x) = \biggl( - \frac{\mu g}{T} \biggr) \cdot \frac{1}{2} x(1 - x)
.\]
That was one way to obtain the solution. Alternatively, we may solve the equation for a single point mass, and superimpose the resulting solution to find the overall solution.

Our single point has mass $\delta m = \mu \delta x$, and is at position $x = \zeta$. The solution to the ODE will then be two straight lines, joining $(0, 0), (1, 0)$ and the mass $(\zeta_i, y_i(\zeta_i))$. If the angle of these straight lines from the horizontal are $\theta_1, \theta_2$, we can resolve the vertical forces:
\begin{align*}
	0 &= T(\sin \theta_1 + \sin \theta_2) - \delta mg = T \biggl( \frac{-y_i}{\zeta_i} + \frac{-y_i}{1 - \zeta_i} \biggr) - \delta mg, \\
	\implies y_i(\zeta_i) &= \frac{- \delta mg}{T} \zeta_i(1 - \zeta_i).
\end{align*}
This is a generalised sawtooth. Alternatively, we can write this as $f_i(\zeta)G(x, \zeta)$, where $f_i$ is a source term, and $G(x, \zeta)$ is the Green's function, which is the solution for a unit point source. As the differential equation is linear, we may sum the solution to find
\[
	y(x) = \sum_{i = 1}^{N} f_i(\zeta)G(x, \zeta_i)
.\]
Taking the limit,
\[
	f_i(\zeta) = \frac{-\delta mg}{T} = \frac{- \mu \delta x g}{T} = f(x) \diff x \implies f(x) = \frac{- \mu g}{T}
.\]
We can thus write
\[
	y(x) = \int_{0}^{1}f(\zeta)G(x, \zeta) \diff \zeta
.\]
If we substitute our calculated values,
\begin{align*}
	y(x) &= \biggl( \frac{- \mu g}{T} \biggr) \biggl[ \int_{0}^{x} \zeta(1 - x)\diff \zeta + \int_{x}^{1}x(1 - \zeta)\diff \zeta \biggr] \\
	     &= \biggl( \frac{- \mu g}{T} \biggr) \left\{ \left[\frac{\zeta^2}{2}(1 - x)\right]_{0}^{x} + \left[x\bigl(\zeta - \frac{\zeta^2}{2} \bigr) \right]_{x}^{1} \right\} \\
	     &= \biggl( \frac{- \mu g}{T} \biggr) \biggl( \frac{x^2}{2} (1 - x) + \frac{x}{2} - x \biggl( x - \frac{x^2}{2} \biggr) \biggr) \\
	     &= \biggl( \frac{-\mu g}{T} \biggr) \cdot \frac{1}{2} x(1 - x),
\end{align*}
which is the same solution as earlier. The benefit of using Green's function is that direct integration may not be possible in all cases, and so Green's functions may have to be used.

\subsection{Definition of Green's Function}%
\label{sub:definition_of_green_s_function}

Suppose we wish to solve the inhomogeneous ODE
\[
	\mathcal{L} y = \alpha(x) '' + \beta(x) y' + \gamma(x) y = f(x)
,\]
on the interval $a \leq x \leq b$, where $\alpha \not \equiv 0$ and $\alpha, \beta, \gamma$ are continuous and bounded, taking homogeneous boundary conditions $y(a) = y(b) = 0$. The Green's function for $\mathcal{L}$ is defined to be the solution for a unit point source at $x = \zeta$. That is, $G(x, \zeta)$ is the function that satisfies the boundary conditions, and
\[
	\mathcal{L}G(x, \zeta) = \delta(x - \zeta)
,\]
with $G(a, \zeta) = G(b, \zeta) = 0$. By linearity, the general solution is
\[
	y(x) = \int_{a}^{b} f(\zeta)G(x, \zeta) \diff \zeta
,\]
where $y(x)$ satisfies the homogeneous boundary conditions. Indeed,
\[
	\mathcal{L}y = \int_{a}^{b}\mathcal{L}G(x, \zeta)f(\zeta)\diff \zeta = \int_{a}^{b}\delta(x - \zeta)f(\zeta)\diff \zeta = f(x)
.\]
Hence the solution is given by the inverse operator $y = \mathcal{L}^{-1} f$, where
\[
	\mathcal{L}^{-1} = \int_{a}^{b} \diff \zeta G(x, \zeta)
.\]
We can split the Green's function into two parts:
\[
	G(x, \zeta) =
	\begin{cases}
		G_1(x, \zeta) & a \leq x < \zeta, \\
		G_2(x, \zeta) & \zeta < x \leq b.
	\end{cases}
\]
For all $x \neq \zeta$, we have $\mathcal{L}G_1 = \mathcal{L}G_2 = 0$, so the parts are homogeneous solutions. Since $G$ satisfies the homogeneous boundary conditions $G_1(a, \zeta) = G_2(b, \zeta) = 0$. Moreover, $G$ must be continuous at $x = \zeta$, so $G_1(\zeta, \zeta) = G_2(\zeta, \zeta)$.

Since $\mathcal{L}G = \delta(x - \zeta)$, $G$ must have a jump condition. For a second order ODE, this implies the derivative of $G$ is discontinuous at $x = \zeta$. Thus, we must have
	\[
		[G']_{\zeta_{-}}^{\zeta_{+}} = \left. \frac{\diff G_2}{\diff x} \right|_{x = \zeta_{+}} - \left.\frac{\diff G_1}{\diff x} \right|_{x = \zeta_{+}} = \frac{1}{\alpha(\zeta)}
	.\]

Hence, solving
\[
	\mathcal{L}G(x, \zeta) = \delta(x - \zeta)
\]
on $a \leq x \leq b$, we have functions $G_1, G_2$ which satisfy the homogeneous equation, so $\mathcal{L}G_i = 0$. Suppose there are two independent homogeneous solutions $y_1(x), y_2(x)$ to $\mathcal{L}y = 0$. Then $G_1 = Ay_1 + By_2$ satisfies $Ay_1(a) + By_2(a) = 0$, constraining $A$ and $B$. Thus there is one complementary function $y_{-}(x)$ such that $y_{-}(a) = 0$. Similarly, we can define $y_{+}$ as a linear combination of $y_1, y_2$ with $y_{+}(b) = 0$. Letting
\[
G_1 = Cy_{-}, \quad G_2 = Dy_{+}
,\]
we impose our other boundary conditions. Since $G_1(\zeta, \zeta) = G_2(\zeta, \zeta)$, we have
\[
	Cy_{-}(\zeta) = Dy_{+}(\zeta)
.\]
Moreover, the jump condition implies
\[
	Dy_{+}'(\zeta) - Cy_{-}'(\zeta) = \frac{1}{\alpha(\zeta)}
.\]
Solving these equation, we find
\[
	C(\zeta) = \frac{y_{+}(\zeta)}{\alpha(\zeta) W(\zeta)}, \quad D(\zeta) = \frac{y_{-}(\zeta)}{\alpha(\zeta)W(\zeta)}
,\]
where $W(\zeta)$ is the Wronskian\index{Wronskian}
\[
	W(\zeta) = y_{-}(\zeta)y_{+}'(\zeta) - y_{+}(\zeta)y'_{-}(\zeta)
,\]
and is non-zero	if $y_{-}$, $y_{+}$ are linearly independent. Hence,
\[
	G(x, \zeta) =
	\begin{dcases}
		\frac{y_{-}(x)y_{+}(\zeta)}{\alpha(\zeta)W(\zeta)} & a \leq x \leq \zeta, \\
		\frac{y_{-}(\zeta)y_{+}(x)}{\alpha(\zeta)W(\zeta)} & \zeta \leq x \leq b.
	\end{dcases}
\]

By linearity, the solution of $\mathcal{L}y = f$ is
\[
	y(x) = \int_{a}^{b} G(x, \zeta)f(\zeta) \diff \zeta
.\]
Split this into two intervals such that $G = G_1$ for $\zeta > x$ and $G = G_2$ for $\zeta < x$: this allows us to compute
\begin{align*}
	y(x) &= \int_{a}^{x} G_2(x, \zeta) f(\zeta) \diff \zeta + \int_{x}^{b}G_1(x, \zeta)f(\zeta) \diff \zeta \\
	     &= y_{+}(x) \int_{a}^{x} \frac{y_{-}(\zeta) f(\zeta)}{\alpha(\zeta)W(\zeta)}\diff \zeta + y_{-}(x) \int_{x}^{b} \frac{y_{+}(\zeta) f(\zeta)}{\alpha(\zeta)W(\zeta)} \diff \zeta.
\end{align*}
If $\mathcal{L}$ is in Sturm-Liouville form, $\beta = \alpha'$. Then, the denominator $\alpha(\zeta)W(\zeta)$ is constant. Notice also $G$ is symmetric, so $G(x, \zeta) = G(\zeta, x)$. Often we take $\alpha = 1$.

\begin{exbox}
	Consider $y'' - y = f(x)$ with $y(0) = y(1) = 0$. The homogeneous solutions are $y_1 = e^{x}$ and $y_2 = e^{-x}$. Imposing boundary conditions,
	\[
	G =
	\begin{cases}
		C \sinh x & 0 \leq x < \zeta, \\
		D \sinh (1 - x) & \zeta < x \leq b.
	\end{cases}
	\]
	Continuity at $x = \zeta$ implies
	\[
		C \sinh \zeta - D \sinh (1 - \zeta) \implies C = D \frac{\sinh (1 - \zeta)}{\sinh \zeta}
	.\]
	The jump condition gives
	\[
		-D \cosh (1 - \zeta) - C \cosh \zeta = 1
	.\]
	Solving simultaneously, we get
	\[
		C = \frac{- \sinh(1 - \zeta)}{\sinh 1}, \quad D = - \frac{\sinh \zeta}{\sinh 1}
	.\]
	Therefore,
	\[
		y(x) = \frac{-\sinh(1 - x)}{\sinh 1} \int_{0}^{x} \sinh \zeta f(\zeta) \diff \zeta - \frac{\sinh x}{\sinh 1} \int_{x}^{1} \sinh (1 - \zeta)f(\zeta) \diff \zeta
	.\]
\end{exbox}

If we are given inhomogeneous boundary conditions, we wish to find a particular integral $y_p$ that is homogeneous and which solves the inhomogeneous boundary conditions. Then subtracting this solution from the original equation, we are reduced to solving for homogeneous boundary conditions.

For example, in the above $y_p=  \frac{\sinh x}{\sinh 1}$ is a homogeneous solution to $y(0) = 0$, $y(1) = 1$.

\subsection{Higher-order ODEs}%
\label{sub:higher_order_odes}

Suppose $\mathcal{L}y = f(x)$, where $\mathcal{L}$ is an $n$'th order linear differential operator, and $\alpha(x)$ is the coefficient for the highest derivative. Suppose that we are given homogeneous boundary conditions. Then we can define the Green's function to be the function that solves
\[
	\mathcal{L}G(x, \zeta) = \delta(x - \zeta)
,\]
and which has properties:
\begin{enumerate}[(i)]
	\item $G_1$, $G_2$ are homogeneous solutions satisfying the homogeneous boundary conditions;
	\item $G_1^{(k)}(\zeta) = G_2^{(k)}(\zeta)$ for all $k \in \{0, 1, \ldots, n-2\}$;
	\item $G_2^{(n-1)}(\zeta_{+}) - G_1^{(n-1)}(\zeta_{-}) = \frac{1}{\alpha(z)}$.
\end{enumerate}

\subsection{Eigenfunction Expansion of Green's Functions}%
\label{sub:eigenfunction_expansion_of_green_s_functions}

Suppose $\mathcal{L}$ is in Sturm-Liouville form, with eigenfunctions $y_n(x)$ and eigenvalues $\lambda_n$. We seek an eigenfunction expansion
\[
	G(x, \xi) = \sum_{n = 1}^{\infty}A_n y_n(x),
\]
where $\mathcal{L}G = \delta(x - \xi)$. Evaluating $\mathcal{L}G$ directly,
\begin{align*}
	\mathcal{L}G &= \sum_{n = 1}^{\infty} A_n \mathcal{L}y_n(x) = \sum_{n = 1}^{\infty} A_n \lambda_n w(x) y_n(x) \\
		     &= \delta(x = \xi) = \omega(x) \sum_{n = 1}^{\infty} \frac{y_n(\xi) y_n(x)}{\mathcal{N}_n}.
\end{align*}
Hence we have $A_n(\xi) = y_n(\xi)/(\lambda_n \mathcal{N}_n)$. Thus,
\[
	G(x, \xi) = \sum_{n = 1}^{\infty}\frac{y_n(\xi)y_n(x)}{\lambda_n \mathcal{N}_n} = \sum_{n = 1}^{\infty} \frac{Y_n(\xi) Y_n(x)}{\lambda_n}
.\]

\subsection{Green's Functions for Initial Value Problems}%
\label{sub:green_s_functions_for_initial_value_problems}

Suppose we want to solve $\mathcal{L}(y) = f(t)$, a second-order ODE, for $t \geq a$, subject to boundary conditions $y(a) = y'(a) = 0$.

Again, we can use Green's functions $G(t, \tau)$ satisfying $\mathcal{L}G = \delta(t - \tau)$.

\begin{itemize}
	\item For $t < \tau$, $G_1 = Ay_1(t) + By_2(t)$, with boundary conditions
		\begin{align*}
			Ay_1(a) + By_2(a) &= 0, \\
			Ay_1'(a) + By_2'(a) &= 0.
		\end{align*}
		This corresponds to $W(a) = 0$, so if $y_1, y_2$ are not linearly dependent, then $A = B = 0$ and $G_1(t, \tau) = 0$.
	\item For $t > \tau$, by continuity, $G_2(\tau, \tau) = 0$. Hence we can choose $G_2 = D y_{+}(t)$, with $y_{+}(t) = Ay_1(t) + By_2(t)$ satisfying $y_{+}(t) = 0$.
\end{itemize}

But by the jump condition, we must have
\[
	[G']_{\tau_{-}}^{\tau_{+}} = G_2'(\tau, \tau) - G_1'(\tau, \tau) = D y_{+}'(\tau) = \frac{1}{\alpha(\tau)}
.\]
Hence we get $D(\tau) = (\alpha(\tau)y_{+}'(\tau))^{-1}$. This gives solution
\[
	G(t, \tau) =
	\begin{dcases}
		0 & t < \tau, \\
		\frac{y_{+}(t)}{\alpha(\tau) y_{+}'(t)} & t \geq \tau.
	\end{dcases}
\]

The IVP is
\[
	y(t) = \int_{a}^{t}G(t, \tau) f(\tau) \diff \tau = \int_{a}^{t} \frac{y_{+}(t) f(\tau)}{\alpha(\tau) y_{+}'(\tau)} \diff \tau
.\]

As we can see, the causality in this solution is ``built-in'': only forces which begin acting prior to a time $t$ can impact the solution at time $t$.

\begin{exbox}
	Suppose we want to solve $y'' - y = f(t)$ with $y(0) = y'(0) = 0$. Then, solving for $G(t, \tau)$:
	\begin{itemize}
		\item At $t < \tau$, $G_1 \equiv 0$.
		\item At $t > \tau$, $G_2 = Ae^{t} + Be^{-t}$.
	\end{itemize}
	By continuity, $G_2 = D \sinh(t - \tau)$, and by the jump condition,
	\[
		[G']_{\tau_{-}}^{\tau_{+}} = \frac{1}{\alpha} = 1 \implies G_2'(\tau, \tau) = D \cosh(0) = 1
	.\]
	So $D = 1$, and the general solution is
	\[
		y(t) = \int_{0}^{t}f(\tau) \sinh (t - \tau) \diff \tau
	.\]
\end{exbox}

\newpage

\section{Fourier Transforms}%
\label{sec:fourier_transforms}

\subsection{Introduction}%
\label{sub:introduction}

\begin{definition}
	The \textit{Fourier transform}\index{Fourier transform} (FT) of a function $f(x)$ is
	\[
		\tilde f(k) = \mathcal{F}(f)(k) = \int_{-\infty}^{\infty}f(x) e^{-ikx}\diff x
	,\]
	and the \textit{inverse Fourier transform} is
	\[
		f(x) = \mathcal{F}^{-1}(\tilde f)(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde f(k) e^{ikx}\diff k
	.\]
\end{definition}

The \textit{Fourier inversion theorem}\index{Fourier inversion theorem} states that
\[
	\mathcal{F}^{-1}(\mathcal{F}(f))(x) = f(x)
,\]
with the sufficient condition that $f$ and $\tilde f$ are \textit{absolutely integrable}\index{absolutely integrable}
\[
	\int_{-\infty}^{\infty}|f(x)| \diff x = M < \infty
.\]

\begin{exbox}[Fourier Transform of Gaussian]
	Take a Gaussian $f(x) = \frac{1}{\sigma \sqrt \pi} e^{-x^2/\sigma^2}$. Then,
	\[
		\tilde f(k) = \frac{1}{\sigma \sqrt \pi} \int_{-\infty}^{\infty} e^{-x^2/\sigma^2}e^{-ikx} \diff x = \frac{1}{\sigma \sqrt \pi} \int_{-\infty}^{\infty} e^{-x^2/\sigma^2} \cos kx \diff x
	,\]
	since the odd part $i \sin kx$ disappears. Consider the derivative of $\tilde f$:
	\begin{align*}
		\tilde f'(k) &= \frac{-1}{\sigma \sqrt \pi} \int_{-\infty}^{\infty} x e^{-x^2/\sigma^2} \sin kx \diff x \\
			     &= \frac{1}{\sigma \sqrt \pi} \biggl[ \frac{\sigma^2}{2} e^{-x^2/\sigma^2} \sin kx \biggr]_{-\infty}^{\infty} - \frac{1}{\sigma \sqrt \pi} \int_{-\infty}^{\infty} \biggl( \frac{k \sigma^2}{2} e^{-x^2/\sigma^2}\biggr) \cos kx \diff x \\
			     &= - \frac{k \sigma^2}{2} \tilde f(k).
	\end{align*}
	This gives $\tilde f(k) = C e^{- k^2 \sigma^2/4}$. But $k = 0$ gives $\tilde f (0) = 1$, so $C = 1$ and so
	\[
		\tilde f(k) = \exp \biggl( - \frac{k^2\sigma^2}{4} \biggr)
	.\]
	We can then show that $\mathcal{F}^{-1} \tilde f = f$.
\end{exbox}

\begin{exbox}
	We can show that $f(x) = e^{-a|x|}$ for $a > 0$ has Fourier transform $\tilde f(k) = 2a/(a^2 + k^2)$ in two ways:
	\begin{enumerate}[(i)]
		\item Integrate by parts
			\[
			2 \int_{0}^{\infty} e^{-ax} \cos k x \diff x
			.\]
		\item Integrate directly
			\[
				\int_{0}^{\infty} e^{-(a-ik)x}\diff x + \int_{-\infty}^{0}e^{(a + ik)x}\diff x
			.\]
	\end{enumerate}
	If $f(x)=  e^{-ax}$ for $x > 0$, and $0$ for $x \leq 0$, then we can show that $\tilde f(k) = (ik+a)^{-1}$.
\end{exbox}

\subsection{Fourier Transforms and Fourier Series}%
\label{sub:fourier_transforms_and_fourier_series}

We can write a Fourier series as
\[
	f(x) = \sum_{n = -\infty}^{\infty} c_n e^{i k_n x}
,\]
where $k_n = (n\pi)/L = n \Delta k$ where $\Delta k = \pi/L$. Then,
\[
	c_n = \frac{1}{2L} \int_{-L}^{L} f(x) e^{-ik_n x}\diff x = \frac{\Delta k}{2 \pi} \int_{-L}^{L} f(x) e^{-ik_nx}\diff x
.\]
In the Fourier series, this gives
\[
	f(x) = \sum_{n = -\infty}^{\infty} \frac{\Delta k}{2 \pi} e^{i k_n x} \int_{-L}^{L}f(x') e^{-ik_n x'}\diff x'
.\]
But we can look at this like a Riemann integral:
\[
	\sum_{n = -\infty}^{\infty} \Delta k g(k_n) \to \int_{-\infty}^{\infty} g(k) \diff k
.\]
Taking the limit $L \to \infty$, we have the following:
\[
f(x) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} e^{ikx} \diff k \Biggr[ \int_{-\infty}^{\infty} f(x') e^{-ikx'} \diff x' \Biggr] = \mathcal{F}^{-1}(\mathcal{F}(f))(x)
.\]
Similar to a Fourier series, when $f(x)$ is discontinuous at $x$, the Fourier transform gives the average over the discontinuity:
\[
	\mathcal{F}^{-1}(\mathcal{F}(f))(x) = \frac{1}{2}(f(x_{-}) + f(x_{+}))
.\]

\subsection{Fourier Transform Properties}%
\label{sub:fourier_transform_properties}

From the definition of a Fourier transform
\[
	\tilde f(k) = \int_{-\infty}^{\infty} f(x) e^{-ikx}\diff x
,\]
we can see the following properties:
\begin{enumerate}[1.]
	\item Linearity: If $h(x) = \lambda f(x) + \mu g(x)$, then $\tilde h(k) = \lambda \tilde f(k) + \mu \tilde g(k)$
	\item Translation: If $h(x) = f(x - \lambda)$, then $\tilde h (k) = e^{-i\lambda k}\tilde f(k)$.
	\item Frequency shift: If $h(x) = e^{i \lambda x}f(x)$, then $\tilde h(k) = \tilde f(k - \lambda)$.
	\item Scaling: If $h(x) = f(\lambda x)$, then $\tilde h(k) = |\lambda|^{-1} f(\frac{k}{\lambda})$.
	\item Multiplication by $x$: If $h(x) = x f(x)$, then $\tilde h(k) = i \tilde f'(k)$. Indeed, this follows from
		\[
			\int_{-\infty}^{\infty} xf(x) e^{-ikx}\diff x = \frac{-1}{i} \frac{\diff}{\diff k} \int_{-\infty}^{\infty} f(x) e^{-ikx} \diff x
		.\]
	\item Derivative: If $h(x) = f'(x)$, then $\tilde h(k) = i k \tilde f(k)$. This follows from
		\[
			\tilde h(k) = \int_{-\infty}^{\infty} f'(x) e^{-ikx}\diff x = [f(x) e^{-ikx}]_{-\infty}^{\infty} + \int_{-\infty}^{\infty} ik f(x) e^{-ikx} \diff x
		.\]
	\item General duality: Taking the inverse Fourier transform and taking $x \mapsto -x$, then
		\[
			f(-x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde f(k) e^{-ikx} \diff k
		.\]
		Now we can swap $k$ and $x$ to get
		\[
			f(-k)= \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde f(x) e^{-ikx} \diff x
		.\]
		Hence if $g(x) = \tilde f(x)$, then $\tilde g(k) = 2 \pi f(-k)$. Since
		\[
			f(-x) = \frac{1}{2\pi}\mathcal{F}(\tilde f)(x) = \frac{1}{2\pi} \mathcal{F}^2(f)(x), \quad \mathcal{F}^{4}(f)(x) = 4 \pi^2 f(x)
		.\]
\end{enumerate}

\begin{exbox}[Top hat]
	We find the Fourier transform for the `top hat'\index{top hat},
	\[
		f(x) =
		\begin{cases}
			1 & |x| \leq a, \\
			0 & |x| > a.
		\end{cases}
	\]
	We can calculate
	\[
		\tilde f(k) = \int_{-\infty}^{\infty} f(x) e^{-ikx} \diff x = \int_{a}^{a} \cos k x \diff x = \frac{2 \sin k a}{k}
	.\]
	The Fourier inversion theorem then says
	\[
	\frac{1}{\pi} \int_{-\infty}^{\infty} e^{ikx} \frac{\sin ka}{k} \diff k =
	\begin{cases}
		1 & |x| < a, \\
		0 & |x| > a.
	\end{cases}
	\]
	Now set $x = 0$ and take $k \to x$, to get the \textit{Dirichlet discontinuous formula}\index{Dirichlet discontinuous formula}
	\[
	\int_{0}^{\infty} \frac{\sin a x}{x} \diff x = \frac{\pi}{2} \sgn(a) =
	\begin{cases}
		\frac{\pi}{2} & a > 0, \\
		0 & a = 0, \\
		- \frac{\pi}{2} & a < 0.
	\end{cases}
	\]
\end{exbox}

\subsection{Convolutions and Pareseval's Theorem}%
\label{sub:convolutions_and_pareseval_s_theorem}

Suppose we want to multiply Fourier transforms in the frequency domain, so $\tilde h(k) = \tilde f(k) \tilde g(k)$. Then the Fourier inverse is
\begin{align*}
	h(x) &= \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde f(k) \tilde g(k) e^{ikx} \diff k = \frac{1}{2 \pi} \int_{-\infty}^{\infty} \Biggl( \int_{-\infty}^{\infty} f(y) e^{-iky}\diff y \Biggr) \tilde g(k) e^{ikx} \diff k \\
	     &= \int_{-\infty}^{\infty} f(y) \Biggl( \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde g(k) e^{ik(x - y)}\diff k \Biggr) \diff y \\
	     &= \int_{-\infty}^{\infty} f(y) g(x - y) \diff y = f \ast g(x).
\end{align*}
This is the definition of the \textit{convolution}\index{convolution}.

By duality, we also have
\[
	h(x) = f(x) g(x) \iff \tilde h(k) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde f(p) \tilde g(k - p) \diff p
.\]

Consider $h(x) = g^{\ast}(-x)$. Then,
\begin{align*}
	\tilde h(k) &= \int_{-\infty}^{\infty} g^{\ast}(-x) e^{-ikx}\diff x = \Biggl[ \int_{-\infty}^{\infty} g(-x) e^{ikx} \diff x \Biggr]^{\ast} \\
		    &= \Biggl[ \int_{-\infty}^{\infty} g(y) e^{-iky} \diff y \Biggr]^{\ast} = \tilde g^{\ast}(k).
\end{align*}
Substituting into the convolution theorem,
\[
	\int_{-\infty}^{\infty} f(y) g^{\ast}(y - x) \diff y = \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde f(k) \tilde g^{\ast}(k) e^{ikx} \diff k
.\]
Take $x = 0$, then if we let $y \mapsto x$ on the left hand side,
\[
	\int_{-\infty}^{\infty}f(x) g^{\ast}(x) \diff x = \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde f(k) \tilde g^{\ast}(k) \diff k
.\]
Equivalently, $\langle g, f \rangle = \frac{1}{2\pi} \langle \tilde g, \tilde f \rangle$. Now, set $g^{\ast} = f^{\ast}$ to obtain
\[
	\int_{-\infty}^{\infty} |f(x)|^2 \diff x = \frac{1}{2\pi} \int_{-\infty}^{\infty} |\tilde f(k)|^2 \diff k
,\]
which is \textit{Pareseval's theorem}\index{Parseval's theorem}

\subsection{Fourier Transform of Generalized Functions}%
\label{sub:fourier_transform_of_generalized_functions}

We will apply $\mathcal{F}$ to generalized functions. These can be treated as limiting distributions for which $\mathcal{F}(f)$ has a limiting approximation, and can be shown with inner products of Schwarz functions using Parseval's theorem.

\subsubsection{Dirac Delta}%
\label{subsub:dirac_delta}

Consider the inversion
\begin{align*}
	f(x) &= \mathcal{F}^{-1}(\mathcal{F}(f))(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \Biggl[ \int_{-\infty}^{\infty}f(u) e^{-iku} \diff u \Biggr] e^{ikx} \diff k \\
	     &= \int_{-\infty}^{\infty} f(u) \Biggl[ \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{ik(x-u)}\diff k \Biggr] \diff u.
\end{align*}
Since we also know that
\[
	f(x) = \int_{-\infty}^{\infty}f(u) \delta(x - u) \diff u
,\]
we can identify
\[
	\delta(x - u) = \frac{1}{2\pi} \int_{-\infty}^{\infty}e^{ik(x - u)}\diff k
.\]
Setting $f(x) = \delta(x)$, we can get
\[
	\tilde f(k) = \int_{-\infty}^{\infty} \delta(x) e^{ikx} \diff x = 1
.\]
Now if $f(x) = 1$, we get
\[
	\tilde f(k) = \int_{-\infty}^{\infty} e^{-ikx} \diff x = 2 \pi \delta(k)
.\]
This is what we expect from the inversion theorem. More generally, if $f(x) = \delta(x - a)$,
\[
	\tilde f(k) = e^{-ika}
.\]

\subsubsection{Trigonometric Functions}%
\label{subsub:trigonometric_functions}

Consider $f(x) = \cos \omega x$. We can write it as a sum of two exponentials, and observe
\[
	\tilde f(k) = \pi (\delta(k + \omega) + \delta(k - \omega))
.\]
Similarly, for $f(x) = \sin \omega x$,
\[
	\tilde f(k) = i \pi(\delta(k+w) - \delta(k - w))
.\]
We can then find the inverse Fourier transform using duality.

\subsubsection{Heaviside Function}%
\label{subsub:heaviside_function}

We use a subtle observation, requiring the central value $H(0) = 1/2$. Then adding two Heaviside functions,
\[
	H(x) + H(-x) = 1
,\]
for all $x$. By linearity, we can take the Fourier transform to get
\[
	\tilde H(k) + \tilde H(-k) = 2 \pi \delta(k)
.\]
Recall that the derivative of the Heaviside function is the Dirac delta function; $H'(x) = \delta(x)$. Taking the Fourier transform,
\[
	ik \tilde H(k) = 1
.\]
These are two different Fourier transforms of $H(k)$. But $k\delta(k) = 0$, so this becomes consistent if
\[
	\tilde H(k) = \pi \delta(k) + \frac{1}{ik}
.\]
From the Dirichlet discontinuous formula, formula, we can write
\[
	\frac{1}{2} \sgn(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \frac{e^{ika}}{ik} \diff k
.\]
Hence
\[
	f(x) = \frac{1}{2} \sgn(x) \iff \tilde f(k) = \frac{1}{ik}
.\]
Hence we tend to use sign functions, rather than Heaviside functions.

\subsection{Applications to Fourier Transforms}%
\label{sub:applications_to_fourier_transforms}

\subsubsection{Solving ODE's}%
\label{subsub:solving_ode_s}

Consider solving the ODE $y'' - y = f(x)$, with homogeneous boundary conditions $y \to 0$ as $x \to \pm \infty$. Taking the Fourier transform,
\[
	(-k^2 - 1)\tilde y = \tilde f
.\]
Hence we can find the solution
\[
	\tilde y(k) = - \frac{\tilde f(k)}{1 + k^2} = \tilde f(k) \tilde g(k), \quad \text{where } \tilde g(k) = \frac{-1}{1 + k^2}
.\]
The Fourier transform of this is $g(x) = - \frac{1}{2} e^{-|x|}$, hence the convolution theorem implies
\begin{align*}
	y(x) &= \int_{-\infty}^{\infty} f(u)g(x - u) \diff u = -\frac{1}{2} \int_{-\infty}^{\infty} f(u) e^{-|x-u|}\diff u \\
	     &= -\frac{1}{2} \int_{-\infty}^{x} f(u) e^{u - x} \diff u - \frac{1}{2} \int_{x}^{\infty} f(u) e^{x - u}\diff u,
\end{align*}
which is in the form of a boundary value problem Green's function. In fact, this is the same solution given by taking the Green's function.

Now suppose we want to solve a signal processing problem, given an input $\mathcal{I}(t)$ acted on by a linear operator $\mathcal{L}$ to yield the output $\mathcal{O}(t)$.

The Fourier transform $\tilde{\mathcal{I}}(\omega)$ is denoted the \textit{resolution}\index{resolution}
\[
	\tilde{\mathcal{I}}(\omega) = \int_{-\infty}^{\infty} \mathcal{I}(t) e^{-i \omega t}\diff t
.\]
In the frequency domain, the action of $\mathcal{L}$ on $\mathcal{I}(t)$ means that $\tilde{\mathcal{I}}(\omega)$ is multiplied by a \textit{transfer function}\index{transfer function} $\tilde{\mathcal{R}}(\omega)$ to yield the output
\[
	\mathcal{O}(t) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} \tilde{\mathcal{R}}(\omega) \tilde{\mathcal{O}}(\omega) e^{i\omega t} \diff \omega
,\]
with the \textit{response function}\index{response function} given by
\[
	R(t) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde{\mathcal{R}}(\omega) e^{i \omega t}\diff \omega
.\]
By the convolution theorem, the output is
\[
	\mathcal{O}(t) = \int_{-\infty}^{\infty} \mathcal{I}(u) \mathcal{R}(t - u) \diff u
.\]
We assume there is no input $\mathcal{I}(t) = 0$ for $t < 0$. By causality, there is zero output for $R(t) = 0$, $t < 0$. Hence we require $0 < u < t$:
\[
	\theta(t) = \int_{0}^{t} \mathcal{I}(u) \mathcal{R}(t - u) \diff u
,\]
which is the same form as initial value problem Green's functions.

\subsection{General Transfer Functions for ODEs}%
\label{sub:general_transfer_functions_for_odes}

Suppose the input/output relation given by a general ODE is
\[
	\mathcal{L}\mathcal{O}(t) = \Biggl( \sum_{i = 1}^{n} a_i \frac{\Diff i}{\diff x^{i}} \Biggr) \mathcal{O}(t) = \mathcal{I}(t)
.\]
Take the Fourier transform:
\[
	(a_0 + a_i(i\omega) + a_2 (i \omega)^2 + \cdots + a_n (i \omega)^{n}) \tilde{\mathcal{O}}(\omega) = \tilde{\mathcal{I}}(\omega)
,\]
so the transfer function is
\[
	\tilde{\mathcal{R}}(\omega) = \frac{1}{a_0 + a_1(i \omega) + \cdots + a_n (i \omega)^{n}}
.\]
Without repeated roots, this is not hard, as we can write it as a sum using partial fraction decomposition, which all correspond to one-sided exponentials.

In general, we can factorise into the product
\[
	\tilde{\mathcal{R}}(\omega)^{-1} = \prod_{j=1}^{J}(i \omega - c_j)^{k_j}
.\]

Using partial fraction decomposition, we can write this as
\[
	\tilde R(\omega) = \sum_{j = 1}^{J} \sum_{m = 1}^{k_j} \frac{\Gamma_{jm}}{(i \omega - c_j)^{m}}
.\]
To solve, we must invert $\frac{1}{(i \omega - a)^{m}}$, for $m \geq 1$. We know that
\[
	\mathcal{F}^{-1} \biggl( \frac{1}{i\omega - a} \biggr) =
	\begin{cases}
		e^{at}, & t > 0, \\
		0, & t < 0,
	\end{cases}
\]
for $\Re(a) < 0$. Now for $m = 2$, note that
\[
	i \frac{\diff}{\diff \omega} \biggl( \frac{1}{i \omega - a} \biggr) = \frac{1}{(i \omega - a)^2}
,\]
and moreover we have $\mathcal{F}(tf(t)) = i \tilde f'(\omega)$, so
\[
	\mathcal{F}^{-1} \biggl( \frac{1}{(i \omega - a)^2} \biggr) =
	\begin{cases}
		t e^{at}, & t > 0, \\
		0, & t < 0.
	\end{cases}
\]
By induction, we find that
\[
	\mathcal{F}^{-1}\biggl( \frac{1}{(i \omega - a)^{m}}\biggr) =
	\begin{cases}
		\frac{t^{m-1}}{(m-1)!}e^{at}, & t > 0, \\
		0, & t < 0.
	\end{cases}
\]
Therefore, the response function takes the form of
\[
	R(t) = \sum_{j}\sum_{m} \Gamma_{jm} \frac{t^{m-1}}{(m-1)!}e^{c_j t}
.\]

\begin{exbox}
	We wish to solve
	\[
		\mathcal{L}y = y'' + 2 p y' + (p^2 + q^2)y = f(t)
	,\]
	with initial conditions $y(0) = y'(0) = 0$. The Fourier transform is
	\[
		(i \omega^2)\tilde y + 2 i p \omega \tilde y + (p^2 + q^2)\tilde y = \tilde f
	,\]
	so
	\[
	\tilde y = \frac{\tilde f}{- \omega^2 + 2 i p \omega + p^2 + q^2} = \tilde R \tilde f
	.\]
	Inverting with the convolution theorem,
	\[
		y(t) = \int_{0}^{t} R(t - \tau)f(\tau) \diff \tau
	,\]
	where the response is given by
	\[
		R(t - \tau) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \frac{e^{i \omega(t - \tau)} \diff \omega}{p^2 + q^2 + 2 i p \omega - \omega^2}
	.\]
\end{exbox}

\subsection{Discrete Fourier Transforms}%
\label{sub:discrete_fourier_transforms}

We sample a signal $h(t)$ at equal times $t_n = n \Delta$, with time-sampling $\Delta$, and values $h_n = h(t_n) = h(n \Delta)$ for $n = \ldots, -2, -1, 0, 1, 2, \ldots,$ i.e. with sampling frequency $1/\Delta$.

The \textit{Nyquist frequency}\index{Nyquist frequency} is $f_c = 1/2\Delta$, which is the highest frequency that is sampled at $\Delta$.

Indeed, if we have a signal with given frequency $f$, then,
\[
	g_f(t) = A \cos(2 \pi f t + \phi) = \frac{1}{2}(Ae^{i \phi}e^{2 \pi i ft} + Ae^{- i \phi}e^{-2 \pi i ft})
.\]
If we were to sample at $f = f_c$, then
\begin{align*}
	g_{f_c}(t_n) &= A \cos \biggl(2 \pi \biggl( \frac{1}{2 \Delta} \biggr) n \Delta + \phi \biggr) \\
		     &= A \cos \pi n \cos \phi + A \sin n \pi \sin \phi \\
		     &= A' \cos (2 \pi f_c t_n),
\end{align*}
with $A' = A \cos \phi$. So the phase and amplitude information is lost. We can identify $f_c \leftrightarrow -f_c$, and these are \textit{aliased} together.

If we sample slightly above $f > f_c$, then taking $f = f_c + \delta f$, we can show that
\[
	g_f(t_n) = A \cos (2 \pi (f_c + \delta f)t_n + \phi) = A \cos (2 \pi (f_c - \delta f)t_n - \phi)
.\]
So the effect is to \textit{alias} a ghost signal to frequency $f_c - \delta f$.

\subsubsection{Sampling Theorem}%
\label{subsub:sampling_theorem}

A signal $g(t)$ is \textit{bandwidth limited}\index{bandwidth limited} if it contains no frequencies above $\omega_{max} = 2 \pi f_{max}$, so $\tilde g(\omega) = 0$ for $|\omega| > \omega_{max}$. Then,
\[
	g(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty} \tilde g(\omega) e^{i \omega t}\diff \omega = \frac{1}{2 \pi} \int_{- \omega_{max}}^{\omega_{max}} \tilde g(\omega) e^{i \omega t}\diff \omega
.\]
We set the sampling to satisfy the Nyquist condition, so $\Delta = 1/2f_{max}$. Then,
\[
	g_n = g(t_n) = \frac{1}{2\pi} \int_{-\omega_{max}}^{\omega_{max}} \tilde g(\omega) e^{i \pi n \omega/\omega_{max}} \diff \omega
,\]
which is a complex Fourier series coefficient. Hence, we can write
\[
	\tilde g_{per}(\omega) = \frac{\pi}{\omega_{max}} \sum_{n = -\infty}^{\infty} g_n e^{-i\pi n\omega/\omega_{max}}
.\]
However, this is a periodic function. The actual Fourier transform $\tilde g(\omega)$ is found by multiplying by the `top hat'
\[
	\tilde h(\omega) =
	\begin{cases}
		1, & |\omega| \leq \omega_{max}, \\
		0, & \text{otherwise}.
	\end{cases}
\]
Then we have $\tilde g(\omega) = \tilde g_{per}(\omega) \tilde h(\omega)$. Inverting, we get
\begin{align*}
	g(t) &= \frac{1}{2\pi}\int_{0\infty}^{\infty} \tilde g_{per}(\omega) \tilde h(\omega) e^{i\omega t}\diff \omega \\
	     &= \frac{1}{2 \omega_{max}} \sum_{n = -\infty}^{\infty} g_n \int_{-\omega_{max}}^{\omega_{max}}\exp \biggl( i \omega \biggl( t - \frac{n \pi}{\omega_{max}} \biggr) \biggr) \diff \omega \\
	     &= \sum_{n = -\infty}^{\infty} g_n \frac{\sin (\omega_{max} t - \pi n)}{\omega_{max} t - \pi n}.
\end{align*}
So $g(t)$ can be exactly represented after sampling at discrete times $t_n$.

\subsubsection{Discrete Fourier Transform}%
\label{subsub:discrete_fourier_transform}

Suppose we have a finite number $N$ of samples, where $h_m = h(t_m) = h(m \Delta)$, $t_m = m \Delta$, and $m = 0, 1, 2, \ldots, N-1$.

We want to approximate the Fourier transform for $N$ frequencies within the Nyquist frequency ($f_c = 1/2\Delta$) using equally-spaced frequencies $\Delta_f = 1/N\Delta$, in the range $-f_c \leq f \leq f_c$.

We could take $f_n = n \Delta f$, with $n = - \frac{N}{2}, -\frac{N}{2} + 1, \ldots, -1, 0, 1, \ldots, \frac{N}{2}$. However this has $N+1$ frequencies, but $f_c$ and $-f_c$ are aliased.

Note also that $(\frac{N}{2} + m)\Delta f$ is aliased back to $-(\frac{N}{2} - m)\Delta f$, so we choose instead $f_n = \frac{n}{N \Delta}$ with $n = 0, 1, 2, \ldots, \frac{N}{2}-1, \frac{N}{2}, \frac{N}{2} + 1, \ldots, N-1$.

The \textit{discrete Fourier transform} at frequency $f_n$ becomes
\begin{align*}
	\tilde h (f_n) &= \int_{-\infty}^{\infty} h(t) e^{-2\pi i f_n t}\diff t \approx \Delta \sum_{n = 0}^{N-1} h_m e^{-2 \pi i f_n t_m} \\
		       &= \Delta \sum_{m = 0}^{N-1}h_m e^{-2 \pi i mn/N} = \Delta \tilde h_d (f_n),
\end{align*}
where $\tilde h_d (f_n) = \tilde h_n$ is the discrete Fourier transform\index{discrete Fourier transform}.

So the discrete Fourier transform matrix is $[DFT]_{mn} = e^{-2 \pi i mn/N}$, which defines the discrete Fourier transform.

The inverse is its adjoint $[DFT]^{-1} = \frac{1}{n} [DFT]^{\dagger}$, and it is built from the roots of unity $\omega = e^{-2 \pi i/N}$.

\begin{exbox}
	Take $N = 4$, then $\omega = -i$, and the DFT matrix is
	\[
	DFT = 
	\begin{pmatrix}
		1 & 1 & 1 & 1 \\
		1 & -i & -1 & i \\
		1 & -1 & 1 & -1 \\
		1 & i & -1 & -i
	\end{pmatrix}
	.\]
	The \textit{inverse DFT} is
	\begin{align*}
		h_m &= h(t_m) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde h(\omega) e^{i \omega t_m} \diff \omega = \int_{-\infty}^{\infty}\tilde h(f) e^{2 \pi i f t_m} \diff f \\
		    &\approx \frac{1}{\Delta N} \sum_{n = 0}^{N-1} \Delta \tilde h_d (f_n) e^{2 \pi i mn/N} \\
		    &= \frac{1}{N} \sum_{n = 0}^{N-1} \tilde h_n e^{2 \pi i mn/N}.
	\end{align*}
\end{exbox}

Moreover, we have Parseval's theorem for the discrete Fourier transform:
\[
\sum_{m = 0}^{N-1}|h_m|^2 = \frac{1}{N} \sum_{n = 0}^{N-1}|\tilde h_n|^2
.\]
The convolution theorem for $g_m, h_m$ is then
\[
c_k = \sum_{m = 0}^{N-1}g_m h_{k-m} \iff \tilde c_k = \tilde g_k \tilde h_k
.\]

\subsection{Fast Fourier Transforms}%
\label{sub:fast_fourier_transforms}

We can split the discrete Fourier transform into even ($n = 2m$) and odd ($n = 2m + 1$) parts. Then,
\begin{align*}
	\tilde h_k &= \sum_{n = 0}^{N-1} h_n \omega_N^{nk} = \sum_{m = 0}^{N/2 - 1} h_{2m}\omega_N^{2mk} + \sum_{m = 0}^{N/2 - 1}h_{2m+1}\omega_N^{(2m+1)k} \\
		   &= \sum_{m = 0}^{N/2 - 1}h_{2m} (\omega_N^2)^{mk} + \omega_N^{k} \sum_{m = 0}^{N/2 - 1}h_{2m+1}(\omega_N^2)^{mk} \\
		   &= \sum_{m = 0}^{N/2-1}h_{2m}(\omega_{N/2})^{mk} + \omega_N^{k} \sum_{m = 0}^{N/2 - 1}h_{2m+1}(\omega_{N/2})^{mk}.
\end{align*}
Hence instead of being $\mathcal{O}(n^2)$, by recursively halving the size, we get down to calculating the Fourier transform in $\mathcal{O}(n \log n)$.

\newpage

\part{PDEs on Unbounded Domains}%
\label{prt:pdes_on_unbounded_domains}

\section{Characteristics}%
\label{sec:characteristics}

\subsection{Well-Posed Cauchy Problems}%
\label{sub:well_posed_cauchy_problems}

Solving PDEs depends on the nature of the equations in combination with the boundary and/or initial data. A \textit{Cauchy problem}\index{Cauchy problem} is the PDE for $\phi$, together with the auxiliary data specified on a surface, which is called \textit{Cauchy data}\index{Cauchy data}.

A Cauchy problem is \textit{well-posed}\index{well-posed} if
\begin{enumerate}[(i)]
	\item a solution exists;
	\item the solution is unique;
	\item the solution depends continuously on the auxiliary data.
\end{enumerate}

\subsection{Method of Characteristics}%
\label{sub:method_of_characteristics}

Consider a parametrised curve $C$ given by $(x(s), y(s))$ with tangent vector
\[
	\mathbf{v} = \biggl( \frac{\diff x(s)}{\diff s}, \frac{\diff y(s)}{\diff s} \biggr)
.\]
For a function $\phi(x, y)$, we can define a directional derivative
\[
	\frac{\diff \phi}{\diff s} \biggr|_{C} = \frac{\diff x(s)}{\diff s} \frac{\partial \phi}{\partial x} + \frac{\diff y(s)}{\diff s} \frac{\partial \phi}{\partial y} = \mathbf{v} \cdot \nabla \phi \bigr|_{C}
.\]
If $\mathbf{v} \cdot \nabla \phi = 0$, then $\frac{\diff \phi}{\diff s} = 0$, and $\phi$ is constant along $C$.

Now suppose we have a vector field
\[
	\mathbf{u}= (\alpha(x, y), \beta(x, y))
,\]
with its family of integral curves $C$, which are non-intersecting and filling $\mathbb{R}^2$, so at a point $(x, y)$, the integral curve has a tangent vector $\mathbf{u}(x, y)$.

Define a curve $B$ by $(x(t), y(t))$, transverse to $\mathbf{u}$, such that the tangent vector
\[
	\mathbf{w} = \biggl( \frac{\diff x(t)}{\diff t}, \frac{\diff y(t)}{\diff t} \biggr)
\]
is nowhere parallel to $\mathbf{u}$. We can then label each integral curve $C$ of $\mathbf{u}$ using $t$ at the intersection point with $B$, then we use $s$ to parametrise along the curve.

Our integral curves $(x(s, t), y(s, t))$ satisfy
\[
	\frac{\diff x}{\diff s} = \alpha(x, y), \qquad \frac{\diff y}{\diff s} = \beta(x, y)
.\]
We can solve these to find a family of characteristic curves along which $t$ remains constant.

\subsection{Characteristics of a 1st order PDE}%
\label{sub:characteristics_of_a_1st_order_pde}

Consider the first order linear PDE
\[
	\alpha(x, y) \frac{\partial \phi}{\partial x} + \beta(x, y) \frac{\partial \phi}{\partial y} = 0
,\]
with specified Cauchy data given on an initial curve $B = (x(t), y(t))$, say
\[
	\phi(x(t), y(t)) = f(t)
.\]
Note that we have
\[
\alpha \phi_x + \beta \phi_y = \mathbf{u} \cdot \nabla \phi = \frac{\diff \phi}{\diff s} \biggr|_{C}
\]
is the directional derivative along the integral curves of $\mathbf{u} = (\alpha, \beta)$, which are the \textit{characteristic curves} of the PDE\index{characteristic curves}. Now since
\[
\frac{\diff \phi}{\diff s} = \alpha \phi_x + \beta \phi_y = 0
,\]
the function $\phi(x, y)$ will be constant along the curves $C$, so the Cauchy data $f(t)$ defined on $B$ at $s = 0$ will be propagated constantly along the curves $C$ to give the solution
\[
	\phi(s, t) = \phi(x(s, t), y(s, t)) = f(t)
.\]
To obtain $\phi(x, y)$, we transform the coordinates from  $\phi(s, t)$ using $s = s(x, y)$, $t = t(x, y)$ (provided the Jacobian is non-zero) to obtain $\phi(x, y) = f(t(x, y))$.

Hence, the prescription for solving these problems are as follows:
\begin{enumerate}[1.]
	\item Find characteristic equations $\frac{\diff x}{\diff s} = \alpha, \frac{\diff y}{\diff s} = \beta$.
	\item Parametrise the initial conditions on $B = (x(t), y(t))$.
	\item Solve the characteristic equations to find
		\[
			x = x(s, t), \qquad y = y(s, t)
		,\]
		subject to the initial conditions at $s = 0$,
		\[
			x(0, t) = x(t), \qquad y(0, t) = y(t).
		\]
	\item Using this, we can solve
		\[
		\frac{\diff \phi}{\diff s} = \alpha \phi_x + \beta \phi_y = 0
		,\]
		as $\phi(s, t) = f(t)$.
	\item Invert the relations $s = s(x, y)$, $t = t(x, y)$.
	\item Change coordinates to obtain $\phi(x, y) = f(t(x, y))$.
\end{enumerate}

\begin{exbox}
	We wish to solve the PDE
	\[
	\frac{\partial \phi}{\partial x} = 0
	,\]
	with $\phi(0, y) = h(y)$. The solution clearly is $\phi(x, y) = h(y)$, but we will follow the prescription anyway.
	\begin{enumerate}[1.]
		\item We are given $\frac{\diff x}{\diff s} = \alpha = 1$, $\frac{\diff y}{\diff s} = \beta = 0$.
		\item We are given initial conditions on the $y$-axis $(x(t), y(t)) = (0, t)$.
		\item The solutions to the equations are $x = s + c$, $y = d$. But $s = 0$ for $x = 0$ gives $c = 0$, and $y = t$ gives $d = t$. So $x = s$, $y = t$.
		\item $\frac{\diff \phi}{\diff s}= 0$ gives $\phi$ constant, so $\phi(s, t) = h(t)$.
		\item We invert $s = x$, $t = y$.
		\item Then $\phi(x, y) = h(t(x, y)) = h(y)$.
	\end{enumerate}
\end{exbox}

\begin{exbox}
	Now we take a more complicated PDE: we wish to solve
	\[
	e^{x} \phi_x + \phi_y = 0
	,\]
	with $\phi(x, 0) = \cosh x$.
	\begin{enumerate}[1.]
		\item The characteristic equations are $\frac{\diff x}{\diff s} = e^{x}$, $\frac{\diff y}{\diff s} = 1$.
		\item The initial conditions are given on $x(t) = t$, $y(t) = 0$.
		\item From separating the equations, $-e^{-x} = s + c$ and $y = s + d$. At $s = 0$, $x = t$ and $y = 0$, so $c = -e^{-t}$ and $d = 0$. Hence,
			\[
			e^{-x} = e^{-t} - s, \qquad y = s
			.\]
		\item Since $\frac{\diff \phi}{\diff s} = 0$, $\phi(s, t) = \cosh t$.
		\item At $s = y$, $e^{-t} = y + e^{-x}$, so $t = - \log(y + e^{-x})$.
		\item Hence $\phi(x, y) = \cosh[- \log(y + e^{-x})]$.
	\end{enumerate}
\end{exbox}

Now we will consider an inhomogeneous first order PDE, such as
\[
	\alpha(x, y) \phi_x + \beta(x, y) \phi_y = \gamma(x, y)
,\]
which Cauchy data $\phi(x(t), y(t)) = f(t)$ on a curve $B$ 

The characteristic curves $C$ are identical, but we now have
\[
	\frac{\diff \phi}{\diff s} \biggr|_{C} = \mathbf{u} \cdot \nabla \phi = \gamma(x, y)
,\]
with $\phi = f(t)$ at $s = 0$ on $B$. Hence $f(t)$ no longer propagates constantly, so we must solve an ODE. Hence we upgrade \textit{4.} in the prescription, to integrate $\phi(s, t)$ along $C$, before reverting to $\phi(x, y)$.

\begin{exbox}
	We wish to solve
	\[
	\phi_x + 2 \phi_y = ye^{x}
	,\]
	with $\phi = \sin x$ n $y = x$.
	\begin{enumerate}[1.]
		\item We have $\frac{\diff x}{\diff s} = 1$, $\frac{\diff y}{\diff s}= 2$.
		\item We have initial conditions on $y = x$ so take $(x(t), y(t)) = (t, t)$.
		\item We have $x = s + c$, $y = 2s + d$. At $s = 0$, $x = t = c$ and $y = t = d$, so
			\[
			x = s + t, \qquad y = 2s + t
			.\]
		\item Now we want to solve
			\[\frac{\diff \phi}{\diff s}= \gamma = ye^{x} = (2s + t)e^{s + t},\]
			with $\phi = \sin t$ at $s = 0$. Note that
			\[
				\frac{\diff}{\diff s}(2s e^{s}) = 2e^{s}+ 2se^{s}
			,\]
			so $\phi(s, t) = (2s - 2 + t)e^{s+t} + C$. But $\phi(0, t) = \sin t = (t - 2)e^{t} + C$, so
			\[
				\phi(s, t) = (2s - 2 + t)e^{s + t} + \sin t + (2 - t)e^{t}
			.\]
		\item We can invert $s = y - x$, $t = 2x - y$.
		\item Hence $\phi(x, y) = (y - 2)e^{x} + (y - 2x + 2)e^{2x - y} + \sin (2x - y)$.
	\end{enumerate}
\end{exbox}

\subsection{Second-order PDE classification}%
\label{sub:second_order_pde_classification}

In two dimensions, the general second order linear PDE is
\begin{align*}
	\mathcal{L} \phi = a(x, y) \frac{\partial^2 \phi}{\partial x^2} &+ 2 b(x, y) \frac{\partial^2 \phi}{\partial x \partial y} + c(x, y) \frac{\partial^2 \phi}{\partial y^2} \\
									     &+ d(x, y) \frac{\partial \phi}{\partial x} + e(x, y) \frac{\partial \phi}{\partial y} + f(x, y) \phi(x, y) = 0
\end{align*}
The principal part\index{principal part} is given by
\[
	\sigma_p(x, y, k_x, k_y) = k^{T}Ak =
	\begin{pmatrix}
		k_x & k_y
	\end{pmatrix}
	\begin{pmatrix}
		a(x, y) & b(x, y) \\
		b(x, y) & c(x, y)
	\end{pmatrix}
	\begin{pmatrix}
		k_x \\
		k_y
	\end{pmatrix}
.\]
The PDE is classified by the properties of the eigenvalues of $A$:
\begin{itemize}
	\item If $b^2 - ac < 0$ then it is \textit{elliptic} ($\lambda_1$ and $\lambda_2$ have the same sign).
	\item If $b^2 - ac > 0$ then it is \textit{hyperbolic} ($\lambda_1$ and $\lambda_2$ have opposite sign).
	\item If $b^2 - ac = 0$, then it is \textit{parabolic} ($\lambda_1$ or $\lambda_2$ is $0$).
\end{itemize}

\begin{exbox}
	\begin{itemize}
		\item The wave equation
			\[
			\frac{1}{c^2} \frac{\partial^2 \phi}{\partial t^2} = \frac{\partial^2 \phi}{\partial x^2}
			\]
			has $a = \frac{1}{c^2}$, $b = 0$ and $c = -1$, and is hyperbolic.
		\item The Laplace equation has $a = 1$, $b = 0$ and $c = 1$, and it elliptic.
	\end{itemize}
\end{exbox}

\subsection{Characteristic Curves}%
\label{sub:characteristic_curves}

A curve defined by $f(x, y) = \text{const}$ will be a characteristic if
\[
\begin{pmatrix}
	f_x & f_y
\end{pmatrix}
\begin{pmatrix}
	a & b \\
	b & c
\end{pmatrix}
\begin{pmatrix}
	f_x \\
	f_y
\end{pmatrix}
= 0
.\]
This is the generalisation of $\nabla f \cdot \mathbf{u} = 0$. The curve can be written as $y = y(x)$. Then by the chain rule,
\[
\frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} \frac{\diff y}{\diff x} = 0 \implies \frac{f_x}{f_y} = - \frac{\diff y}{\diff x}
.\]
Substituting, we obtain
\[
	a \biggl( \frac{\diff y}{\diff x} \biggr)^2 - 2b \frac{\diff y}{\diff x} + c = 0
,\]
which gives quadratic solutions
\[
	\frac{\diff y}{\diff x} = \frac{b \pm \sqrt{b^2 - ac}}{a}
.\]
\begin{itemize}
	\item If hyperbolic, then $b^2 - ac > 0$, so we have 2 solutions.
	\item If parabolic, then $b^2 - ac = 0$, so we have one solution.
	\item If elliptic, then $b^2 - ac < 0$, so we have no solutions.
\end{itemize}

If we transform to characteristic coordinates $(u, v)$, we will have $a = c = 0$, so the PDE will take the \textit{canonical form}\index{canonical form}
\[
\frac{\partial^2 \phi}{\partial u \partial v} + \cdots = 0
,\]
where the only second-order term is mixed.

\begin{exbox}
	Consider
	\[
	-y \phi_{xx} + \phi_{yy} = 0
	,\]
	with $a = -y$, $b = 0$ and $c = 1$, so $b^2 - ac = y$.

	This is hyperbolic for $y > 0$, elliptic for $y < 0$ and parabolic for $y = 0$. We find the characteristics for $y > 0$ satisfying
	\[
		\frac{\diff y}{\diff x} = \frac{b \pm \sqrt{b^2 - ac}}{a} = \pm \frac{1}{\sqrt y} \implies \sqrt y \diff y = \pm \diff x
	\]
	\[
	 \implies \frac{2}{3} y^{3/2} \pm x = C_{\pm}
	.\]
	So the characteristic curves are $u = \frac{2}{3} y^{3/2} + x$, $v = \frac{2}{3} y^{3/2} - x$. The derivatives are
	\begin{align*}
		u_x &= 1, & u_y &= y^{1/2}, & v_x &= -1, & v_y &= y^{1/2}.
	\end{align*}
	Hence we can rewrite
	\begin{align*}
		\phi_x &= \phi_u - \phi_v, & \phi_y &= y^{1/2}(\phi_u + \phi_v), \\
		\phi_{xx} &= \phi_{uu} - 2 \phi_{uv} + \phi_{vv}, & \phi_{yy} &= y(\phi_{uu} + 2 \phi_{uv} + \phi_{vv}) + \frac{1}{2y^{1/2}}(\phi_u + \phi_v).
	\end{align*}
	Hence we have
	\[
		-y \phi_{xx} + \phi_{yy} = y\biggl(4 \phi_{uv} + \frac{1}{2y^{3/2}}(\phi_u + \phi_v) \biggr)
	.\]
	Therefore the canonical form is
	\[
		\phi_{uv} + \frac{1}{6(u+v)}(\phi_u + \phi_v) = 0
	.\]
\end{exbox}

\subsection{General Solution for the Wave Equation}%
\label{sub:general_solution_for_the_wave_equation}

We look to solve the wave equation
\[
\frac{1}{c^2} \frac{\partial^2 \phi}{\partial t^2} - \frac{\partial^2 \phi}{\partial x^2} = 0
,\]
with initial conditions $\phi(x, 0) = f(x)$, $\phi_t(x, 0) = g(x)$.

We have $a = c^{-2}$, $b = 0$ and $c = -1$. Therefore the characteristic equation is
\[
	\frac{\diff x}{\diff t}= \frac{0 \pm \sqrt{0 + c^{-2}}}{c^{-2}} = \pm c
,\]
so we choose $u = x - ct$, $v = x + ct$. This yields the simple canonical form
\[
\frac{\partial^2 \phi}{\partial u \partial v} = 0
.\]
Integrating with regards to $u$, we get
\[
	\frac{\partial \phi}{\partial v} = F(v)
,\]
and integrating with respect to $v$,
\[
	\phi = G(u) + \int^{v} F(y) \diff y = G(u) + H(v)
.\]
Imposing initial conditions at $t = 0$, which is when $u = v = x$, we get
\begin{align*}
	\phi(x, 0) &= G(x) + H(x) = f(x), \\
	\phi_t(x, 0) &= -c G'(x) + c H'(x) = g(x).
\end{align*}
Differentiating the first equation,
\[
	G'(x) + H'(x) = f'(x)
.\]
Now we can solve these simultaneous equations to get
\[
H'(x) = \frac{1}{2} \biggl(f'(x) + \frac{1}{c}g(x) \biggr), \qquad G'(x) = \frac{1}{2}\biggl(f'(x) - \frac{1}{c} g(x) \biggr)
.\]
Now we can integrate to find
\[
	H(x) = \frac{1}{2}(f(x) - f(0)) + \frac{1}{2c} \int_{0}^{x}g(y) \diff y, \quad G(x) = \frac{1}{2}(f(x) + f(0)) - \frac{1}{2c} \int_{0}^{x} g(y) \diff y
.\]
Putting these together, we find that
\[
	\phi(x, t) = G(x - ct) + H(x + ct) = \frac{1}{2}(f(x - ct) + f(x + ct)) + \frac{1}{2c} \int_{x-ct}^{x+ct}g(y) \diff y
.\]

Note the waves propagate at a velocity of $c$, so $\phi(x, t)$ is fully determined by the values of $f, g$ in the interval $[x-ct, x+ct]$.

\newpage

\section{Solving PDEs with Green's Functions}%
\label{sec:solving_pdes_with_green_s_functions}

\subsection{Diffusion Equation and Fourier Transforms}%
\label{sub:diffusion_equation_and_fourier_transforms}

Recall the heat equation for a conducting wire:
\[
	\frac{\partial \theta}{\partial t}(x, t) - D \frac{\partial^2 \theta}{\partial x^2}(x, t) = 0
,\]
with initial conditions $\theta(x, 0) = h(x)$ and boundary conditions $\theta \to 0$ as $x \to \pm \infty$. Take the Fourier transform with respect to $x$, then
\[
	\frac{\partial}{\partial t}\tilde \theta(k, t) = - D k^2 \tilde \theta (k, t)
.\]
Integrating with respect to $t$, we find
\[
	\tilde \theta(k, t) = C e^{-D k^2 t}
,\]
with initial conditions $\theta(k, 0) = \tilde h(k)$. Hence, we get
\[
	\tilde \theta(k, t) = \tilde h(k) e^{-D k^2 t}
.\]
Now $\tilde \theta$ is the product of $\tilde h$ and $e^{-Dk^2t}$, which is a Gaussian, hence using product convolution theorem,
\begin{align*}
	\theta(x, t) &= \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde h(k) e^{-D k^2 t} e^{ikx} \diff k = \frac{1}{\sqrt{4 \pi D t}} \int_{-\infty}^{\infty} h(u) \exp \biggl( - \frac{(x - u)^2}{4 D t} \biggr) \diff u \\
	&= \int_{-\infty}^{\infty}h(u) S_d(x - u, t) \diff u,
\end{align*}
where the \textit{fundamental solution}\index{fundamental solution} is
\[
	S_d(x, t) = \frac{1}{\sqrt{4 \pi D t}} e^{-x^2/4Dt}
.\]
This is also known as the \textit{diffusion kernel}\index{diffusion kernel} or \textit{source function}\index{source function}. In the case $\theta(x, 0) = \theta_0 \delta(x)$, then
\[
	\theta(x, t) = \theta_0 S_d(x, t) = \frac{\theta_0}{\sqrt{4 \pi D t}} e^{- \eta^2}
,\]
where $\eta = x/2\sqrt{Dt}$ is the similarity parameter.

\begin{exbox}[Gaussian Pulse]
	Suppose initially we have
	\[
		f(x) = \sqrt{\frac{a}{\pi}} \theta_0 e^{-ax^2}
	.\]
	Then this implies
	\begin{align*}
		\theta(x, t) &= \frac{\theta_0\sqrt a}{\sqrt{4 \pi^2 D t}} \int_{-\infty}^{\infty} \!\exp \biggl[ - au^2 - \frac{(x - u)^2}{4 D t} \biggr] \diff u \\
			     &= \frac{\theta_0 \sqrt a}{\sqrt{4 \pi^2 D t}} \int_{-\infty}^{\infty} \!\exp \biggl[ - \frac{(1 + 4 a D t)u^2 - 2 x u + x^2}{4 D t} \biggr] \diff u \\
			     &= \frac{\theta_0 \sqrt a}{\sqrt{4 \pi^2 D t}} \int_{-\infty}^{\infty} \!\exp \biggl[ - \frac{(1 + 4aDt)}{4Dt} \biggl(u - \frac{x}{1 + 4aDt} \biggr)^2-\frac{ax^2}{1 + 4 a D t} \biggr] \diff u \\
			     &= \theta_0 \sqrt{\frac{a}{\pi(1 + 4aDt)}} \exp \biggl[ \frac{-ax^2}{1 + 4aDt}\biggr].
	\end{align*}
	Here the asymptotic width spreads proportional to $\sqrt t$, and moreover the area is constant (i.e. heat energy is conserved).
\end{exbox}

\subsection{Forced Diffusion Equation}%
\label{sub:forced_diffusion_equation}

Consider the diffusion equation with a forcing term,
\[
	\frac{\partial \theta}{\partial t}(x, t) - D \frac{\partial^2 \theta}{\partial x^2}(x, t) = f(x, t)
,\]
with homogeneous initial conditions $\theta(x, 0) = 0$.

We construct a 2-dimensional Green's function $G(x, t; \xi, \tau)$ such that
\[
	\frac{\partial G}{\partial t} - D \frac{\partial^2 G}{\partial x^2} = \delta(x - \xi) \partial(t - \tau)
,\]
with $G(x, 0; \xi, \tau) = 0$. Taking the Fourier transform with respect to $x$, we have
\[
	\frac{\partial \tilde G}{\partial t} + D k^2 \tilde G = e^{-i k \xi} \delta(t - \tau)
.\]
Using the multiplicative factor $e^{Dk^2t}$, we find
\[
	\frac{\partial}{\partial t} \bigl[ e^{Dk^2t} \tilde G \bigr] = e^{-ik\xi + Dk^2t} \delta(t - \tau)
.\]
Integrating with respect to $t$, using $G = 0$ at $t = 0$,
\[
	e^{Dk^2t} \tilde G = e^{-ik \xi} \int_{0}^{t} e^{Dk^2t'}\delta(t' - \tau) \diff t' = e^{-ik\xi} e^{Dk^2\tau}H(t - \tau)
.\]
Hence we find that
\[
	\tilde G(k, t; \xi, \tau) = H(t - \tau) e^{-ik \xi}e^{-Dk^2(t - \tau)}
.\]
inverting, we get the Green's function, letting $x' = x - \xi$ and $t' = t - \tau$,
\begin{align*}
	G(x, t;\xi, \tau) &= \frac{H(t - \tau)}{2 \pi} \int_{-\infty}^{\infty} e^{ik(x - \xi)}e^{-Dk^2(t - \tau)}\diff k \\
			  &= \frac{H(t')}{2 \pi}\int_{-\infty}^{\infty}e^{ikx'}e^{-Dk^2t'} \diff k \\
			  &= \frac{H(t')}{\sqrt{4 \pi D t'}}e^{-x'^2/4Dt'} \\
			  &= H(t - \tau) S_d(x - \xi, t - \tau),
\end{align*}
where $S_d$ is the fundamental solution. The general solution is then
\begin{align*}
	\theta(x, t) &= \int_{0}^{\infty}\int_{-\infty}^{\infty}G(x, t;\xi, \tau) f(\xi, \tau) \diff \xi \diff \tau \\
		     &= \int_{0}^{t}\int_{-\infty}^{\infty}f(u, \tau) S_d(x - u, t - \tau) \diff u \diff \tau.
\end{align*}
This looks very similar to the solution to the initial condition problem, and this is not a coincidence.

This is an example of \textit{Duhamel's principle}\index{Duhamel's principle}, relating the solution of the forced PDE with homogeneous boundary conditions, to solutions of the homogeneous PDE with inhomogeneous boundary conditions. Recall the solution to the homogeneous PDE with initial conditions at $t = \tau$ is
\[
	\theta(x, t) = \int_{-\infty}^{\infty}f(u) S_d (x - u, t - \tau) \diff u
.\]
So the forcing term at $t = \tau$ acts as an initial condition for subsequence evolution. The integral for the forced PDE is then a superposition over all times $0 < t < \tau$.

\subsection{Forced Wave Equation}%
\label{sub:forced_wave_equation}

We make the wave equation inhomogeneous. The equation is then
\[
	\frac{\partial^2 \phi}{\partial t^2} - c^2 \frac{\partial^2 \phi}{\partial x^2} = f(x, t)
,\]
with $\phi(x, 0) = 0$ and $\phi_t(x, 0) = 0$. Construct the Green's solution
\[
	\frac{\partial^2 G}{\partial t^2} - c^2 \frac{\partial^2 G}{\partial x^2} = \delta(x - \xi)\partial(t - \tau)
,\]
with $G = 0$, $G_t = 0$ at $t = 0$. Taking the Fourier transform,
\[
	\frac{\partial^2}{\partial t^2} \tilde G + c^2 k^2 \tilde G = e^{-ik\xi}\delta(t - \tau)
.\]
By inspection, we find that
\[
\tilde G =
\begin{cases}
	0 & t < \tau \\
	e^{-ik\xi} \frac{\sin kc(t - \tau)}{kc} & t > \tau
\end{cases} = e^{-ik\xi} \frac{\sin kc(t - \tau)}{kc} H(t - \tau).
\]
Inverting the Fourier transform,
\begin{align*}
	G(x, t;\xi, \tau) &= \frac{H(t - \tau)}{2\pi c} \int_{-\infty}^{\infty} e^{ik(t - \xi)} \frac{\sin kc(t - \tau)}{k} \diff k \\
			  &= \frac{H(t - \tau)}{\pi c} \int_{0}^{\infty} \frac{\cos k A \sin k B}{k} \diff k \\
			  &= \frac{H(t - \tau)}{2 \pi c}\int_{0}^{\infty} \frac{\sin k(A + B) - \sin k(A - B)}{k} \diff k \\
			  &= \frac{H(t - \tau)}{2 \pi c}[\sgn(A + B) - \sgn(A - B)],
\end{align*}
where $A = x - \xi$, $B = c(t - \tau) > 0$. This is only non-zero if $|A| < B$. So the Green's function or \textit{causal fundamental solution}\index{causal fundamental solution} is
\[
	G(x, t; \xi, \tau) = \frac{1}{2c}H(c(t - \tau) - |x - \xi|)
.\]
The solution is then
\begin{align*}
	\phi(x, t) &= \int_{0}^{\infty}\int_{-\infty}^{\infty}f(\xi, t) G(x, t; \xi, \tau) \diff \xi \diff \tau \\
		   &= \frac{1}{2c} \int_{0}^{t} \int_{x-c(t - \tau)}^{x+c(t + \tau)}f(\xi, \tau) \diff \xi \diff \tau.
\end{align*}

\subsection{Poisson's Equation}%
\label{sub:poisson_s_equation}

Now we look at Poisson's equation
\[
	\nabla^2 \phi = - \rho(\mathbf{r})
,\]
on some domain $D$ with Dirichlet boundary conditions $\phi = 0$ on $\partial D$.

When we look at the fundamental solution, we note that the delta function $\delta(\mathbf{r})$ in $\mathbb{R}^3$ has the following properties.

First, $\delta(\mathbf{r} - \mathbf{r}') = 0$ for all $\mathbf{r} \neq \mathbf{r}'$. Then,
\[
	\int_{D}\delta(\mathbf{r} - \mathbf{r}') \Diff3 \mathbf{r} = 
	\begin{cases}
		1 & \mathbf{r}' \in D, \\
		0 & \text{otherwise}.
	\end{cases}
\]
Then, similarly we have
\[
	\int_{D}f(\mathbf{r})\delta(\mathbf{r} - \mathbf{r}')\Diff3 \mathbf{r} = f(\mathbf{r}')
.\]
The \textit{free-space Green's function}\index{free-space Green's function} is defined to be
\[
	\nabla^2 G_{FS}(\mathbf{r}, \mathbf{r}') = \delta(\mathbf{r} - \mathbf{r}')
,\]
with homogeneous boundary conditions $G \to 0$ as $r \to \infty$ on $\mathbb{R}^3$. Now this is spherically symmetric, so $G(\mathbf{r}, \mathbf{r}') = G(|\mathbf{r} - \mathbf{r}'|)$. Suppose $\mathbf{r}' = 0$, so $G = G(r)$. Then integrating over a ball $B$ with radius $r$ around $\mathbf{r}' = 0$, then the LHS is
\[
\int_{B}\nabla^2 G_{FS} \Diff3 \mathbf{r} = \int_{S} \nabla G_{FS} \cdot \mathbf{n} \diff S = \int_{S} \frac{\partial G_{FS}}{\partial r} r^2 \diff \Omega = 4\pi r^2 \frac{\partial G_{FS}}{\partial r}
,\]
by the divergence theorem, and by the RHS,
\[
	\int_{B}\delta(\mathbf{r})\Diff3 \mathbf{r} = 1
.\]
Hence we have an ODE
\[
\frac{\partial G_{FS}}{\partial r} = \frac{1}{4 \pi r^2} \implies G_{FS} = \frac{-1}{4 \pi r} + c
.\]
By the homogeneous boundary conditions, $c = 0$. Hence the free-space Green's function is
\[
	G(\mathbf{r}, \mathbf{r}') = \frac{-1}{4\pi|\mathbf{r} - \mathbf{r}'|}
.\]

In two dimensions, we derive
\[
	G_2(\mathbf{r}, \mathbf{r}') = - \frac{1}{2\pi} \log(|\mathbf{r} - \mathbf{r}'|) + c_2
.\] 

\subsection{Green's Identities}%
\label{sub:green_s_identities}

We consider two scalar functions $\phi, \psi$, twice differentiable on $D$. Then,
\begin{align*}
	\int_{D} \nabla \cdot(\phi \nabla \psi) \Diff3 \mathbf{r} &= \int_{D}(\phi \nabla^2 \psi + \nabla \phi \cdot \nabla \psi)\Diff3 \mathbf{r} \\
								  &= \int_{\partial D} \phi \nabla \psi \cdot \mathbf{n} \diff S,
\end{align*}
by the divergence theorem. This is \textit{Green's first identity}\index{Green's identities}. Now, if we switch $\phi$ and $\psi$ and subtract, we get \textit{Green's second identity}
\[
	\int_{\partial D} \biggl( \phi \frac{\partial \psi}{\partial n} - \psi \frac{\partial \phi}{\partial n} \biggr) \diff S = \int_{D} (\phi \nabla^2 \psi - \psi \nabla^2 \phi) \Diff3 \mathbf{r}
.\]
We want to integrate our Green's function over $D$. To do this, we excise a small spherical ball $B_{\eps}$ about $\mathbf{r}'$, which we may set to be $0$. Take $\phi$ that satisfies the Poisson equation (i.e. $\nabla^2 \phi = - \rho$) and $\psi = G_{FS}(\mathbf{r}, \mathbf{r}')$. Then, the RHS of Green's identity is
\[
	\int_{D - B_{\eps}}(\phi \nabla^2 G_{FS} - G_{FS} \nabla^2 \phi) \Diff3 \mathbf{r} = \int_{D - B_{\eps}}G_{FS} \rho \Diff3 \mathbf{r}
,\]
and the LHS gives
\[
	\int_{\partial D} \biggl( \phi \frac{\partial G_{FS}}{\partial n} - G_{FS} \frac{\partial \phi}{\partial n} \biggr) \diff S + \int_{S_{\eps}} \biggl( \phi \frac{\partial G_{FS}}{\partial n} - G_{FS} \frac{\partial \phi}{\partial n} \biggr) \diff S
.\]
The second integral is over a small sphere $S_{\eps}$, so we may take a limit as $\eps \to 0$. Then, it becomes
\[
	\biggl( \bar \phi \frac{-1}{4 \pi \eps^2} - \frac{1}{4 \pi \eps} \bar{\frac{\partial \phi}{\partial r}} \biggr) 4 \pi \eps^2
,\]
where we put a bar over a variable to indicate its average value. Then, as $\eps \to 0$, this value approaches $- \phi(0)$. Combining these, we get \textit{Green's third identity}
\[
	\phi(\mathbf{r}') = \int_{D}G_{FS}(\mathbf{r}, \mathbf{r}')(-\rho(\mathbf{r}))\Diff3 \mathbf{r} + \int_{\partial D} \biggl( \phi(\mathbf{r}) \frac{\partial G_{FS}}{\partial n}(\mathbf{r}, \mathbf{r}') - G_{FS}(\mathbf{r}, \mathbf{r}') \frac{\partial \phi}{\partial n}(\mathbf{r}) \biggr) \diff S
.\]

\subsection{Dirichlet Green's Function}%
\label{sub:dirichlet_green_s_function}

We wish to solve $\nabla^2 \phi = - \rho$ on $D$ with inhomogeneous boundary conditions $\phi(\mathbf{r}) = h(\mathbf{r})$ on $\partial D$.

To solve, we introduce the Dirichlet Green's function\index{Dirichlet Green's function}, which satisfies
\begin{enumerate}[(i)]
	\item $\nabla^2 G(\mathbf{r}, \mathbf{r}') = 0$ for all $\mathbf{r} \neq \mathbf{r}'$;
	\item $G(\mathbf{r}, \mathbf{r}') = 0$ on the boundary $\partial D$;
	\item $G(\mathbf{r}, \mathbf{r}') = G_{FS}(\mathbf{r},\mathbf{r}') + H(\mathbf{r},\mathbf{r}')$ with $\nabla^2 H(\mathbf{r}, \mathbf{r}') = 0$ for all $\mathbf{r} \in D$.
\end{enumerate}

Using Green's second identity with $\phi$, $H$, we get
\[
	\int_{\partial D}\biggl( \phi \frac{\partial H}{\partial n} - H \frac{\partial \phi}{\partial n} \biggr) \diff S = \int_{D} H \rho \Diff3 \mathbf{r}
.\]
Now using $G_{FS} = G - H$ in Green's third identity,
\[
	\phi(\mathbf{r}') = \int_{D}(G - H)(-\rho)\Diff3 \mathbf{r} + \int_{\partial D} \biggl( \phi \frac{\partial(G - H)}{\partial n} - (G - H) \frac{\partial \phi}{\partial n}\biggr) \diff S
.\]
Subtracting the $H$ terms above, we get
\[
	\phi(\mathbf{r}') = \int_{D}G(\mathbf{r}, \mathbf{r}')(-\rho(\mathbf{r}))\Diff3 \mathbf{r} + \int_{\partial D}h(\mathbf{r}) \frac{\partial G(\mathbf{r}, \mathbf{r}')}{\partial n}\diff S
.\]

We can use Green's second identity to show that $G$ is symmetric.

For Neumann boundary conditions, if we specify
\[
	\frac{\partial \phi}{\partial n} = k(\mathbf{r}) \text{ on } \partial D
,\]
we have
\[
	\phi(\mathbf{r}') = \int_{D}G(\mathbf{r},\mathbf{r}')(-\rho(\mathbf{r})) \Diff3 \mathbf{r} + \int_{\partial D}G(\mathbf{r}, \mathbf{r}')(-k(\mathbf{r})) \diff S
.\]

\subsection{Method of Images}%
\label{sub:method_of_images}\index{method of images}

For symmetric domains $D$, we can construct Green's functions with $G = 0$ on $\partial D$, by cancelling the boundary potential with an opposite mirror image Green's function placed outside $D$.

\subsubsection{Laplace's equation on the Half-Space}%
\label{subsub:laplace_s_equation_on_the_half_space}

Suppose we want to solve $\nabla^2 \phi = 0$ on $D = \{(x, y, z) \mid z > 0\}$ with $\phi(x, y, 0) = h(x, y)$, and $\phi \to 0$ as $|\mathbf{r}| \to \infty$. Now, our regular free-space Green's function satisfies $G_{FS}(\mathbf{r}; \mathbf{r}') \to 0$ as $|\mathbf{r}| \to \infty$, but have $G_{FS} \neq 0$ at $z = 0$.

To combat this, for the $G_{FS}$ at $\mathbf{r}' = (x', y', z')$, we subtract the $G_{FS}$ at $\mathbf{r}'' = (x', y', -z')$, so
\begin{align*}
	G(\mathbf{r}, \mathbf{r}') &= \frac{-1}{4 \pi|\mathbf{r} - \mathbf{r}'|} - \frac{-1}{4 \pi |\mathbf{r}- \mathbf{r}''|} = 0 \text{ when } z = 0,
\end{align*}
so on $\partial D$, $G(\mathbf{r}, \mathbf{r}') = 0$. Now we have
\begin{align*}
	\frac{\partial G}{\partial n} \biggr|_{z = 0} &= \frac{\partial G}{\partial z} \biggr|_{z = 0} = -\frac{1}{4\pi} \biggl( \frac{z - z'}{|\mathbf{r}-\mathbf{r}'|^3} - \frac{z + z'}{|\mathbf{r} - \mathbf{r}'|^3} \biggr) \\
						      &= \frac{z'}{2\pi}((x - x')^2 + (y-y')^2 + z'^2)^{-3/2}.
\end{align*}
The solution is then
\[
	\Phi(x',y',z') = \frac{z'}{2\pi} \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} ((x-x')^2 + (y-y')^2 + z'^2)^{-3/2} h(x, y) \diff x \diff y
.\]

\subsubsection{Wave equation on a Semi-infinite Line}%
\label{subsub:wave_equation_on_a_semi_infinite_line}

Consider $\ddot \phi - c^2 \phi'' = f(x, t)$ for $x > 0$, with Dirichlet boundary conditions $\phi(0, t) = 0$.

We create a matching Green's function with an appropriate sign centred at $x = - \xi$, so
\[
	G(x, t; \xi, \tau) = \frac{1}{2c}H(c(t - \tau) - |x - \xi|) - \frac{1}{2c}H(c(t - \tau) - |x + \xi|)
.\]
Solving for $f \equiv 0$ and an initial Gaussian pulse, we get
\[
	\phi(x, t) = \exp [(x - \xi + ct)^2] - \exp[(-x-\xi+ct)^2]
.\]
The solution is a left-travelling wave, which cancels with another image at $t = \xi/c$. This emerges and travels right as the reflected wave.

Hence this can be thought of in two ways:
\begin{itemize}
	\item The wave coming from $x < 0$ becoming visible.
	\item The left-travelling wave reflecting and changing phase by $\pi$.
\end{itemize}


\newpage

\printindex

\end{document}
