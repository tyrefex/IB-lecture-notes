%%% TO DO %%%


\documentclass[12pt]{article}

\usepackage{ishn}

\makeindex[intoc]

\begin{document}

\hypersetup{pageanchor=false}
\begin{titlepage}
	\begin{center}
		\vspace*{1em}
		\Huge
		\textbf{IB Statistics}

		\vspace{1em}
		\large
		Ishan Nath, Lent 2023

		\vspace{1.5em}

		\Large

		Based on Lectures by Dr. Sergio Bacallado

		\vspace{1em}

		\large
		\today
	\end{center}
	
\end{titlepage}
\hypersetup{pageanchor=true}

\tableofcontents

\newpage

\section{Introduction}
\label{sec:introduction}

\emph{Statistics} is the science of making informed decisions. It can include:
\begin{itemize}
	\item Design of experiments,
	\item Graphical exploration of data,
	\item Formal statistical inference (part of Decision theory),
	\item Communication of results.
\end{itemize}

Let $X_1, X_2, \ldots, X_n$ be independent observations from a distribution $f(x\mid\theta)$, with parameter $\theta$. We wish to make inferences about the value of $\theta$ from $X_1, X_2, \ldots, X_n$. Such inference can include:
\begin{itemize}
	\item Estimating $\theta$,
	\item Quantifying uncertainty in estimates,
	\item Testing a hypothesis about $\theta$.
\end{itemize}

\subsection{Probability Review}
\label{sub:probability_review}

Let $\Omega$ be the \emph{sample space}\index{sample space} of outcomes in an experiment. A measurable subset of $\Omega$ is called an \emph{event}\index{event}. We denote the set of events as $\mathcal{F}$.

A function $\mathbb{P} : \mathcal{F} \to [0, 1]$ is called a \emph{probability measure}\index{probability measure} if:
\begin{itemize}
	\item $\mathbb{P}(\emptyset) = 0$,
	\item $\mathbb{P}(\Omega) = 1$,
	\item $\mathbb{P}(\bigcup_{i} A_i) = \sum_{i} \mathbb{P}(A_i)$, if $(A_i)$ are disjoint and countable.
\end{itemize}

A \emph{random variable}\index{random variable} is a (measurable) function $X : \Omega \to \mathbb{R}$.

The \emph{distribution function}\index{distribution function} of $X$ is
\[
F_X(x) = \mathbb{P}(X \leq x)
.\]
A \emph{discrete random variable}\index{discrete random variable} takes values in a countable subset $E \subset \mathbb{R}$, and its \emph{probability mass function}\index{probability mass function} or pmf is $p_X(x) = \mathbb{P}(X = x)$.

We say $X$ has \emph{continuous} distribution\index{continuous random variable} if it has a \emph{probability density function}\index{probability density function} or pdf, satisfying
\[
\mathbb{P}(X \in A) = \int_{A}f_X(x) \diff x
,\]
for any measurable $A$. The \emph{expectation}\index{expectation} of $X$ is defined
\[
\mathbb{E}[X] =
\begin{cases}
	\sum_{x \in X}x \cdot p_X(x) & X \text{ discrete}, \\
	\int x \cdot f_X(x) \diff x & X \text{ continuous}.
\end{cases}
\]
If $g : \mathbb{R} \to \mathbb{R}$, then
\[
\mathbb{E}[g(x)] = \int g(x) f_X(x) \diff x
.\]
The \emph{variance}\index{variance} of $X$ is
\[
\Var(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]
.\]
We say that $X_1, X_2, \ldots, X_n$ are \emph{independent}\index{independence} if for all $x_1, x_2, \ldots, x_n$,
 \[
\mathbb{P}(X_1 \leq x_1, \ldots, X_n \leq x_n) = \mathbb{P}(X_1 \leq x_1) \cdots \mathbb{P}(X_n \leq x_n)
.\]
If the variables have probability density functions, then
\[
f_X(x) = \prod_{i = 1}^{n}f_{X_i}(x_i)
,\]
where $X$ is the vector of variables $(X_1, \ldots, X_n)$ and $x$ is the vector $(x_1, \ldots, x_n)$.

Importantly, if $a_1, \ldots, a_n \in \mathbb{R}$,
\[
\mathbb{E}[a_1 X_1 + \cdots + a_n X_n] = a_1 \mathbb{E}[X_1] + \cdots + a_n \mathbb{E}[X_n]
.\]
Moreover,
\[
\Var(a_1 X_1 + \cdots + a_n X_n) = \sum_{i,j}a_{i}a_j \Cov(X_i, X_j)
.\]
Here the \emph{covariance}\index{covariance} of $X_i$ and $X_j$ is
\[
\Cov(X_i, X_j) = \mathbb{E}[(X_i - \mathbb{E}[X_i])(X_j - \mathbb{E}[X_j])]
.\]
If $X = (X_1, \ldots, X_n)^{T}$ and $\mathbb{E}[X] = (\mathbb{E}[X_1], \ldots, \mathbb{E}[X_n])$, then the linearity of expectation can be rewritten as
\[
\mathbb{E}[a^{T}X] = a^{T}\mathbb{E}[X]
,\]
and moreover
\[
\Var(a^{T}X) = a^{T} \Var(X) a
,\]
where $\Var(X)$ is the \emph{covariance matrix}: $(\Var(X))_{ij} = \Cov(X_i, X_j)$.

\subsection{Moment Generating Functions}

The \emph{moment generating function} of a variable $X$ is
\[
M_X(t) = \mathbb{E}[e^{tx}]
.\]
This may only exist for $t$ in some neighbourhood of $0$. The important properties of MGFs is that
\[
	\mathbb{E}[X^{n}] = \frac{\Diff{n}}{\diff t^{n}} M_X(0)
,\]
and from this we obtain $M_X = M_Y \iff F_x = F_y$.

MGFs also make it easy to find the distribution function of sums of iid variables.

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $\Poisson(\mu)$. Then
	\begin{align*}
		M_{X_1}(t) &= \mathbb{E}[e^{tX_1}] = \sum_{x = 0}^{\infty} e^{tx} \cdot \frac{e^{-\mu}\mu^{x}}{x!} \\
			   &= e^{-\mu} \sum_{x = 0}^{\infty} \frac{(e^{t}\mu)^{x}}{x!} = e^{-\mu}e^{\mu \exp(t)} = e^{-\mu(1-e^{t})}.
	\end{align*}
	If $S_n = X_1 + \cdots + X_n$, then
	\begin{align*}
		M_{S_n}(t) &= \mathbb{E}[e^{t(X_1 + \cdots + X_n)}] = \prod_{i=1}^{n}\mathbb{E}[e^{tX_i}] \\
			   &= e^{-\mu(1-e^{t})n}
	\end{align*}
	This is the same as a $\Poisson(\mu n)$ MGF, so $S_n \sim \Poisson(\mu \cdot n)$.
\end{exbox}

\subsection{Limit Theorems}
\label{sub:limit_theorems}

We list some important limit theorems, starting with the \emph{weak law of large numbers}\index{weak law of large numbers} (WLLN). This says if $X_1, \ldots, X_n$ are iid with $\mathbb{E}[X_1] = \mu$, then let $\overline{X_n} = \frac{1}{n} \sum_{i = 1}^{n}X_i$ be the sample mean. WLLN says that for all $\eps > 0$,
\[
\mathbb{P}(|\overline{X_n} - \mu| > \eps) \to 0
,\]
as $n \to \infty$.

The \emph{strong law of large numbers}\index{strong law of large numbers} (SLLN) says a stronger result, namely
\[
\mathbb{P}(\overline{X_n} \to \mu) = 1
,\]
i.e. $\overline{X_n}$ converges to $\mu$ almost surely.

The \emph{central limit theorem}\index{central limit theorem} is another important limit theorem. If we take
\[
Z_n = \frac{\sqrt n (\overline{X_n} - \mu)}{\sigma}
,\]
where $\sigma^2 = \Var(X_i)$, then $Z_n$ is ``approximately'' $N(0,1)$ as $n \to \infty$.

What this means is that $\mathbb{P}(Z_n \leq z) \to \Phi(z)$ as $n \to \infty$ for all $z \in \mathbb{R}$, where $\Phi$ is the distribution function of a $N(0,1)$ variable.

\subsection{Conditioning}
\label{sub:conditioning}

Let $X$ and $Y$ be discrete random variables. Their \emph{joint pmf}\index{joint probability mass function} is
\[
p_{X,Y}(x, y) = \mathbb{P}(X = x, Y = y)
.\]
The \emph{marginal pmf}\index{marginal probability mass function} is
\[
p_X(x) = \mathbb{P}(X = x) = \sum_{y \in Y}p_{X,Y}(x, y)
.\]
The \emph{conditional pmf}\index{conditional probability mass function} of $X$ given $Y = y$ is
\[
p_{X\mid Y}(x\mid y) = \mathbb{P}(X = x \mid Y = y) = \frac{\mathbb{P}(X = x, Y = y)}{\mathbb{P}(Y = y)} = \frac{p_{X,Y}(x, y)}{p_Y(y)}
.\]
This is defined to be $0$ if $p_Y(y) = 0$.

For continuous random variables $X$, $Y$, the \emph{joint pdf}\index{joint probability density function} $f_{X,Y}$ has
\[
\mathbb{P}(X \leq x', y \leq y') = \int_{-\infty}^{x'}\int_{-\infty}^{y'} f_{X,Y}(x,y) \diff y \diff x
.\]
The \emph{marginal pdf}\index{marginal probability density function} of $Y$ is
\[
f_Y(y) = \int_{-\infty}^{\infty}f_{X,Y}(x,y) \diff x
.\]
The \emph{conditional pdf}\index{conditional probability density function} of $X$ given $Y$ is
\[
f_{X\mid Y}(x \mid y) = \frac{f_{X,Y}(x, y)}{f_Y(y)}
.\]

The \emph{conditional expectation}\index{conditional expectation} is given by
\[
\mathbb{E}[X \mid Y] =
\begin{cases}
	\sum_{x} x \cdot p_{X \mid Y}(x \mid y) & X,Y \text{ discrete},\\
	\int_{-\infty}^{\infty}x \cdot f_{X\mid Y}(x\mid y) \diff x & X,Y \text{ continuous}.
\end{cases}
\]
This is a random variable, which is a function of $Y$. The \emph{tower property}\index{tower property} says that
\[
\mathbb{E}[\mathbb{E}[X \mid Y]] = \mathbb{E}[X]
.\]
Hence we can write the variance of $X$ as follows:
\begin{align*}
	\Var(X) &= \mathbb{E}[X^2] - (\mathbb{E}[X])^2 \\
		&= \mathbb{E}[\mathbb{E}[X^2 \mid Y]] - (\mathbb{E}[\mathbb{E}[X \mid Y]])^2 \\
		&= \mathbb{E}[\mathbb{E}[X^2\mid Y] - (\mathbb{E}[X \mid Y])^2] + \mathbb{E}[\mathbb{E}[X\mid Y]^2] - \mathbb{E}[\mathbb{E}[X\mid Y]]^2 \\
		&= \mathbb{E}[\Var(X\mid Y)] + \Var(\mathbb{E}[X\mid Y]).
\end{align*}

\subsection{Change of Variables}
\label{sub:change_of_variables}

The \emph{change of variables}\index{change of variables} formula is as follows:

Let $(x, y) \mapsto (u, v)$ be a differentiable bijection. Then,
\begin{align*}
	f_{U,V}(u,v) = f_{X,Y}(x(u,v),y(u,v)) \cdot |\!\det J|, \\
	J = \frac{\partial (x, y)}{\partial (u, v)} =
	\begin{pmatrix}
		\partial x/\partial u & \partial x/\partial v \\
		\partial y/\partial u & \partial y/\partial v
	\end{pmatrix}.
\end{align*}

\subsection{Important Distributions}
\label{sub:important_distributions}

$X \sim \Negbin(k, p)$ if $X$ models the time in successive iid $\Ber(p)$ trials to achieve $k$ successes. If $k = 1$, this is the same as a geometric distribution.

$X \sim \Poisson(\lambda)$ is the limit of $\Bin(n, \lambda/n)$ random variables, as $n \to \infty$.

If $X_i \sim \Gamma(\alpha_i,\lambda)$ for $i = 1, \ldots, n$ with $X_1, \ldots, X_n$ independent, then if $S_n = X_1 + \cdots + X_n$,
\begin{align*}
	M_{S_n}(t) &= \prod_{i = 1}^{n} M_{X_i}(t) = \biggl( \frac{\lambda}{\lambda - 1} \biggr)^{\alpha_1 + \cdots + \alpha_n}
\end{align*}
which is the mgf of a $\Gamma(\sum \alpha_i, \lambda)$ random variable. Hence $S_n \sim \Gamma(\sum \alpha_i, \lambda)$.

Also, if $X \sim \Gamma(a, \lambda)$, then for any $b \in (0, \infty)$, $bX \sim \Gamma(a, \lambda/b)$.

Special cases of the Gamma distribution include $\Gamma(1, \lambda) = \Exp(\lambda)$, and $\Gamma(\frac{k}{2}, \frac{1}{2}) = \chi_k^2$, the Chi-squared distribution with $k$ degrees of freedom. This can be thought of as the sum of $k$ independent squared $N(0,1)$ random variables.

\newpage

\section{Estimation}
\label{sec:estimation}

Suppose we observe data $X_1, X_2, \ldots, X_n$, which are iid from some pdf (or pmf) $f_X(x \mid \theta)$, with $\theta$ unknown. We let $X = (X_1, \ldots, X_n)$.

\begin{definition}
	An \emph{estimator}\index{estimator} is a statistic or a function of the data $T(X) = \hat \theta$, which we use to approximate the true parameter $\theta$. The distribution of $T(X)$ is called the \emph{sampling distribution}\index{sampling distribution}.
\end{definition}

\begin{exbox}
	If $X_1, \ldots, X_n$ are iid $N(\mu, 1)$, we can define an estimator for the mean as
	\[
	\hat \mu = T(X) = \frac{1}{n} \sum_{i = 1}^{n}X_i
	.\]
	The sampling distribution of $\hat \mu$ is $N(\mu, \frac{1}{n})$.
\end{exbox}

\begin{definition}
	The \emph{bias}\index{bias} of $\hat \theta = T(X)$ is
	\[
	\bias(\hat \theta) = \mathbb{E}_\theta[\hat \theta] - \theta
	.\]
\end{definition}

\begin{remark}
	In general, the bias is a function of $\theta$, even if the notation $\bias(\hat \theta)$ does not make that explicit.
\end{remark}

\begin{definition}
	We say that $\hat \theta$ is \emph{unbiased}\index{unbiased estimator} if $\bias(\hat \theta) = 0$ for all $\theta \in \Theta$.
\end{definition}

\begin{exbox}
	Out previous estimator 
	\[
	\hat \mu = \frac{1}{n} \sum_{i = 1}^{n}X_i
	\]
	is unbiased because $\mathbb{E}_\mu[\hat \mu] = \mu$ for all $\mu \in \mathbb{R}$.
\end{exbox}

\begin{definition}
	The \emph{mean squared error}\index{mean squared error} (mse) of $\hat \theta$ is
	\[
	\mse(\hat \theta) = \mathbb{E}_\theta[(\hat \theta - \theta)^2]
	.\]
\end{definition}

Like the bias, the mean squared error of $\hat \theta$ is a function of $\theta$.

\subsection{Bias-Variance Decomposition}
\label{sub:bias_variance_decomposition}

We can write the mean squared error as
\begin{align*}
	\mse(\hat \theta) &= \mathbb{E}_\theta[(\hat \theta - \theta)^2] = \mathbb{E}_\theta[(\hat \theta - \mathbb{E}_\theta[\hat \theta] + \mathbb{E}_\theta[\hat \theta] - \theta)^2] \\
			  &= \Var_{\theta}(\hat \theta) + \bias^2(\hat \theta) + 2\underbrace{[\mathbb{E}_\theta[(\hat \theta - \mathbb{E}_\theta[\hat \theta])]]}_{0}(\mathbb{E}_\theta[\hat \theta] - \theta).
\end{align*}

The two terms on the right hand side are non-negative, so there is a trade off between bias and variance.

\begin{exbox}
	Let $X \sim \Bin(n, \theta)$, where $n$ is known, and we wish to estimate $\theta$. The standard estimator is
	\[
	T_u = \frac{X}{n}, \quad \mathbb{E}_\theta[T_u] = \frac{\mathbb{E}_\theta[X]}{n} = \theta
	.\]
	Hence $T_u$ is unbiased. We can also calculate the mean squared error as
	\begin{align*}
		\mse(T_u) &= \Var_\theta(T_u) = \frac{\Var_\theta(X)}{n^2} = \frac{n\theta(1-\theta)}{n^2} = \frac{\theta(1-\theta)}{n}.
	\end{align*}
	Consider a second estimator
	\[
	T_B = \frac{X+1}{n+2} = w\frac{X}{n} + (1-w)\frac{1}{2}
	,\]
	for $w = \frac{n}{n+2}$. In this case $T_B$ is interpolating between our unbiased estimator, and the constant estimator. The bias of $T_B$ is
	\[
	\bias(T_B) = \mathbb{E}_\theta[T_B] - \theta = \mathbb{E}[\frac{X+1}{n+2}] - \theta = \frac{1}{n+2} - \frac{2}{n+2}\theta
	.\]
	This is not equal to zero for all but one value of $\theta$. Hence, $T_B$ is biased. We can also calculate the variance
	\begin{align*}
		\Var_{\theta}(T_B) &= \frac{1}{(n+2)^2}n\theta(1-\theta) - w^2 \frac{\theta(1-\theta)}{n}, \\
		\mse(T_B) &= \Var_{\theta}(T_B) + \bias^2(T_B) \\
			  &= w^2\frac{\theta(1-\theta)}{n} + (1-w)^2 \biggl(\frac{1}{2} - \theta\biggr)^2.
	\end{align*}
	Hence the $\mse$ of the biased estimator is a weighted average of the $\mse$ of the unbiased estimator, and a parabola. For $\theta$ around $1/2$, the biased estimator has a lower $\mse$ than the unbiased estimator.
\end{exbox}

The message here is that our prior judgements about $\theta$ affect our choice of estimator, and unbiasedness is not always desirable.

\begin{exbox}
	Suppose $X \sim \Poisson(\lambda)$. We wish the estimate $\theta = \mathbb{P}(X = 0)^2 = e^{-2\lambda}$. For an estimator $T(X)$ to be unbiased, we must have for all $\lambda$,
	\begin{align*}
		\mathbb{E}_\lambda[\hat \theta] &= \sum_{x = 0}^{\infty}T(x) \frac{e^{-\lambda} \lambda^{x}}{x!} = e^{-2 \lambda} = \theta \\
						&\iff \sum_{x = 0}^{\infty}T(x) \frac{\lambda^{x}}{x!} = e^{-\lambda} = \sum_{x = 0}^{\infty}(-1)^{x} \frac{\lambda^{x}}{x!}.
	\end{align*}
	For this to hold for all $\lambda \geq 0$, we should take $T(X) = (-1)^{X}$. But this estimator makes no sense.
\end{exbox}

\subsection{Sufficiency}
\label{sub:sufficiency}

Suppose $X_1, \ldots, X_n$ are iid random variables from a distribution with pdf (or pmf) $f_X(\cdot \mid \theta)$. Let $X = (X_1, \ldots, X_n)$.

The question is: is there a statistic $T(X)$ which contains all the information in $X$ needed to estimate $\theta$?

\begin{definition}
	A statistic $T$ is \emph{sufficient}\index{sufficiency} for $\theta$ if the conditional distribution of $X$ given $T(X)$ does not depend on $\theta$.
\end{definition}

Note $\theta$ and $T(X)$ may be vector-valued.

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $\Ber(\theta)$ for $\theta \in [0,1]$. Then,
	\begin{align*}
		f_X(\cdot \mid \theta) = \prod_{i = 1}^{n} \theta^{x_i} (1 - \theta)^{1 - x_i} = \theta^{x_1 + \cdots + x_n}(1 - \theta)^{n - x_1 - \cdots - x_n}.
	\end{align*}
	This only depends on $X$ through
	 \[
	T(X) = \sum_{i = 1}^{n} x_i
	.\]
	Indeed, for $x$ with $x_1 + \cdots + x_n = t$,
	\begin{align*}
		f_{X \mid T = t}(x \mid T(x) = t) &= \frac{\mathbb{P}_\theta(X = x, T(X) = t)}{\mathbb{P}_\theta(T(X) = t)} = \frac{\mathbb{P}_\theta(X=x)}{\mathbb{P}\theta(T(x) = t)} \\
						  &= \frac{\theta^{x_1 + \cdots + x_n}(1 - \theta)^{n - x_1 - \cdots - x_n}}{\binom{n}{t}\theta^{t}(1 - \theta)^{n-t}} = \binom{n}{t}^{-1},
	\end{align*}
	and otherwise this probability is $0$. As this doesn't depend on $\theta$, $T(X)$ is sufficient for $\theta$.
\end{exbox}

\begin{theorem}[Factorization criterion]\index{factorization criterion}
	$T$ is sufficient for $\theta$ if and only if
	\[
	f_X(x \mid \theta) = g(T(x), \theta) \cdot h(x)
	,\]
	for suitable functions $g, h$.
\end{theorem}

\begin{proofbox}
	We only do the discrete case.

	Suppose that $f_X(x\mid \theta) = g(T(x),\theta) h(x)$. If $T(x) = t$, then
	\begin{align*}
		f_{X \mid T=t}(x \mid T = t) &= \frac{\mathbb{P}_\theta(X = x, T(X) = t)}{\mathbb{P}_\theta(T(X) = t)} \\
					       &= \frac{g(T(x), \theta)h(x)}{\sum_{T(x') = t} g(T(x'),\theta) h(x')} \\
					       &= \frac{g(t, \theta)}{g(t, \theta)} \cdot \frac{h(x)}{\sum_{T(x') = t} h(x')}.
	\end{align*}
	This doesn't depend on $\theta$, so $T(X)$ is sufficient. Conversely, if $T(X)$ is sufficient, then
	\begin{align*}
		\mathbb{P}_\theta(X = x) &= \mathbb{P}_\theta(X=x, T(X) = t) \\
					 &= \underbrace{\mathbb{P}_\theta(T(X) = t)}_{g(t, \theta)} \cdot \underbrace{\mathbb{P}_\theta(X = x \mid T(X) = t)}_{h(x)}.
	\end{align*}
	Therefore the pmf of $X$ factorizes.
\end{proofbox}

\begin{exbox}
	Return to our example from before, where $X_1, \ldots, X_n$ are iid $\Ber(\theta)$. Then
	\[
	f_X(x \mid \theta) = \theta^{x_1+ \cdots + x_n} (1 - \theta)^{n - x_1 - \cdots - x_n}
	.\]
	Hence if we take $g(t, \theta) = \theta^{t}(1-\theta)^{n-t}$, and $h(x) = 1$, we immediately get that $T(X) = \sum x_i$ is sufficient.
\end{exbox}

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $U([0, \theta])$, for $\theta > 0$. Then,
	\begin{align*}
		f_X(x\mid \theta) &= \prod_{i = 1}^{n} \frac{1}{\theta} \mathbbm{1}(X_i \in [0, \theta]) \\
				  &= \underbrace{\frac{1}{\theta^{n}} \mathbbm{1} (\max_{i} x_i \leq \theta)}_{g(T(x), \theta)} \underbrace{\mathbbm{1} (\min_{i} x_i \geq 0)}_{h(x)}.
	\end{align*}
	Hence $T(x) = \max_{i} x_i$ is a sufficient statistic for $\theta$.
\end{exbox}

\subsection{Minimal Sufficiency}
\label{sub:minimal_sufficiency}

Sufficient statistics are not unique. Indeed, any one-to-one function of a sufficient statistic is also sufficient. Also $T(X) = X$ is always sufficient, but not very useful.

\begin{definition}
	A sufficient statistic $T$ is \emph{minimal sufficient}\index{minimal sufficiency} if it is a function of any other sufficient statistic, so if $T'$ is also sufficient, then
	\[
	T'(x) = T'(y) \implies T(x) = T(y)
	,\]
	for all $x, y$ in our space.
\end{definition}

By this definition, any two minimal sufficient statistics $T, T'$ are in bijection with each other, so
\[
T(x) = T(y) \iff T'(x) = T'(y)
.\]

\begin{theorem}
	Suppose that $T(X)$ is a statistic such that
	\[
	\frac{f_X(x \mid \theta)}{f_X(y \mid \theta)}
	\]
	is constant as a function of $\theta$, if and only if $T(x) = T(y)$. Then $T$ is minimal sufficient.
\end{theorem}

Let $x \overset{1}{\sim} y$ if
\[
\frac{f_X(x \mid \theta)}{f_X(y \mid \theta)}
\]
is constant in $\theta$. It is easy to check that $\overset{1}{\sim}$ is an equivalence relation.

Similarly, for a given statistic $T$, $x \overset{2}{\sim} y$ if $T(x) = T(y)$ defines another equivalence relation.

The condition of the theorem says that $\overset{1}{\sim}$ and $\overset{2}{\sim}$ are the same for minimal sufficient statistics.

\begin{remark}
	We can always construct a statistic $T$ which is constant on the equivalence classes of $\overset{1}{\sim}$, which by the theorem is minimal sufficient.
\end{remark}

\begin{proofbox}
	For any value of $T$, let $z_t$ be a representative from the equivalence class
	\[
		\{x \mid T(x) = t\}
	.\]
	Then,
	\[
	f_X(x \mid \theta) = f_X(z_{T(x)}\mid \theta) \frac{f_X(x, \theta)}{f_X(z_{T(x)} \mid \theta)}
	.\]
	This is exactly in the form $g(T(x), t) h(x)$, so by the factorization criterion $T$ is sufficient.

	To prove that $T$ is minimal, take any other sufficient statistic $S$. We want to show that if $S(x) = S(y)$, then $T(x) = T(y)$.

	By the factorization criterion, there are functions $g_s, h_s$ such that
	\[
	f_X(x, \theta) = g_s(S(x), \theta) h_s(x)
	.\]
	Suppose $S(x) = S(y)$. Then the ratio
	\[
	\frac{f_X(x \mid \theta)}{f_X(y \mid \theta)} = \frac{g_s(S(x), \theta) h_s(x)}{g_s(S(y), \theta) h_s(y)} = \frac{h_s(x)}{h_s(y)}
	,\]
	is independent of $\theta$. Hence $x \overset{1}{\sim} y$. By the hypothesis, we get that $T(x) = T(y)$.
\end{proofbox}

\begin{remark}
	Sometimes the range of $X$ depends on $\theta$. In this case we can interpret
	\[
		\frac{f_X(x \mid \theta)}{f_Y(y \mid \theta)} \text{ constant in } \theta
	,\]
	to mean that
	\[
	f_X(x \mid \theta) = c(x, y) f_X(y \mid \theta)
	,\]
	for some function $c$ which does not depend on $\theta$.
\end{remark}

\begin{exbox}
	Suppose that $X_1, \ldots, X_n$ are iid $N(\mu, \sigma^2)$, with parameters $(\mu, \sigma^2)$ unknown. Then,
	\begin{align*}
	\frac{f_X(x \mid t)}{f_X(y \mid t)} &= \frac{(2 \pi \sigma^2)^{-n/2} \exp (-\frac{1}{2 \sigma^2} \sum (x_i - \mu)^2)}{(2 \pi \sigma^2)^{-n/2} \exp (-\frac{1}{2 \sigma^2} \sum (y_i - \mu)^2)} \\
					    &= \exp \biggl[ - \frac{1}{2 \sigma^2} \biggl(\sum x_i^2 - \sum y_i^2\biggr) + \frac{\mu}{\sigma^2} \biggl(\sum x_i - \sum y_i\biggr) \biggr].
	\end{align*}
	Hence if $\sum x_i^2 = \sum y_i^2$ and $\sum x_i = \sum y_i$, this ratio does not depend on $(\mu, \sigma^2)$. The converse is also true: if the ratio does not depend on $(\mu, \sigma^2)$, then we must have $\sum x_i^2 = \sum y_i^2$ and $\sum x_i = \sum y_i$. By the theorem, $T(x) = (\sum x_i^2, \sum x_i)$ is minimal sufficient.

	Recall that bijections of $T$ are also minimal sufficient. A more common way of expressing a minimal sufficient statistic in this model is $S(X) = (\bar X, S_{x x})$, where
	\[
	\bar X = \frac{1}{n} \sum_{i} X_i, \qquad S_{x x} = \sum_{i} (X_i - \bar X)^2
	.\]
	In this example, $(\mu, \sigma^2)$ and $T(X)$ are both 2-dimensional. In general, the parameter and sufficient statistic can have different dimensions.

	For example, if $X_1, \ldots, X_n$ are iid $N(\mu, \mu^2)$, where $\mu \geq 0$, then the minimal sufficient statistic is $S(X) = (\bar X, S_{x x})$.
\end{exbox}

\subsection{Rao-Blackwell Theorem}
\label{sub:rao_blackwell_theorem}

So far we have written $\mathbb{E}_{\theta}$ and $\mathbb{P}_{\theta}$ to denote the expectations and probabilities in the model where $X_1, \ldots, X_n$ are iid drawn from $f_X(\cdot \mid \theta)$. From now on, we drop the subscript $\theta$.

\begin{theorem}[Rao-Blackwell Theorem]\index{Rao-Blackwell theorem}
	Let $T$ be a sufficient statistic for $\theta$. Let $\tilde \theta$ be some estimator for $\theta$, with $\mathbb{E}[\tilde \theta^2] < \infty$ for all $\theta$. Define a new estimator $\hat \theta = \mathbb{E}[\tilde \theta \mid T(X)]$. Then, for all $\theta$,
	\[
	\mathbb{E}[(\hat \theta - \theta)^2] \leq \mathbb{E}[(\tilde \theta - \theta)^2]
	,\]
	with equality if and only if $\tilde \theta$ is a function of $T(X)$.
\end{theorem}

\begin{remark}
	$\hat \theta$ is a valid estimator, as it does not depend on $\theta$, only on $X$, as $T$ is sufficient:
	\[
	\hat \theta (T(x)) = \int \tilde \theta(x) f_{X|T}(x|T) \diff x
	,\]
	where neither $\tilde \theta$ nor the conditional distribution depend on $\theta$.
\end{remark}

The message is that we can improve the mean squared error of any estimator $\tilde \theta$ by taking a conditional expectation given $T(X)$.

\begin{proofbox}
	By the tower property,
	\[
	\mathbb{E}[\hat \theta] = \mathbb{E}[\mathbb{E}[\tilde \theta \mid T]] = \mathbb{E}[\tilde \theta]
	.\]
	So $\bias(\hat \theta) = \bias(\tilde \theta)$ for all $\theta$. By the conditional variance formula,
	\begin{align*}
		\Var(\tilde \theta) &= \mathbb{E}[\Var(\tilde \theta \mid T)] + \Var(\mathbb{E}[\tilde \theta \mid T]) \\
				    &= \mathbb{E}[\Var(\tilde \theta \mid T)] + \Var(\hat \theta).
	\end{align*}
	Hence $\Var (\tilde \theta) \geq \Var (\hat \theta)$ for all $\theta$. Hence $\mse(\tilde \theta) \geq \mse(\hat \theta)$.

	Note that $\Var(\tilde \theta \mid T) > 0$ with some positive probability unless $\tilde \theta$ is a function of $T(X)$. So $\mse(\tilde \theta) > \mse(\hat \theta)$ unless $\tilde \theta$ is a function of $T(X)$.
\end{proofbox}

\begin{exbox}
	Say $X_1, \ldots, X_n$ are iid $\Poisson(\lambda)$. We wish to estimate $\theta = \mathbb{P}(X_1 = 0) = e^{-\lambda}$. Then
	\begin{align*}
		f_X(x \mid \lambda) &= \frac{e^{-n \lambda}\lambda^{x_1 + \cdots + x_n}}{x_1! \cdots x_n!} \\
				    &= \frac{\theta^{n} (- \log \theta)^{x_1 + \cdots + x_n}}{x_1! \cdots x_n!}
	\end{align*}
	Letting $h(x) = 1/(x_1! \cdots x_n!)$, $g(T(x), \theta) = \theta^{n}(- \log \theta)^{T(x)}$, by the factorization criterion, $T(x) = \sum x_i$ is a sufficient statistic.
	Let $\tilde \theta = \mathbbm{1}(X_i = 0)$. This is unbiased, but only uses one observation $X_1$. Using Rao-Blackwell, we can find
	\begin{align*}
		\hat \theta &= \mathbb{E}[\tilde \theta \mid T = t] = \mathbb{P}\biggl(X_1 = 0 \biggm| \sum_{i= 1}^{n} X_i = t\biggr) \\
			    &= \frac{\mathbb{P}(X_1 = 0, X_1 + \cdots + X_n = t)}{\mathbb{P}(X_1 + \cdots + X_n = t)} = \frac{\mathbb{P}(X_1 = 0, X_2 + \cdots + X_n = t)}{\mathbb{P}(X_1 + \cdots + X_n = t)} \\
			    &= \frac{\mathbb{P}(X_1 = 0) \mathbb{P}(X_2 + \cdots + X_n = t)}{\mathbb{P}(X_1 + \cdots + X_n = t)} = \frac{e^{-\lambda} \mathbb{P}(\Poisson((n-1) \lambda) = t)}{\mathbb{P}(\Poisson(n \lambda) = t)} \\
			    &= \frac{e^{-n \lambda} ((n-1)\lambda)^{t}/t!}{e^{-n \lambda} (n \lambda)^{t}/t!} = \biggl( 1 - \frac{1}{n} \biggr)^{t}.
	\end{align*}
	So $\hat \theta = (1 - \frac{1}{n})^{x_1 + \cdots + x_n}$ is an estimator which by the Rao-Blackwell theorem has $\mse(\hat \theta) < \mse(\tilde \theta)$.

	As $n \to \infty$,
	\[
		\hat \theta = \biggl(1 - \frac{1}{n} \biggr)^{n \bar x} \overset{n \to \infty}{\to} e^{-\bar x}
	,\]
	and by the strong law of large numbers
	\[
	\bar x \to \mathbb{E}[X_1] = \lambda
	.\]
	so $\hat \theta \to e^{-\lambda}$.
\end{exbox}

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $U([0, \theta])$ where $\theta$ is unknown and $\theta \geq 0$. Then recall $T(X) = \max_{i} X_i$ is sufficient for $\theta$.

	Let $\tilde \theta = 2 X_1$, which is unbiased. Then,
	\begin{align*}
		\hat \theta &= \mathbb{E}[\tilde \theta \mid T = t] = 2 \mathbb{E}[X_1 \mid \max_{i} X_i = t] \\
			    &= 2 \mathbb{E}[X_1 \mid \max_{i} X_i = t, \max_{i} X_i = X_1] \mathbb{P}(\max_{i} X_i = X_1 \mid \max_{i} X_i = t) \\
			    &+ 2 \mathbb{E}[X_1 \mid \max_{i}X_i = t, \max_{i} X_i \neq X_1] \mathbb{P}(\max_{i}X_i \neq X_1 \mid \max_{i}X_i = t) \\
			    &= \frac{2t}{n} + \frac{2(n-1)}{n} \mathbb{E}[X_1 \mid X_1 \leq t, \max_{i>1} X_i = t] = \frac{2t}{n} + \frac{2(n-1)}{n} \frac{t}{2} = \frac{n+1}{n} t.
	\end{align*}
	So $\hat \theta = \frac{n+1}{n} \max_{i} X_i$ is a valid estimator with $\mse(\hat \theta) < \mse(\tilde \theta)$.
\end{exbox}

\subsection{Maximum Likelihood Estimation}
\label{sub:maximum_likelihood_estimation}

Let $X = (X_1, \ldots, X_n)$ have joint pdf (or pmf) $f_X(X \mid \theta)$.

\begin{definition}
	The likelihood function is
	\[
	L : \theta \mapsto f_X(X \mid \theta)
	.\]
\end{definition}

The \emph{maximum likelihood estimator}\index{maximum likelihood estimator} is any value of $\theta$ maximizing $L(\theta)$.

If $X_1, \ldots, X_n$ are iid each with pdf (or pmf) $f_X(\cdot \mid \theta)$, then
\[
L(\theta) = \prod_{i=1}^{n} f_X(x_i \mid \theta)
.\]
We will denote the logarithm
\[
\ell(\theta) = \log L (\theta) = \sum_{i = 1}^{n} \log f_X(x_i \mid \theta)
.\]

\begin{exbox}
	If $X_1, \ldots, X_n$ are iid $\Ber(\theta)$, then
	\[
	\ell(\theta) = \Biggl(\sum x_i\Biggr) \log \theta = \Biggl(n - \sum_{x_i} \Biggr) \log(1 - \theta)
	,\]
	and the derivative
	\[
	\frac{\partial \ell}{\partial \theta} = \frac{\sum x_i}{\theta} - \frac{n - \sum x_i}{1 - \theta}
	.\]
	This is zero if and only if $\theta = \frac{1}{n} \sum x_i = \bar X$.

	Hence $\bar X$ is the maximum likelihood estimator for $\theta$, and is unbiased as $\mathbb{E}[\bar X] = 0$.
\end{exbox}

\begin{exbox}
	If $X_1, \ldots, X_n$ are iid $N(\mu, \sigma^2)$, then
	\[
	\log (\mu, \sigma^2) = - \frac{n}{2} \log(2 \pi) - \frac{n}{2} \log \sigma^2 - \frac{1}{2 \sigma^2} \sum_{i = 1}^{n} (x_i - \mu)^2
	.\]
	This is maximized when $\partial \ell/\partial \mu = \partial \ell/\partial \sigma^2 = 0$. First, we get
	\[
	\frac{\partial \ell}{\partial \mu} = - \frac{1}{\sigma^2} \sum_{i = 1}^{n}(x_i - \mu)
	,\]
	which is equal to zero when $\mu = \bar X$. Then
	\[
	\frac{\partial \ell}{\partial \sigma^2} = - \frac{n}{2 \sigma^2} + \frac{1}{2 \sigma^{4}} \sum_{i = 1}^{n} (x_i - \mu)^{2}
	.\]
	This is zero when
	\[
	\sigma^2 = \frac{1}{n} \sum_{i = 1}^{n} (x_i - \bar x)^2 = \frac{1}{n} S_{x x}
	.\]
	Hence $(\hat \mu, \hat \sigma^2) = (\bar X, S_{x x}/n)$ give the maximum likelihood estimator in this model.

	Note that $\hat \mu = \bar X$ is unbiased. Now we want to see if $\hat \sigma^2$ is biased. We could compute it directly, but later in the course we will show that
	\[
	\frac{S_{x x}}{\sigma^2} = \frac{n \hat \sigma^2}{\sigma^2} \sim \chi^2_{n-1}
	,\]
	hence
	\[
	\mathbb{E}[\hat \sigma^2] = \mathbb{E}[\chi^2_{n-1}] \frac{\sigma^2}{n} = \frac{n-1}{n} \sigma^2 \neq \sigma^2
	,\]
	which is biased, but asymptotically unbiased.
\end{exbox}

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $U([0,\theta])$. Then
	\[
		L(\theta) = \frac{1}{\theta^{n}} \mathbbm{1}(\max_{i} X_i \leq \theta)
	.\]
	We can see from the plot that $\hat \theta_{\rm{mle}} = \max_{i} X_i$ is the maximum likelihood estimator for $\theta$. We also started from an unbiased estimator, and using Rao-Blackwellization we found an estimator
	\[
	\hat \theta = \frac{n+1}{n} \max_i X_i
	.\]
	This is also unbiased. So in this model the mle is biased as
	\[
		\mathbb{E}[[\hat \theta_{\rm{mle}}] = \mathbb{E}\biggl[ \frac{n+1}{n} \hat \theta \biggr] = \frac{n}{n+1} \theta
	,\]
	however it is asymptotically unbiased.
\end{exbox}

The maximum likelihood estimator has the following properties:

\begin{enumerate}[1.]
	\item If $T$ is a sufficient statistic, then the maximum likelihood estimator is a function of $T(X)$. By the factorization criterion,
		\[
		L(\theta) = g(T(X), \theta) h(X)
		.\]
		If $T(x) = T(y)$, then the likelihood function with data $x$ and $y$ is the same up to a multiplicative constant. Hence the maximum likelihood estimator in each case is the same.
	\item If  $\phi = h(\theta)$ where $h$ is a bijection, then the maximum likelihood estimator of $\phi$ is
		\[
		\hat \phi = h(\hat \theta)
		,\]
		where $\hat \theta$ is the maximum likelihood estimator of $\theta$.
	\item Asymptotically, we have normality. This says $\sqrt n (\hat \theta - \theta)$ is approximately normal with mean $0$ when $n$ is large. Under some regularity conditions, for a measurable set $A$,
		\[
			\mathbb{P}(\sqrt n (\hat \theta - \theta) \in A) \overset{n \to \infty}{\to} \mathbb{P}(Z \in A)
		,\]
		where $Z \sim N(0, \Sigma)$. This holds for all regular values of $\theta$.

		Here $\Sigma$ is some function of $\ell$, and there is a theorem (Cramer-Rao) which says this is the smallest variance attainable.
	\item Sometimes, if the maximum likelihood estimator is not available analytically, we can find it numerically.
\end{enumerate}

\subsection{Confidence Intervals}
\label{sub:confidence_intervals}

\begin{definition}
	A $(100 \cdot \gamma)$\% \emph{confidence interval}\index{confidence interval} for a parameter $\theta$ is a random interval $(A(X), B(X))$ such that
	\[
	\mathbb{P}(A(X) \leq \theta \leq B(X)) = \gamma
	,\]
	for all values of $\theta$.
\end{definition}

The frequentist interpretation of the confidence interval is:
\begin{center}
	There exists some fixed true parameter $\theta$. We repeat the experiment many times. On average, $100 \cdot \gamma$\% of the time the interval $(A(X), B(X))$ contains $\theta$.
\end{center}

The incorrect interpretation is:
\begin{center}
	Having observed $X = x$, there is a probability $\gamma$ that $\theta$ is in $(A(x), B(x))$.
\end{center}

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $N(\theta, 1)$. To find a 95\% confidence interval for $\theta$, we know that
	\[
	\bar X = \frac{1}{n} \sum_{x_i} \sim N\biggl(\theta, \frac{1}{n} \biggr)
	.\]
	Hence
	\[
		Z = \sqrt{n}(\bar X - \theta) \sim N(0,1)
	.\]
	$Z$ has this distribution for all $\theta$. Then let $z_1, z_2$ be any two numbers such that $\Phi(z_2) - \Phi(z_1) = 0.95$. Then,
	\[
	\mathbb{P}(z_1 \leq \sqrt n (\bar X - \theta) \leq z_2) = 0.95
	.\]
	Rearranging,
	\[
	\mathbb{P}\biggl( \bar X - \frac{z_2}{\sqrt n} \leq \theta \leq \bar X - \frac{z_1}{\sqrt n} \biggr) = 0.95
	.\]
	Therefore $(\bar X - \frac{z_2}{\sqrt{n}}, \bar X - \frac{z_1}{\sqrt n})$ is a 95\% confidence interval.

	There are multiple ways to choose $z_1, z_2$. Usually we minimise the width of the interval, which is achieved by $z_1 = \Phi^{-1} (0.025), z_2 = \Phi^{-1}(0.975)$.
\end{exbox}


\newpage

\printindex

\end{document}
