%%% TO DO %%%


\documentclass[12pt]{article}

\usepackage{ishn}

\makeindex[intoc]

\begin{document}

\hypersetup{pageanchor=false}
\begin{titlepage}
	\begin{center}
		\vspace*{1em}
		\Huge
		\textbf{IB Statistics}

		\vspace{1em}
		\large
		Ishan Nath, Lent 2023

		\vspace{1.5em}

		\Large

		Based on Lectures by Dr. Sergio Bacallado

		\vspace{1em}

		\large
		\today
	\end{center}
	
\end{titlepage}
\hypersetup{pageanchor=true}

\tableofcontents

\newpage

\section{Introduction}
\label{sec:introduction}

\emph{Statistics} is the science of making informed decisions. It can include:
\begin{itemize}
	\item Design of experiments,
	\item Graphical exploration of data,
	\item Formal statistical inference (part of Decision theory),
	\item Communication of results.
\end{itemize}

Let $X_1, X_2, \ldots, X_n$ be independent observations from a distribution $f(x\mid\theta)$, with parameter $\theta$. We wish to make inferences about the value of $\theta$ from $X_1, X_2, \ldots, X_n$. Such inference can include:
\begin{itemize}
	\item Estimating $\theta$,
	\item Quantifying uncertainty in estimates,
	\item Testing a hypothesis about $\theta$.
\end{itemize}

\subsection{Probability Review}
\label{sub:probability_review}

Let $\Omega$ be the \emph{sample space}\index{sample space} of outcomes in an experiment. A measurable subset of $\Omega$ is called an \emph{event}\index{event}. We denote the set of events as $\mathcal{F}$.

A function $\mathbb{P} : \mathcal{F} \to [0, 1]$ is called a \emph{probability measure}\index{probability measure} if:
\begin{itemize}
	\item $\mathbb{P}(\emptyset) = 0$,
	\item $\mathbb{P}(\Omega) = 1$,
	\item $\mathbb{P}(\bigcup_{i} A_i) = \sum_{i} \mathbb{P}(A_i)$, if $(A_i)$ are disjoint and countable.
\end{itemize}

A \emph{random variable}\index{random variable} is a (measurable) function $X : \Omega \to \mathbb{R}$.

The \emph{distribution function}\index{distribution function} of $X$ is
\[
F_X(x) = \mathbb{P}(X \leq x)
.\]
A \emph{discrete random variable}\index{discrete random variable} takes values in a countable subset $E \subset \mathbb{R}$, and its \emph{probability mass function}\index{probability mass function} or pmf is $p_X(x) = \mathbb{P}(X = x)$.

We say $X$ has \emph{continuous} distribution\index{continuous random variable} if it has a \emph{probability density function}\index{probability density function} or pdf, satisfying
\[
\mathbb{P}(X \in A) = \int_{A}f_X(x) \diff x
,\]
for any measurable $A$. The \emph{expectation}\index{expectation} of $X$ is defined
\[
\mathbb{E}[X] =
\begin{cases}
	\sum_{x \in X}x \cdot p_X(x) & X \text{ discrete}, \\
	\int x \cdot f_X(x) \diff x & X \text{ continuous}.
\end{cases}
\]
If $g : \mathbb{R} \to \mathbb{R}$, then
\[
\mathbb{E}[g(x)] = \int g(x) f_X(x) \diff x
.\]
The \emph{variance}\index{variance} of $X$ is
\[
\Var(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]
.\]
We say that $X_1, X_2, \ldots, X_n$ are \emph{independent}\index{independence} if for all $x_1, x_2, \ldots, x_n$,
 \[
\mathbb{P}(X_1 \leq x_1, \ldots, X_n \leq x_n) = \mathbb{P}(X_1 \leq x_1) \cdots \mathbb{P}(X_n \leq x_n)
.\]
If the variables have probability density functions, then
\[
f_X(x) = \prod_{i = 1}^{n}f_{X_i}(x_i)
,\]
where $X$ is the vector of variables $(X_1, \ldots, X_n)$ and $x$ is the vector $(x_1, \ldots, x_n)$.

Importantly, if $a_1, \ldots, a_n \in \mathbb{R}$,
\[
\mathbb{E}[a_1 X_1 + \cdots + a_n X_n] = a_1 \mathbb{E}[X_1] + \cdots + a_n \mathbb{E}[X_n]
.\]
Moreover,
\[
\Var(a_1 X_1 + \cdots + a_n X_n) = \sum_{i,j}a_{i}a_j \Cov(X_i, X_j)
.\]
Here the \emph{covariance}\index{covariance} of $X_i$ and $X_j$ is
\[
\Cov(X_i, X_j) = \mathbb{E}[(X_i - \mathbb{E}[X_i])(X_j - \mathbb{E}[X_j])]
.\]
If $X = (X_1, \ldots, X_n)^{T}$ and $\mathbb{E}[X] = (\mathbb{E}[X_1], \ldots, \mathbb{E}[X_n])$, then the linearity of expectation can be rewritten as
\[
\mathbb{E}[a^{T}X] = a^{T}\mathbb{E}[X]
,\]
and moreover
\[
\Var(a^{T}X) = a^{T} \Var(X) a
,\]
where $\Var(X)$ is the \emph{covariance matrix}: $(\Var(X))_{ij} = \Cov(X_i, X_j)$.

\subsection{Moment Generating Functions}

The \emph{moment generating function} of a variable $X$ is
\[
M_X(t) = \mathbb{E}[e^{tx}]
.\]
This may only exist for $t$ in some neighbourhood of $0$. The important properties of MGFs is that
\[
	\mathbb{E}[X^{n}] = \frac{\Diff{n}}{\diff t^{n}} M_X(0)
,\]
and from this we obtain $M_X = M_Y \iff F_x = F_y$.

MGFs also make it easy to find the distribution function of sums of iid variables.

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $\Poisson(\mu)$. Then
	\begin{align*}
		M_{X_1}(t) &= \mathbb{E}[e^{tX_1}] = \sum_{x = 0}^{\infty} e^{tx} \cdot \frac{e^{-\mu}\mu^{x}}{x!} \\
			   &= e^{-\mu} \sum_{x = 0}^{\infty} \frac{(e^{t}\mu)^{x}}{x!} = e^{-\mu}e^{\mu \exp(t)} = e^{-\mu(1-e^{t})}.
	\end{align*}
	If $S_n = X_1 + \cdots + X_n$, then
	\begin{align*}
		M_{S_n}(t) &= \mathbb{E}[e^{t(X_1 + \cdots + X_n)}] = \prod_{i=1}^{n}\mathbb{E}[e^{tX_i}] \\
			   &= e^{-\mu(1-e^{t})n}
	\end{align*}
	This is the same as a $\Poisson(\mu n)$ MGF, so $S_n \sim \Poisson(\mu \cdot n)$.
\end{exbox}

\subsection{Limit Theorems}
\label{sub:limit_theorems}

We list some important limit theorems, starting with the \emph{weak law of large numbers}\index{weak law of large numbers} (WLLN). This says if $X_1, \ldots, X_n$ are iid with $\mathbb{E}[X_1] = \mu$, then let $\overline{X_n} = \frac{1}{n} \sum_{i = 1}^{n}X_i$ be the sample mean. WLLN says that for all $\eps > 0$,
\[
\mathbb{P}(|\overline{X_n} - \mu| > \eps) \to 0
,\]
as $n \to \infty$.

The \emph{strong law of large numbers}\index{strong law of large numbers} (SLLN) says a stronger result, namely
\[
\mathbb{P}(\overline{X_n} \to \mu) = 1
,\]
i.e. $\overline{X_n}$ converges to $\mu$ almost surely.

The \emph{central limit theorem}\index{central limit theorem} is another important limit theorem. If we take
\[
Z_n = \frac{\sqrt n (\overline{X_n} - \mu)}{\sigma}
,\]
where $\sigma^2 = \Var(X_i)$, then $Z_n$ is ``approximately'' $N(0,1)$ as $n \to \infty$.

What this means is that $\mathbb{P}(Z_n \leq z) \to \Phi(z)$ as $n \to \infty$ for all $z \in \mathbb{R}$, where $\Phi$ is the distribution function of a $N(0,1)$ variable.

\subsection{Conditioning}
\label{sub:conditioning}

Let $X$ and $Y$ be discrete random variables. Their \emph{joint pmf}\index{joint probability mass function} is
\[
p_{X,Y}(x, y) = \mathbb{P}(X = x, Y = y)
.\]
The \emph{marginal pmf}\index{marginal probability mass function} is
\[
p_X(x) = \mathbb{P}(X = x) = \sum_{y \in Y}p_{X,Y}(x, y)
.\]
The \emph{conditional pmf}\index{conditional probability mass function} of $X$ given $Y = y$ is
\[
p_{X\mid Y}(x\mid y) = \mathbb{P}(X = x \mid Y = y) = \frac{\mathbb{P}(X = x, Y = y)}{\mathbb{P}(Y = y)} = \frac{p_{X,Y}(x, y)}{p_Y(y)}
.\]
This is defined to be $0$ if $p_Y(y) = 0$.

For continuous random variables $X$, $Y$, the \emph{joint pdf}\index{joint probability density function} $f_{X,Y}$ has
\[
\mathbb{P}(X \leq x', y \leq y') = \int_{-\infty}^{x'}\int_{-\infty}^{y'} f_{X,Y}(x,y) \diff y \diff x
.\]
The \emph{marginal pdf}\index{marginal probability density function} of $Y$ is
\[
f_Y(y) = \int_{-\infty}^{\infty}f_{X,Y}(x,y) \diff x
.\]
The \emph{conditional pdf}\index{conditional probability density function} of $X$ given $Y$ is
\[
f_{X\mid Y}(x \mid y) = \frac{f_{X,Y}(x, y)}{f_Y(y)}
.\]

The \emph{conditional expectation}\index{conditional expectation} is given by
\[
\mathbb{E}[X \mid Y] =
\begin{cases}
	\sum_{x} x \cdot p_{X \mid Y}(x \mid y) & X,Y \text{ discrete},\\
	\int_{-\infty}^{\infty}x \cdot f_{X\mid Y}(x\mid y) \diff x & X,Y \text{ continuous}.
\end{cases}
\]
This is a random variable, which is a function of $Y$. The \emph{tower property}\index{tower property} says that
\[
\mathbb{E}[\mathbb{E}[X \mid Y]] = \mathbb{E}[X]
.\]
Hence we can write the variance of $X$ as follows:
\begin{align*}
	\Var(X) &= \mathbb{E}[X^2] - (\mathbb{E}[X])^2 \\
		&= \mathbb{E}[\mathbb{E}[X^2 \mid Y]] - (\mathbb{E}[\mathbb{E}[X \mid Y]])^2 \\
		&= \mathbb{E}[\mathbb{E}[X^2\mid Y] - (\mathbb{E}[X \mid Y])^2] + \mathbb{E}[\mathbb{E}[X\mid Y]^2] - \mathbb{E}[\mathbb{E}[X\mid Y]]^2 \\
		&= \mathbb{E}[\Var(X\mid Y)] + \Var(\mathbb{E}[X\mid Y]).
\end{align*}

\subsection{Change of Variables}
\label{sub:change_of_variables}

The \emph{change of variables}\index{change of variables} formula is as follows:

Let $(x, y) \mapsto (u, v)$ be a differentiable bijection. Then,
\begin{align*}
	f_{U,V}(u,v) = f_{X,Y}(x(u,v),y(u,v)) \cdot |\!\det J|, \\
	J = \frac{\partial (x, y)}{\partial (u, v)} =
	\begin{pmatrix}
		\partial x/\partial u & \partial x/\partial v \\
		\partial y/\partial u & \partial y/\partial v
	\end{pmatrix}.
\end{align*}

\subsection{Important Distributions}
\label{sub:important_distributions}

$X \sim \Negbin(k, p)$ if $X$ models the time in successive iid $\Ber(p)$ trials to achieve $k$ successes. If $k = 1$, this is the same as a geometric distribution.

$X \sim \Poisson(\lambda)$ is the limit of $\Bin(n, \lambda/n)$ random variables, as $n \to \infty$.

If $X_i \sim \Gamma(\alpha_i,\lambda)$ for $i = 1, \ldots, n$ with $X_1, \ldots, X_n$ independent, then if $S_n = X_1 + \cdots + X_n$,
\begin{align*}
	M_{S_n}(t) &= \prod_{i = 1}^{n} M_{X_i}(t) = \biggl( \frac{\lambda}{\lambda - 1} \biggr)^{\alpha_1 + \cdots + \alpha_n}
\end{align*}
which is the mgf of a $\Gamma(\sum \alpha_i, \lambda)$ random variable. Hence $S_n \sim \Gamma(\sum \alpha_i, \lambda)$.

Also, if $X \sim \Gamma(a, \lambda)$, then for any $b \in (0, \infty)$, $bX \sim \Gamma(a, \lambda/b)$.

Special cases of the Gamma distribution include $\Gamma(1, \lambda) = \Exp(\lambda)$, and $\Gamma(\frac{k}{2}, \frac{1}{2}) = \chi_k^2$, the Chi-squared distribution with $k$ degrees of freedom. This can be thought of as the sum of $k$ independent squared $N(0,1)$ random variables.

\newpage

\section{Estimation}
\label{sec:estimation}

Suppose we observe data $X_1, X_2, \ldots, X_n$, which are iid from some pdf (or pmf) $f_X(x \mid \theta)$, with $\theta$ unknown. We let $X = (X_1, \ldots, X_n)$.

\begin{definition}
	An \emph{estimator}\index{estimator} is a statistic or a function of the data $T(X) = \hat \theta$, which we use to approximate the true parameter $\theta$. The distribution of $T(X)$ is called the \emph{sampling distribution}\index{sampling distribution}.
\end{definition}

\begin{exbox}
	If $X_1, \ldots, X_n$ are iid $N(\mu, 1)$, we can define an estimator for the mean as
	\[
	\hat \mu = T(X) = \frac{1}{n} \sum_{i = 1}^{n}X_i
	.\]
	The sampling distribution of $\hat \mu$ is $N(\mu, \frac{1}{n})$.
\end{exbox}

\begin{definition}
	The \emph{bias}\index{bias} of $\hat \theta = T(X)$ is
	\[
	\bias(\hat \theta) = \mathbb{E}_\theta[\hat \theta] - \theta
	.\]
\end{definition}

\begin{remark}
	In general, the bias is a function of $\theta$, even if the notation $\bias(\hat \theta)$ does not make that explicit.
\end{remark}

\begin{definition}
	We say that $\hat \theta$ is \emph{unbiased}\index{unbiased estimator} if $\bias(\hat \theta) = 0$ for all $\theta \in \Theta$.
\end{definition}

\begin{exbox}
	Out previous estimator 
	\[
	\hat \mu = \frac{1}{n} \sum_{i = 1}^{n}X_i
	\]
	is unbiased because $\mathbb{E}_\mu[\hat \mu] = \mu$ for all $\mu \in \mathbb{R}$.
\end{exbox}

\begin{definition}
	The \emph{mean squared error}\index{mean squared error} (mse) of $\hat \theta$ is
	\[
	\mse(\hat \theta) = \mathbb{E}_\theta[(\hat \theta - \theta)^2]
	.\]
\end{definition}

Like the bias, the mean squared error of $\hat \theta$ is a function of $\theta$.

\subsection{Bias-Variance Decomposition}
\label{sub:bias_variance_decomposition}

We can write the mean squared error as
\begin{align*}
	\mse(\hat \theta) &= \mathbb{E}_\theta[(\hat \theta - \theta)^2] = \mathbb{E}_\theta[(\hat \theta - \mathbb{E}_\theta[\hat \theta] + \mathbb{E}_\theta[\hat \theta] - \theta)^2] \\
			  &= \Var_{\theta}(\hat \theta) + \bias^2(\hat \theta) + 2\underbrace{[\mathbb{E}_\theta[(\hat \theta - \mathbb{E}_\theta[\hat \theta])]]}_{0}(\mathbb{E}_\theta[\hat \theta] - \theta).
\end{align*}

The two terms on the right hand side are non-negative, so there is a trade off between bias and variance.

\begin{exbox}
	Let $X \sim \Bin(n, \theta)$, where $n$ is known, and we wish to estimate $\theta$. The standard estimator is
	\[
	T_u = \frac{X}{n}, \quad \mathbb{E}_\theta[T_u] = \frac{\mathbb{E}_\theta[X]}{n} = \theta
	.\]
	Hence $T_u$ is unbiased. We can also calculate the mean squared error as
	\begin{align*}
		\mse(T_u) &= \Var_\theta(T_u) = \frac{\Var_\theta(X)}{n^2} = \frac{n\theta(1-\theta)}{n^2} = \frac{\theta(1-\theta)}{n}.
	\end{align*}
	Consider a second estimator
	\[
	T_B = \frac{X+1}{n+2} = w\frac{X}{n} + (1-w)\frac{1}{2}
	,\]
	for $w = \frac{n}{n+2}$. In this case $T_B$ is interpolating between our unbiased estimator, and the constant estimator. The bias of $T_B$ is
	\[
	\bias(T_B) = \mathbb{E}_\theta[T_B] - \theta = \mathbb{E}[\frac{X+1}{n+2}] - \theta = \frac{1}{n+2} - \frac{2}{n+2}\theta
	.\]
	This is not equal to zero for all but one value of $\theta$. Hence, $T_B$ is biased. We can also calculate the variance
	\begin{align*}
		\Var_{\theta}(T_B) &= \frac{1}{(n+2)^2}n\theta(1-\theta) - w^2 \frac{\theta(1-\theta)}{n}, \\
		\mse(T_B) &= \Var_{\theta}(T_B) + \bias^2(T_B) \\
			  &= w^2\frac{\theta(1-\theta)}{n} + (1-w)^2 \biggl(\frac{1}{2} - \theta\biggr)^2.
	\end{align*}
	Hence the $\mse$ of the biased estimator is a weighted average of the $\mse$ of the unbiased estimator, and a parabola. For $\theta$ around $1/2$, the biased estimator has a lower $\mse$ than the unbiased estimator.
\end{exbox}

The message here is that our prior judgements about $\theta$ affect our choice of estimator, and unbiasedness is not always desirable.

\begin{exbox}
	Suppose $X \sim \Poisson(\lambda)$. We wish the estimate $\theta = \mathbb{P}(X = 0)^2 = e^{-2\lambda}$. For an estimator $T(X)$ to be unbiased, we must have for all $\lambda$,
	\begin{align*}
		\mathbb{E}_\lambda[\hat \theta] &= \sum_{x = 0}^{\infty}T(x) \frac{e^{-\lambda} \lambda^{x}}{x!} = e^{-2 \lambda} = \theta \\
						&\iff \sum_{x = 0}^{\infty}T(x) \frac{\lambda^{x}}{x!} = e^{-\lambda} = \sum_{x = 0}^{\infty}(-1)^{x} \frac{\lambda^{x}}{x!}.
	\end{align*}
	For this to hold for all $\lambda \geq 0$, we should take $T(X) = (-1)^{X}$. But this estimator makes no sense.
\end{exbox}

\subsection{Sufficiency}
\label{sub:sufficiency}

Suppose $X_1, \ldots, X_n$ are iid random variables from a distribution with pdf (or pmf) $f_X(\cdot \mid \theta)$. Let $X = (X_1, \ldots, X_n)$.

The question is: is there a statistic $T(X)$ which contains all the information in $X$ needed to estimate $\theta$?

\begin{definition}
	A statistic $T$ is \emph{sufficient}\index{sufficiency} for $\theta$ if the conditional distribution of $X$ given $T(X)$ does not depend on $\theta$.
\end{definition}

Note $\theta$ and $T(X)$ may be vector-valued.

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $\Ber(\theta)$ for $\theta \in [0,1]$. Then,
	\begin{align*}
		f_X(\cdot \mid \theta) = \prod_{i = 1}^{n} \theta^{x_i} (1 - \theta)^{1 - x_i} = \theta^{x_1 + \cdots + x_n}(1 - \theta)^{n - x_1 - \cdots - x_n}.
	\end{align*}
	This only depends on $X$ through
	 \[
	T(X) = \sum_{i = 1}^{n} x_i
	.\]
	Indeed, for $x$ with $x_1 + \cdots + x_n = t$,
	\begin{align*}
		f_{X \mid T = t}(x \mid T(x) = t) &= \frac{\mathbb{P}_\theta(X = x, T(X) = t)}{\mathbb{P}_\theta(T(X) = t)} = \frac{\mathbb{P}_\theta(X=x)}{\mathbb{P}\theta(T(x) = t)} \\
						  &= \frac{\theta^{x_1 + \cdots + x_n}(1 - \theta)^{n - x_1 - \cdots - x_n}}{\binom{n}{t}\theta^{t}(1 - \theta)^{n-t}} = \binom{n}{t}^{-1},
	\end{align*}
	and otherwise this probability is $0$. As this doesn't depend on $\theta$, $T(X)$ is sufficient for $\theta$.
\end{exbox}

\begin{theorem}[Factorization criterion]\index{factorization criterion}
	$T$ is sufficient for $\theta$ if and only if
	\[
	f_X(x \mid \theta) = g(T(x), \theta) \cdot h(x)
	,\]
	for suitable functions $g, h$.
\end{theorem}

\begin{proofbox}
	We only do the discrete case.

	Suppose that $f_X(x\mid \theta) = g(T(x),\theta) h(x)$. If $T(x) = t$, then
	\begin{align*}
		f_{X \mid T=t}(x \mid T = t) &= \frac{\mathbb{P}_\theta(X = x, T(X) = t)}{\mathbb{P}_\theta(T(X) = t)} \\
					       &= \frac{g(T(x), \theta)h(x)}{\sum_{T(x') = t} g(T(x'),\theta) h(x')} \\
					       &= \frac{g(t, \theta)}{g(t, \theta)} \cdot \frac{h(x)}{\sum_{T(x') = t} h(x')}.
	\end{align*}
	This doesn't depend on $\theta$, so $T(X)$ is sufficient. Conversely, if $T(X)$ is sufficient, then
	\begin{align*}
		\mathbb{P}_\theta(X = x) &= \mathbb{P}_\theta(X=x, T(X) = t) \\
					 &= \underbrace{\mathbb{P}_\theta(T(X) = t)}_{g(t, \theta)} \cdot \underbrace{\mathbb{P}_\theta(X = x \mid T(X) = t)}_{h(x)}.
	\end{align*}
	Therefore the pmf of $X$ factorizes.
\end{proofbox}

\begin{exbox}
	Return to our example from before, where $X_1, \ldots, X_n$ are iid $\Ber(\theta)$. Then
	\[
	f_X(x \mid \theta) = \theta^{x_1+ \cdots + x_n} (1 - \theta)^{n - x_1 - \cdots - x_n}
	.\]
	Hence if we take $g(t, \theta) = \theta^{t}(1-\theta)^{n-t}$, and $h(x) = 1$, we immediately get that $T(X) = \sum x_i$ is sufficient.
\end{exbox}

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $U([0, \theta])$, for $\theta > 0$. Then,
	\begin{align*}
		f_X(x\mid \theta) &= \prod_{i = 1}^{n} \frac{1}{\theta} \mathbbm{1}(X_i \in [0, \theta]) \\
				  &= \underbrace{\frac{1}{\theta^{n}} \mathbbm{1} (\max_{i} x_i \leq \theta)}_{g(T(x), \theta)} \underbrace{\mathbbm{1} (\min_{i} x_i \geq 0)}_{h(x)}.
	\end{align*}
	Hence $T(x) = \max_{i} x_i$ is a sufficient statistic for $\theta$.
\end{exbox}

\subsection{Minimal Sufficiency}
\label{sub:minimal_sufficiency}

Sufficient statistics are not unique. Indeed, any one-to-one function of a sufficient statistic is also sufficient. Also $T(X) = X$ is always sufficient, but not very useful.

\begin{definition}
	A sufficient statistic $T$ is \emph{minimal sufficient}\index{minimal sufficiency} if it is a function of any other sufficient statistic, so if $T'$ is also sufficient, then
	\[
	T'(x) = T'(y) \implies T(x) = T(y)
	,\]
	for all $x, y$ in our space.
\end{definition}

By this definition, any two minimal sufficient statistics $T, T'$ are in bijection with each other, so
\[
T(x) = T(y) \iff T'(x) = T'(y)
.\]

\begin{theorem}
	Suppose that $T(X)$ is a statistic such that
	\[
	\frac{f_X(x \mid \theta)}{f_X(y \mid \theta)}
	\]
	is constant as a function of $\theta$, if and only if $T(x) = T(y)$. Then $T$ is minimal sufficient.
\end{theorem}

Let $x \overset{1}{\sim} y$ if
\[
\frac{f_X(x \mid \theta)}{f_X(y \mid \theta)}
\]
is constant in $\theta$. It is easy to check that $\overset{1}{\sim}$ is an equivalence relation.

Similarly, for a given statistic $T$, $x \overset{2}{\sim} y$ if $T(x) = T(y)$ defines another equivalence relation.

The condition of the theorem says that $\overset{1}{\sim}$ and $\overset{2}{\sim}$ are the same for minimal sufficient statistics.

\begin{remark}
	We can always construct a statistic $T$ which is constant on the equivalence classes of $\overset{1}{\sim}$, which by the theorem is minimal sufficient.
\end{remark}

\begin{proofbox}
	For any value of $T$, let $z_t$ be a representative from the equivalence class
	\[
		\{x \mid T(x) = t\}
	.\]
	Then,
	\[
	f_X(x \mid \theta) = f_X(z_{T(x)}\mid \theta) \frac{f_X(x, \theta)}{f_X(z_{T(x)} \mid \theta)}
	.\]
	This is exactly in the form $g(T(x), t) h(x)$, so by the factorization criterion $T$ is sufficient.

	To prove that $T$ is minimal, take any other sufficient statistic $S$. We want to show that if $S(x) = S(y)$, then $T(x) = T(y)$.

	By the factorization criterion, there are functions $g_s, h_s$ such that
	\[
	f_X(x, \theta) = g_s(S(x), \theta) h_s(x)
	.\]
	Suppose $S(x) = S(y)$. Then the ratio
	\[
	\frac{f_X(x \mid \theta)}{f_X(y \mid \theta)} = \frac{g_s(S(x), \theta) h_s(x)}{g_s(S(y), \theta) h_s(y)} = \frac{h_s(x)}{h_s(y)}
	,\]
	is independent of $\theta$. Hence $x \overset{1}{\sim} y$. By the hypothesis, we get that $T(x) = T(y)$.
\end{proofbox}

\begin{remark}
	Sometimes the range of $X$ depends on $\theta$. In this case we can interpret
	\[
		\frac{f_X(x \mid \theta)}{f_Y(y \mid \theta)} \text{ constant in } \theta
	,\]
	to mean that
	\[
	f_X(x \mid \theta) = c(x, y) f_X(y \mid \theta)
	,\]
	for some function $c$ which does not depend on $\theta$.
\end{remark}

\begin{exbox}
	Suppose that $X_1, \ldots, X_n$ are iid $N(\mu, \sigma^2)$, with parameters $(\mu, \sigma^2)$ unknown. Then,
	\begin{align*}
	\frac{f_X(x \mid t)}{f_X(y \mid t)} &= \frac{(2 \pi \sigma^2)^{-n/2} \exp (-\frac{1}{2 \sigma^2} \sum (x_i - \mu)^2)}{(2 \pi \sigma^2)^{-n/2} \exp (-\frac{1}{2 \sigma^2} \sum (y_i - \mu)^2)} \\
					    &= \exp \biggl[ - \frac{1}{2 \sigma^2} \biggl(\sum x_i^2 - \sum y_i^2\biggr) + \frac{\mu}{\sigma^2} \biggl(\sum x_i - \sum y_i\biggr) \biggr].
	\end{align*}
	Hence if $\sum x_i^2 = \sum y_i^2$ and $\sum x_i = \sum y_i$, this ratio does not depend on $(\mu, \sigma^2)$. The converse is also true: if the ratio does not depend on $(\mu, \sigma^2)$, then we must have $\sum x_i^2 = \sum y_i^2$ and $\sum x_i = \sum y_i$. By the theorem, $T(x) = (\sum x_i^2, \sum x_i)$ is minimal sufficient.

	Recall that bijections of $T$ are also minimal sufficient. A more common way of expressing a minimal sufficient statistic in this model is $S(X) = (\bar X, S_{x x})$, where
	\[
	\bar X = \frac{1}{n} \sum_{i} X_i, \qquad S_{x x} = \sum_{i} (X_i - \bar X)^2
	.\]
	In this example, $(\mu, \sigma^2)$ and $T(X)$ are both 2-dimensional. In general, the parameter and sufficient statistic can have different dimensions.

	For example, if $X_1, \ldots, X_n$ are iid $N(\mu, \mu^2)$, where $\mu \geq 0$, then the minimal sufficient statistic is $S(X) = (\bar X, S_{x x})$.
\end{exbox}

\subsection{Rao-Blackwell Theorem}
\label{sub:rao_blackwell_theorem}

So far we have written $\mathbb{E}_{\theta}$ and $\mathbb{P}_{\theta}$ to denote the expectations and probabilities in the model where $X_1, \ldots, X_n$ are iid drawn from $f_X(\cdot \mid \theta)$. From now on, we drop the subscript $\theta$.

\begin{theorem}[Rao-Blackwell Theorem]\index{Rao-Blackwell theorem}
	Let $T$ be a sufficient statistic for $\theta$. Let $\tilde \theta$ be some estimator for $\theta$, with $\mathbb{E}[\tilde \theta^2] < \infty$ for all $\theta$. Define a new estimator $\hat \theta = \mathbb{E}[\tilde \theta \mid T(X)]$. Then, for all $\theta$,
	\[
	\mathbb{E}[(\hat \theta - \theta)^2] \leq \mathbb{E}[(\tilde \theta - \theta)^2]
	,\]
	with equality if and only if $\tilde \theta$ is a function of $T(X)$.
\end{theorem}

\begin{remark}
	$\hat \theta$ is a valid estimator, as it does not depend on $\theta$, only on $X$, as $T$ is sufficient:
	\[
	\hat \theta (T(x)) = \int \tilde \theta(x) f_{X|T}(x|T) \diff x
	,\]
	where neither $\tilde \theta$ nor the conditional distribution depend on $\theta$.
\end{remark}

The message is that we can improve the mean squared error of any estimator $\tilde \theta$ by taking a conditional expectation given $T(X)$.

\begin{proofbox}
	By the tower property,
	\[
	\mathbb{E}[\hat \theta] = \mathbb{E}[\mathbb{E}[\tilde \theta \mid T]] = \mathbb{E}[\tilde \theta]
	.\]
	So $\bias(\hat \theta) = \bias(\tilde \theta)$ for all $\theta$. By the conditional variance formula,
	\begin{align*}
		\Var(\tilde \theta) &= \mathbb{E}[\Var(\tilde \theta \mid T)] + \Var(\mathbb{E}[\tilde \theta \mid T]) \\
				    &= \mathbb{E}[\Var(\tilde \theta \mid T)] + \Var(\hat \theta).
	\end{align*}
	Hence $\Var (\tilde \theta) \geq \Var (\hat \theta)$ for all $\theta$. Hence $\mse(\tilde \theta) \geq \mse(\hat \theta)$.

	Note that $\Var(\tilde \theta \mid T) > 0$ with some positive probability unless $\tilde \theta$ is a function of $T(X)$. So $\mse(\tilde \theta) > \mse(\hat \theta)$ unless $\tilde \theta$ is a function of $T(X)$.
\end{proofbox}

\begin{exbox}
	Say $X_1, \ldots, X_n$ are iid $\Poisson(\lambda)$. We wish to estimate $\theta = \mathbb{P}(X_1 = 0) = e^{-\lambda}$. Then
	\begin{align*}
		f_X(x \mid \lambda) &= \frac{e^{-n \lambda}\lambda^{x_1 + \cdots + x_n}}{x_1! \cdots x_n!} \\
				    &= \frac{\theta^{n} (- \log \theta)^{x_1 + \cdots + x_n}}{x_1! \cdots x_n!}
	\end{align*}
	Letting $h(x) = 1/(x_1! \cdots x_n!)$, $g(T(x), \theta) = \theta^{n}(- \log \theta)^{T(x)}$, by the factorization criterion, $T(x) = \sum x_i$ is a sufficient statistic.
	Let $\tilde \theta = \mathbbm{1}(X_i = 0)$. This is unbiased, but only uses one observation $X_1$. Using Rao-Blackwell, we can find
	\begin{align*}
		\hat \theta &= \mathbb{E}[\tilde \theta \mid T = t] = \mathbb{P}\biggl(X_1 = 0 \biggm| \sum_{i= 1}^{n} X_i = t\biggr) \\
			    &= \frac{\mathbb{P}(X_1 = 0, X_1 + \cdots + X_n = t)}{\mathbb{P}(X_1 + \cdots + X_n = t)} = \frac{\mathbb{P}(X_1 = 0, X_2 + \cdots + X_n = t)}{\mathbb{P}(X_1 + \cdots + X_n = t)} \\
			    &= \frac{\mathbb{P}(X_1 = 0) \mathbb{P}(X_2 + \cdots + X_n = t)}{\mathbb{P}(X_1 + \cdots + X_n = t)} = \frac{e^{-\lambda} \mathbb{P}(\Poisson((n-1) \lambda) = t)}{\mathbb{P}(\Poisson(n \lambda) = t)} \\
			    &= \frac{e^{-n \lambda} ((n-1)\lambda)^{t}/t!}{e^{-n \lambda} (n \lambda)^{t}/t!} = \biggl( 1 - \frac{1}{n} \biggr)^{t}.
	\end{align*}
	So $\hat \theta = (1 - \frac{1}{n})^{x_1 + \cdots + x_n}$ is an estimator which by the Rao-Blackwell theorem has $\mse(\hat \theta) < \mse(\tilde \theta)$.

	As $n \to \infty$,
	\[
		\hat \theta = \biggl(1 - \frac{1}{n} \biggr)^{n \bar x} \overset{n \to \infty}{\to} e^{-\bar x}
	,\]
	and by the strong law of large numbers
	\[
	\bar x \to \mathbb{E}[X_1] = \lambda
	.\]
	so $\hat \theta \to e^{-\lambda}$.
\end{exbox}

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $U([0, \theta])$ where $\theta$ is unknown and $\theta \geq 0$. Then recall $T(X) = \max_{i} X_i$ is sufficient for $\theta$.

	Let $\tilde \theta = 2 X_1$, which is unbiased. Then,
	\begin{align*}
		\hat \theta &= \mathbb{E}[\tilde \theta \mid T = t] = 2 \mathbb{E}[X_1 \mid \max_{i} X_i = t] \\
			    &= 2 \mathbb{E}[X_1 \mid \max_{i} X_i = t, \max_{i} X_i = X_1] \mathbb{P}(\max_{i} X_i = X_1 \mid \max_{i} X_i = t) \\
			    &+ 2 \mathbb{E}[X_1 \mid \max_{i}X_i = t, \max_{i} X_i \neq X_1] \mathbb{P}(\max_{i}X_i \neq X_1 \mid \max_{i}X_i = t) \\
			    &= \frac{2t}{n} + \frac{2(n-1)}{n} \mathbb{E}[X_1 \mid X_1 \leq t, \max_{i>1} X_i = t] = \frac{2t}{n} + \frac{2(n-1)}{n} \frac{t}{2} = \frac{n+1}{n} t.
	\end{align*}
	So $\hat \theta = \frac{n+1}{n} \max_{i} X_i$ is a valid estimator with $\mse(\hat \theta) < \mse(\tilde \theta)$.
\end{exbox}

\subsection{Maximum Likelihood Estimation}
\label{sub:maximum_likelihood_estimation}

Let $X = (X_1, \ldots, X_n)$ have joint pdf (or pmf) $f_X(X \mid \theta)$.

\begin{definition}
	The likelihood function is
	\[
	L : \theta \mapsto f_X(X \mid \theta)
	.\]
\end{definition}

The \emph{maximum likelihood estimator}\index{maximum likelihood estimator} is any value of $\theta$ maximizing $L(\theta)$.

If $X_1, \ldots, X_n$ are iid each with pdf (or pmf) $f_X(\cdot \mid \theta)$, then
\[
L(\theta) = \prod_{i=1}^{n} f_X(x_i \mid \theta)
.\]
We will denote the logarithm
\[
\ell(\theta) = \log L (\theta) = \sum_{i = 1}^{n} \log f_X(x_i \mid \theta)
.\]

\begin{exbox}
	If $X_1, \ldots, X_n$ are iid $\Ber(\theta)$, then
	\[
	\ell(\theta) = \Biggl(\sum x_i\Biggr) \log \theta = \Biggl(n - \sum_{x_i} \Biggr) \log(1 - \theta)
	,\]
	and the derivative
	\[
	\frac{\partial \ell}{\partial \theta} = \frac{\sum x_i}{\theta} - \frac{n - \sum x_i}{1 - \theta}
	.\]
	This is zero if and only if $\theta = \frac{1}{n} \sum x_i = \bar X$.

	Hence $\bar X$ is the maximum likelihood estimator for $\theta$, and is unbiased as $\mathbb{E}[\bar X] = 0$.
\end{exbox}

\begin{exbox}
	If $X_1, \ldots, X_n$ are iid $N(\mu, \sigma^2)$, then
	\[
	\log (\mu, \sigma^2) = - \frac{n}{2} \log(2 \pi) - \frac{n}{2} \log \sigma^2 - \frac{1}{2 \sigma^2} \sum_{i = 1}^{n} (x_i - \mu)^2
	.\]
	This is maximized when $\partial \ell/\partial \mu = \partial \ell/\partial \sigma^2 = 0$. First, we get
	\[
	\frac{\partial \ell}{\partial \mu} = - \frac{1}{\sigma^2} \sum_{i = 1}^{n}(x_i - \mu)
	,\]
	which is equal to zero when $\mu = \bar X$. Then
	\[
	\frac{\partial \ell}{\partial \sigma^2} = - \frac{n}{2 \sigma^2} + \frac{1}{2 \sigma^{4}} \sum_{i = 1}^{n} (x_i - \mu)^{2}
	.\]
	This is zero when
	\[
	\sigma^2 = \frac{1}{n} \sum_{i = 1}^{n} (x_i - \bar x)^2 = \frac{1}{n} S_{x x}
	.\]
	Hence $(\hat \mu, \hat \sigma^2) = (\bar X, S_{x x}/n)$ give the maximum likelihood estimator in this model.

	Note that $\hat \mu = \bar X$ is unbiased. Now we want to see if $\hat \sigma^2$ is biased. We could compute it directly, but later in the course we will show that
	\[
	\frac{S_{x x}}{\sigma^2} = \frac{n \hat \sigma^2}{\sigma^2} \sim \chi^2_{n-1}
	,\]
	hence
	\[
	\mathbb{E}[\hat \sigma^2] = \mathbb{E}[\chi^2_{n-1}] \frac{\sigma^2}{n} = \frac{n-1}{n} \sigma^2 \neq \sigma^2
	,\]
	which is biased, but asymptotically unbiased.
\end{exbox}

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $U([0,\theta])$. Then
	\[
		L(\theta) = \frac{1}{\theta^{n}} \mathbbm{1}(\max_{i} X_i \leq \theta)
	.\]
	We can see from the plot that $\hat \theta_{\rm{mle}} = \max_{i} X_i$ is the maximum likelihood estimator for $\theta$. We also started from an unbiased estimator, and using Rao-Blackwellization we found an estimator
	\[
	\hat \theta = \frac{n+1}{n} \max_i X_i
	.\]
	This is also unbiased. So in this model the mle is biased as
	\[
		\mathbb{E}[[\hat \theta_{\rm{mle}}] = \mathbb{E}\biggl[ \frac{n}{n+1} \hat \theta \biggr] = \frac{n}{n+1} \theta
	,\]
	however it is asymptotically unbiased.
\end{exbox}

The maximum likelihood estimator has the following properties:

\begin{enumerate}[1.]
	\item If $T$ is a sufficient statistic, then the maximum likelihood estimator is a function of $T(X)$. By the factorization criterion,
		\[
		L(\theta) = g(T(X), \theta) h(X)
		.\]
		If $T(x) = T(y)$, then the likelihood function with data $x$ and $y$ is the same up to a multiplicative constant. Hence the maximum likelihood estimator in each case is the same.
	\item If  $\phi = h(\theta)$ where $h$ is a bijection, then the maximum likelihood estimator of $\phi$ is
		\[
		\hat \phi = h(\hat \theta)
		,\]
		where $\hat \theta$ is the maximum likelihood estimator of $\theta$.
	\item Asymptotically, we have normality. This says $\sqrt n (\hat \theta - \theta)$ is approximately normal with mean $0$ when $n$ is large. Under some regularity conditions, for a measurable set $A$,
		\[
			\mathbb{P}(\sqrt n (\hat \theta - \theta) \in A) \overset{n \to \infty}{\to} \mathbb{P}(Z \in A)
		,\]
		where $Z \sim N(0, \Sigma)$. This holds for all regular values of $\theta$.

		Here $\Sigma$ is some function of $\ell$, and there is a theorem (Cramer-Rao) which says this is the smallest variance attainable.
	\item Sometimes, if the maximum likelihood estimator is not available analytically, we can find it numerically.
\end{enumerate}

\subsection{Confidence Intervals}
\label{sub:confidence_intervals}

\begin{definition}
	A $(100 \cdot \gamma)$\% \emph{confidence interval}\index{confidence interval} for a parameter $\theta$ is a random interval $(A(X), B(X))$ such that
	\[
	\mathbb{P}(A(X) \leq \theta \leq B(X)) = \gamma
	,\]
	for all values of $\theta$.
\end{definition}

The frequentist interpretation of the confidence interval is:
\begin{center}
	There exists some fixed true parameter $\theta$. We repeat the experiment many times. On average, $100 \cdot \gamma$\% of the time the interval $(A(X), B(X))$ contains $\theta$.
\end{center}

The incorrect interpretation is:
\begin{center}
	Having observed $X = x$, there is a probability $\gamma$ that $\theta$ is in $(A(x), B(x))$.
\end{center}

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $N(\theta, 1)$. To find a 95\% confidence interval for $\theta$, we know that
	\[
	\bar X = \frac{1}{n} \sum_{x_i} \sim N\biggl(\theta, \frac{1}{n} \biggr)
	.\]
	Hence
	\[
		Z = \sqrt{n}(\bar X - \theta) \sim N(0,1)
	.\]
	$Z$ has this distribution for all $\theta$. Then let $z_1, z_2$ be any two numbers such that $\Phi(z_2) - \Phi(z_1) = 0.95$. Then,
	\[
	\mathbb{P}(z_1 \leq \sqrt n (\bar X - \theta) \leq z_2) = 0.95
	.\]
	Rearranging,
	\[
	\mathbb{P}\biggl( \bar X - \frac{z_2}{\sqrt n} \leq \theta \leq \bar X - \frac{z_1}{\sqrt n} \biggr) = 0.95
	.\]
	Therefore $(\bar X - \frac{z_2}{\sqrt{n}}, \bar X - \frac{z_1}{\sqrt n})$ is a 95\% confidence interval.

	There are multiple ways to choose $z_1, z_2$. Usually we minimise the width of the interval, which is achieved by $z_1 = \Phi^{-1} (0.025), z_2 = \Phi^{-1}(0.975)$.
\end{exbox}

To find a confidence interval, we can do the following:
\begin{enumerate}
	\item Find some quantity $R(X, \theta)$ such that the $\mathbb{P}_{\theta}$ distribution of $R(X,\theta)$ does not depend on $\theta$. This is called a \emph{pivot}\index{pivot}.

		For example, we chose $Z = \sqrt n (\bar X - \mu) \sin N(0,1)$.
	\item Write down a probabilistic statement about the pivot of the form
		\[
		\mathbb{P}(c_1 \leq R(x, \theta) \leq c_2) = \gamma
		,\]
		by using quantiles $c_1, c_2$ of the distribution of $R(X,\theta)$ (typically $N(0,1)$ or $\chi^2_{p}$).
	\item Rearrange the inequalities to leave $\theta$ in the middle.
\end{enumerate}

\begin{proposition}
	If $T$ is a monotone increasing function $T : \mathbb{R} \to \mathbb{R}$, and $(A(X), B(X))$ is a $100 \cdot \gamma$\% confidence interval for $\theta$, then $(T(A(X)), T(B(X)))$ is a confidence interval for $T(\theta)$.
\end{proposition}

\begin{remark}
	When $\theta$ is a vector, we talk about confidence sets.
\end{remark}

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $N(0, \sigma^2)$. We want to find a $95$\% confidence interval for $\sigma^2$.

	Note that $\frac{X_i}{\sigma} \sim N(0,1)$, so using all the data points,
	\[
	R(X, \sigma^2) = \sum_{i=1}^{n} \frac{X_i^2}{\sigma^2} \sim \chi^2_n
	\]
	is a pivot. Let $c_1 = F_{\chi^2_n}^{-1}(0.025)$, $c_2 = F_{\chi^2_n}^{-1}(0.975)$. Then,
	\[
	\mathbb{P}(c_1 \leq R(X, \sigma^2) \leq c_2) = 0.95
	.\]
	Rearranging,
	\[
	\mathbb{P}\biggl( \frac{\sum x_i^2}{c_2} \leq \sigma^2 \leq \frac{\sum x_i^2}{c_1} \biggr) = 0.95
	.\]
	Hence $[\sum x_i^2/c_2, \sum x_i^2/c_1]$ is a $95$\% confidence interval $\sigma^2$.

	Applying the proposition, $[\sqrt{\sum x_i^2/c_2}, \sqrt{\sum x_i^2/c_1}]$ is a $95$\% confidence interval for $\sigma$.
\end{exbox}

\begin{exbox}
	Let $X_1, \ldots, X_n$ be $\Ber(p)$, for $n$. To find an approximate $95$\% for confidence interval $p$.

	Recall that the maximum likelihood estimator for $p$ is
	\[
	\hat p = \frac{1}{n} \sum_{i = 1}^{n} X_i
	.\]
	By the central limit theorem, when $n$ is large, $\hat p$ is approximately $N(p, \frac{p(1-p)}{n})$. Hence,
	\[
		\sqrt n \frac{(\hat p - p)}{\sqrt{p(1-p)}} \sim N(0,1)
	,\]
	approximately. If $z = \Phi^{-1}(0.975)$, then
	\[
		\mathbb{P}\biggl( - z \leq \frac{\sqrt n (\hat p - p)}{\sqrt{p(1-p)}} \leq z \biggr) \approx 0.95
	.\]
	Rearranging this is tricky. Instead, we argue that $n \to \infty$, $\hat p(1 - \hat p) \to p(1-p)$. So replacing this in the denominator,
	\[
		\mathbb{P}\biggl( - z \leq \frac{\sqrt n (\hat p - p)}{\sqrt{\hat p(1- \hat p)}} \leq z \biggr) \approx 0.95
	.\]
	Rearranging this, we get
	\[
		\mathbb{P}\biggl( \hat p - z \frac{\sqrt{\hat p (1 - \hat p)}}{\sqrt n} \leq p \leq \hat p + z \frac{\sqrt{\hat p (1 - \hat p)}}{\sqrt n} \biggr) \approx 0.95
	.\]
	Hence this is an approximate $95$\% confidence interval for $p$.

	Note that $z \approx 1.96$ and $\sqrt{\hat p(1 - \hat p)} \leq \frac{1}{2}$ for all $\hat p \in (0, 1)$. So a conservative confidence interval is $[\hat p \pm 1.96 \cdot \frac{1}{2} \cdot \frac{1}{\sqrt{n}}]$.
\end{exbox}

\subsection{Interpreting Confidence Intervals}
\label{sub:interpreting_confidence_intervals}

Suppose $X_1, X_2$ are iid $U[\theta-\frac{1}{2}, \theta+\frac{1}{2}]$. We find a sensible $50$\% confidence interval for $\theta$. Consider
\begin{align*}
	\mathbb{P}(\theta \text{ between } X_1, X_2) &= \mathbb{P}(\min(X_1,X_2) \leq \theta \leq \max(X_1,X_2)) \\
						     &= \mathbb{P}(X_1 \leq \theta \leq X_2) + \mathbb{P}(X_2 \leq \theta \leq X_1) \\
						     &= \frac{1}{4} + \frac{1}{4} = \frac{1}{2}.
\end{align*}
Hence we can immediately conclude that $(\min(X_1, X_2), \max(X_1,X_2))$ is a $50$\% confidence interval for $\theta$.

This does not mean for specific $X_1 = x_1$, $X_2 = x_2$ the value $\theta$ lies in the interval $(\min(x_1,x_2), \max(x_1,x_2))$ with probability $\frac{1}{2}$: consider when $|x_1 - x_2| > \frac{1}{2}$.

In this case, we can be sure that $\theta$ is in $(\min(x_1, x_2), \max(x_1, x_2))$, as the value $\theta$ can be at most distance $\frac{1}{2}$ from $x_1$ and $x_2$.

However the frequentist interpretation makes sense: if we repeat the experiment many times, we see $\theta \in (\min(X_1, X_2), \max(X_1,X_2))$ exactly $50$\% of the time. We cannot say, given a specific observation that we are $50$\% certain that $\theta$ is in the confidence interval.

\newpage

\section{Bayesian Inference}
\label{sec:bayesian_inference}

So far, we have assumed there is some true parameter $\theta$. That data $X$ has pdf (or pmf) $f_X(\cdot \mid \theta)$.

\emph{Bayesian analysis}\index{Bayesian analysis} is a different framework, where we treat $\theta$ as a random variable, taking values in $\Theta$.

We begin by assigning to $\theta$ a \emph{prior distribution}\index{prior distribution} $\pi(\theta)$, which represents the opinions or information about $\theta$ before seeing on any data.

Conditional on $\theta$, the data $X$ has pdf (or pmf) $f_X(x\mid\theta)$. Having observed a specific value of $X = x$, this information is combined with the prior to form the \emph{posterior distribution}\index{posterior distribution} $\pi(\theta \mid x)$, which is the conditional distribution of $\theta$ given $X = x$. By Bayes' rule,
\[
\pi(\theta \mid x) = \frac{\pi(\theta) \cdot f_X(x \mid \theta)}{f_X(x)}
,\]
where $f_X(x)$ is the marginal probability of $X$, and
\[
f_X(x) =
\begin{cases}
	\int_{\Theta} f_X(x \mid \theta) \pi(\theta) \diff \theta & \theta \text{ continuous}, \\
	\sum_{\Theta} f_X(x \mid \theta) \pi(\theta) & \theta \text{ discrete}.
\end{cases}
\]

\begin{exbox}
	Consider a patient getting a COVID test. Then the possible values are $\theta \in \{0, 1\}$, corresponding to the patient not having COVID, and the patient having COVID, respectively.

	We also have data $X \in \{0, 1\}$, corresponding to the patient getting a negative test, or positive test, respectively.

	We also know the sensitivity of the test as $f_X(X = 1 \mid \theta = 1)$, and the specificity $f_X(X = 0 \mid \theta = 0)$.

	To run Bayesian analysis, we can take a prior $\pi(\theta = 1) = p$, if we know the proportion $p$ of people infected. Then the chance of infection given a true test is
	\[
	\pi(\theta = 1 \mid X = 1) = \frac{\pi(\theta = 1) f_X(X=1\mid \theta = 1)}{\pi(\theta = 0) f_X(X = 1\mid \theta = 0) + \pi(\theta = 1) f_X(X = 1 \mid \theta = 1)}
	.\]
	If $\pi(\theta = 0) \gg \pi(\theta = 1)$, then this posterior can still be very small.
\end{exbox}

\begin{exbox}
	Let $\theta \in [0,1]$ be the mortality rate for a new surgery. In the first 10 operations, there were not deaths.

	If we have a model $X_i \sim \Ber(\theta)$, where $X_i = 1$ if the $i$'th operation is fatal, and $0$ otherwise, then
	\[
		f_X(x \mid \theta) = \theta^{x_1 + \cdots + x_{10}}(1 - \theta)^{10 - x_1 + \cdots + x_{10}}
	.\]
	For our prior, we are told that the surgery is performed in other hospital with a mortality rate ranging from $0.03$ to $0.2$, with an average of $0.1$. We can take $\pi(\theta) \sim \Bta(a, b)$, with $a = 3$ and $b = 27$, so that the mean of $\pi(\theta)$ is $0.1$ and $\pi(0.03 < \theta < 0.2) = 0.9$.

	The posterior distribution is then
	\begin{align*}
		\pi(\theta \mid x) &\propto \pi(\theta) f_X(x \mid \theta) \\
				   &\propto \theta^{a-1} (1 - \theta)^{b - 1} \theta^{x_1 + \cdots + x_{10}}(1 - \theta)^{10 - x_1 - \cdots - x_{10}} \\
				   &= \theta^{x_1 + \cdots + x_{10} + a - 1} (1 - \theta)^{b + 10 - x_1 - \cdots - x_{10} - 1}.
	\end{align*}
	We can deduce that the posterior distribution is a $\Bta(\sum x_i + a, 10 - \sum x_i + b)$ distribution. In our case, since there are no deaths, the posterior distribution is $\Bta(3, 37)$.
\end{exbox}

Note that the prior and posterior distributions are in the same family of distributions. This is known as \emph{conjugacy}\index{conjugacy}.

With the information gained from the posterior, we can make decisions under uncertainty. The formal process is:
\begin{enumerate}
	\item We must pick a decision $\delta \in D$.
	\item The loss function\index{loss function} $L(\theta, \delta)$ is the loss incurred when we make decision $\delta$ and the true parameter has value $\theta$.
	\item We pick the decision which minimizes the posterior expected loss:
		\[
		\delta^{\ast} = \underset{\delta \in D}{\mathrm{argmin}} \int_{\Theta} L(\theta, \delta) \pi(\theta \mid x) \diff \theta
		.\]
\end{enumerate}

For point estimation, the decision is a ``best guess'' for the true parameter, so $\delta \in \Theta$.

The \emph{Bayes estimator}\index{Bayes estimator} $\hat \theta^{(k)}$ minimizes
\[
h(\delta) = \int_{\Theta} L(\theta, \delta) \pi(\theta \mid x) \diff \theta
.\]

\begin{exbox}
	Consider quadratic loss $L(\theta, \delta) = (\theta - \delta)^2$. Then
	\[
	h(\delta) = \int_{\Theta}(\theta - d)^2 \pi(\theta \mid x) \diff \theta
	.\]
	Now $h'(\delta) = 0$ if
	\[
	\int_{\Theta} (\theta - d) \pi(\theta \mid x) \diff \theta = 0
	.\]
	Hence
	\[
	\int_{\Theta}\pi(\theta \mid x) \diff \theta = \delta \int_{\Theta} \pi(\theta \mid x) \diff \theta = \delta
	.\]
	So the Bayes estimator is the posterior mean of $\theta$.
\end{exbox}

\begin{exbox}
	Consider an absolute error loss $L(\theta, \delta) = |\theta - \delta|$. Then,
	\begin{align*}
		h(\delta) &= \int_{\Theta} |\theta - \delta| \pi(\theta \mid x) \diff \theta \\
			  &= \int_{-\infty}^{\delta}-(\theta - \delta) \pi(\theta \mid x) \diff \theta + \int_{\delta}^{\infty}(\theta - \delta) \pi(\theta \mid x) \diff \theta \\
			  &= - \int_{-\infty}^{\delta} \theta \pi(\theta \mid x) \diff \theta + \int_{\delta}^{\infty} \theta \pi(\theta \mid x) \diff \theta \\
			  &\quad{} + \delta \int_{-\infty}^{\delta} \pi(\theta \mid x) \diff \theta - \delta \int_{\delta}^{\infty} \pi(\theta \mid x) \diff \theta.
	\end{align*}
	Taking the derivative with respect to $\delta$, by the fundamental theorem of calculus,
	\[
	h'(\delta) = \int_{-\infty}^{\delta} \pi(\theta \mid x) \diff \theta - \int_{\delta}^{\infty} \pi(\theta \mid x) \diff \theta
	.\]
	Hence $h'(\delta) = 0$ if and only if
	\[
	\int_{-\infty}^{\delta} \pi(\theta \mid x) \diff \theta = \int_{d}^{\infty} \pi(\theta \mid x) \diff \theta
	.\]
	In this case, the Bayes estimator is the median of the posterior.
\end{exbox}

\subsection{Credible Interval}
\label{sub:credible_interval}

A $100 \cdot \gamma$\% \emph{credible interval}\index{credible interval} $(A(x), B(x))$ is one which satisfies
\[
\pi(A(x) \leq \theta \leq B(x) \mid x) = \gamma
.\]
Hence,
\[
\int_{A(x)}^{B(x)} \pi(\theta \mid x) \diff \theta = \gamma
.\]
Note that we can interpret credible intervals conditionally.

If $T$ is a sufficient statistic, then $\pi(\theta \mid x)$ only depends on $x$ through $T(x)$, as
\begin{align*}
	\pi(\theta \mid x) &\propto \pi(\theta) f_X(x \mid \theta) \\
			   &= \pi(\theta) g(T(x), \theta) h(x) \\
			   &\propto \pi(\theta) g(T(x), \theta).
\end{align*}

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $N(\mu, 1)$. We assign a prior for $\mu$ as $\pi(\mu) \sim N(0,1/\tau^2)$. Then
	\begin{align*}
		\pi(\mu \mid x) &\propto f_X(x\mid \mu) \cdot \pi(\mu) \\
				&\propto \exp \Biggl[ -\frac{1}{2} \sum_{i=1}^{n} (x_i - \mu)^2 \Biggr] \exp \Biggl[ \frac{- \mu^2 \tau^2}{2} \Biggr] \\
				&\propto \exp \Biggl[ -\frac{1}{2} (n + \tau^2) \biggl( \mu - \frac{x_1 + \cdots + x_n}{n + \tau^2} \biggr)^2 \Biggr].
	\end{align*}
	We recognise that this is a normal distribution, namely
	\[
	N \biggl( \frac{x_1 + \cdots + x_n}{n + \tau^2}, \frac{1}{n + \tau^2} \biggr)
	.\]
	The Bayes estimator is $\hat \mu^{(b)} = \frac{x_1 + \cdots + x_n}{n + \tau^2}$ for both the quadratic loss and absolute error loss. Contrast this to the maximum likelihood estimator $\frac{x_1 + \cdots + x_n}{n}$.
	A $95$\% credible interval is
	\[
		\biggl( \hat \mu^{(b)} - \frac{1.96}{\sqrt{n + \tau^2}}, \hat \mu^{(b)} + \frac{1.96}{\sqrt{n+\tau^2}} \biggr)
	.\]
	This is close to a $95$\% confidence interval when $n \gg \tau^2$.
\end{exbox}

\begin{exbox}
	Take $X_1, \ldots, X_n$ iid $\Poisson(\lambda)$. Then we take a prior for $\lambda$ as $\pi(\lambda) \sim \Exp(1)$. Hence
	\begin{align*}
		\pi(\lambda \mid x) &\propto f_X(x \mid \lambda) \cdot \pi(\lambda) \\
				    &\propto e^{-n \lambda} \lambda^{x_1 + \cdots + x_n} \cdot e^{-\lambda} \\
				    &= e^{-(n+1)\lambda} \lambda^{x_1 + \cdots + x_n}.
	\end{align*}
	This is a $\Gamma(x_1 + \cdots + x_n + 1, n +1)$ distribution. The Bayes estimator under quadratic loss is the posterior mean,
	\[
		\hat \lambda^{(b)} = \frac{x_1 + \cdots + x_n + 1}{n + 1} \overset{n \to \infty}{\to} \frac{x_1 + \cdots + x_n}{n} = \hat \lambda^{(mle)}
	.\]
	Under the absolute error loss, the Bayes estimator $\tilde \lambda^{(b)}$ has property
	\[
	\int_{0}^{\tilde \lambda^{(b)}} \frac{(n+1)^{x_1 + \cdots + x_n - 1}}{(x_1 + \cdots + x_n)!} \lambda^{x_1 + \cdots + x_n} e^{-(n+1)\lambda} \diff \lambda = \frac{1}{2}
	.\]
	This has no closed form solution.
\end{exbox}

\newpage

\section{Simple Hypotheses}
\label{sec:simple_hypotheses}

A \emph{hypothesis}\index{hypothesis} is some assumption about the distribution of the data $X$. Scientific questions are phrased as a choice between a \emph{null hypothesis}\index{null hypothesis} $H_0$ (also known as a base case, simple model, or no effect) and an \emph{alternative hypothesis}\index{alternative hypothesis} $H_1$ (also known as a complex model, interesting case, positive or negative effect).

\begin{exbox}
	\begin{enumerate}
	\item Let $X_1, \ldots, X_n$ be iid $\Ber(\theta)$. Consider two hypothesis, $H_0 : \theta = \frac{1}{2}$ (i.e. we have a fair coin), and $H_1 : \theta = \frac{3}{4}$.
	\item We also may consider $H_0 : \theta = \frac{1}{2}$, $H_1 : \theta \neq \frac{1}{2}$.
	\item Let $X_1, \ldots, X_n$ take values in $\mathbb{N}_{0}$. Then we can consider $H_0 : X_i \sim \Poisson(\lambda)$ for some $\lambda > 0$, and $H_1 : X_1 \sim f_1$ for some other mass function $f_1$.
	\item Finally, if $X$ has probability distribution function $f(\cdot \mid \theta)$, where $\theta \in \Theta$, then we can consider $H_0 : \theta \in \Theta_0 \subset \Theta$, and $H_1 : \theta \not \in \Theta_0$. This is a goodness-of-fit test.
	\end{enumerate}
\end{exbox}

A hypothesis is said to be \emph{simple}\index{simple hypothesis} if it fully specifies the distribution of $X$.

\begin{exbox}
	We look at the above hypotheses.
	\begin{enumerate}
		\item Both $H_0$ and $H_1$ are simple.
		\item $H_0$ is simple, but $H_1$ is not, as it does not determine the value of $\theta$, hence does not determine the distribution of $X$.
		\item Neither $H_0$ nor $H_1$ are simple.
		\item $H_0$ is simple if and only if $\Theta_0$ contains exactly one value. Similarly $H_1$ is simple if and only if $\Theta_0^{c}$ contains exactly once value.
	\end{enumerate}
\end{exbox}

A test of $H_0$ is defined by a \emph{critical region}\index{critical region} $C \subset \mathcal{X}$. When  $X \in C$, we ``reject'' $H_0$, and when $X \not \in C$ we say we ``fail to reject'' or ``find no evidence against'' $H_0$.

To each test, we can associate two types of errors\index{errors}\index{type 1 error}\index{type 2 error}:
\begin{itemize}
	\item[Type 1 error:] We reject $H_0$ when $H_0$ is true.
	\item[Type 2 error:] We fail to reject $H_0$ when $H_0$ is false.
\end{itemize}

When $H_0$ and $H_1$ are simple, we define
\begin{align*}
	\alpha &= \mathbb{P}_{H_0}(H_0 \text{ is rejected}) = \mathbb{P}_{H_0}(X \in C), \\
	\beta &= \mathbb{P}_{H_1}(H_0 \text{ is not rejected}) = \mathbb{P}_{H_1}(X \not \in C).
\end{align*}
Here $\alpha$ is the probability of a type 1 error, and $\beta$ is the probability of a type 2 error. The \emph{size}\index{size} of a test is $\alpha$, and the \emph{power}\index{power} of the test is $1 - \beta$.

There is a trade-off between minimizing size and maximizing power. Usually, we fix an acceptable size, then pick a test of size $\alpha$ which maximizes the power.

\subsection{Neyman-Pearson Lemma}
\label{sub:neyman_pearson_lemma}

Let $H_0, H_1$ be simple, and let $X$ have probability distribution function $f_i$ under $H_0$, for $i = 0, 1$.

The \emph{likelihood ratio statistic}\index{likelihood ratio statistic} is
\[
\Lambda_x (H_0, H_1) = \frac{f_1(X)}{f_0(X)}
.\]
A \emph{likelihood ratio test}\index{likelihood ratio test} (or LRT) rejects $H_0$ when
\[
	X \in C = \{x \mid \Lambda_x(H_0, H_1) > k\}
,\]
for some threshold or critical value $k$.

\begin{theorem}[Neyman-Pearson Lemma]
	Suppose that $f_0, f_1$ are non-zero on the same sets. Suppose there exists $k$ such that the likelihood ratio test with critical region
	\[
		C = \{x \mid \Lambda_x(H_0, H_1) > k\}
	\]
	has size $\alpha$.

	Then, this is the test with the smallest $\beta$ (highest power) out of all tests of size less than or equal to $\alpha$.
\end{theorem}

\begin{remark}
	A LRT of size $\alpha$ may not exist. Even then, there is a ``randomized LRT'' with size $\alpha$.
\end{remark}

\begin{proofbox}
	Let $\bar{C}$ be the complement of $C$. The LRT has
	\begin{align*}
		\alpha &= \mathbb{P}_{H_0}(X \in C) = \int_{C} f_0(x) \diff x, \\
		\beta &= \mathbb{P}_{H_1}(X \not \in C) = \int_{\bar{C}} f_1(x) \diff x.
	\end{align*}
	Let $C^{\ast}$ be the critical region of another test with size $\alpha^{\ast}$ and power $1 - \beta^{\ast}$, with $\alpha^{\ast} \leq \alpha$. Then we will prove $\beta \leq \beta^{\ast}$, or $\beta - \beta^{\ast} \leq 0$. Now,
	\begin{align*}
		\beta - \beta^{\ast} &= \int_{\bar C} f_1(x) \diff x - \int_{\bar{C^{\ast}}} f_1(x) \diff x \\
				     &= \int_{\bar C \cap C^{\ast}} f_1(x) \diff x - \int_{\bar{C^{\ast}} \cap C} f_1(x) \diff x \\
				     &= \int_{\bar C \cap C^{\ast}} \frac{f_1(x)}{f_0(x)} f_0(x) \diff x - \int_{\bar{C^{\ast}} \cap C} \frac{f_1(x)}{f_0(x)} f_0(x) \diff x \\
				     &\leq k \Biggl[ \int_{\bar C \cap C^{\ast}} f_0(x) \diff x - \int_{\bar{C^{\ast}} \cap C}f_0(x) \diff x \Biggr] \\
				     &= k \Biggl[ \int_{C^{\ast}} f_0(x) \diff x - \int_{C} f_0(x) \diff x \Biggr] \\
				     &= k(\alpha^{\ast} - \alpha) \leq 0.
	\end{align*}
\end{proofbox}

\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $N(\mu, \sigma_0^2)$, where the variance $\sigma_0^2$ is known. We want the best size $\alpha$ test for the hypotheses $H_0 : \mu = \mu_0$, and $H_1 : \mu = \mu_1$, for some fixed $\mu_1 > \mu_0$.

	The likelihood ratio statistic is
	\begin{align*}
		\Lambda_x(H_0,H_1) &= \frac{\exp(-\frac{1}{2\sigma_0^2}\sum(x_i - \mu_1)^2)}{\exp(-\frac{1}{2 \sigma_0^2} \sum(x_i - \mu_0^2)} \\
				   &= \exp \biggl( \frac{\mu_1 - \mu_0}{\sigma_0^2} n \bar x + \frac{n(\mu_0^2 - \mu_1^2)}{2 \sigma_0^2} \biggr).
	\end{align*}
	This is monotone in $\bar x$, the sample mean. Hence, for any $k$, there is a $c$ such that
	\[
	\Lambda_x (H_0, H_1) > k \iff \bar x > c
	.\]
	Thus the likelihood critical region is $\{x \mid \bar x > c\}$ for some constant $c$.

	By the same logic, the likelihood ratio test is of the form
	\[
		C = \biggl\{ \sqrt n \frac{(\bar x - \mu_0)}{\sigma_0} > c' \biggr\}
	.\]
	We want to pick $c'$ such that
	\[
	\mathbb{P}_{H_0} \biggl( \sqrt n \frac{(\bar x - \mu_0)}{\sigma_0} > c'\biggr) = \alpha
	.\]
	But we know that
	\[
	\sqrt n \frac{(\bar x - \mu_0)}{\sigma_0} \sim N(0, 1)
	,\]
	so if we take $c' = \Phi^{-1}(1 - \alpha) = z_{\alpha}$, the LRT has critial region
	\[
		\biggl\{ x \bigm| \frac{\sqrt n (\bar x - \mu_0)}{\sigma_0} > z_{\alpha} \biggr\}
	.\]
	By the Neyman-Pearson lemma, this is the most powerful test of size $\alpha$.

	This is called a $z$-test\index{$z$-test}, because we use a $z$ statistic to define the critical region.
\end{exbox}

\subsection{P-value}
\label{sub:p_value}

For any test with critical region of the form $\{x \mid T(x) > k\}$ for some statistic $T$, a \emph{p-value}\index{p-value} or observed significance level is
\[
p = \mathbb{P}_{H_0}(T(X) > T(x^{\ast}))
,\]
where $x^{\ast}$ is the observed data. In the above example, if we let $\mu_0 = 5$, $\mu_1 = 6$, $\sigma_0 = 1$ and $\alpha = 0.05$, and we observe
\[
x^{\ast} = (5.1, 5.5, 4.9, 5.3)
,\]
then $\bar{x^{\ast}} = 5.2$, and $z^{\ast} = 0.4$. The value $z_{\alpha} = \Phi^{-1}(1 - \alpha) = 1.645$, and so in this case we fail to reject $H_0 : \mu = 5$, with $p$-value $0.35$.

\begin{proposition}
	Under $H_0$, $p$ has $U[0, 1]$ distribution, where $p$ is a function of $x^{\ast}$, and the null distribution assumes $x^{\ast} \sim \mathbb{P}_{H_0}$.
\end{proposition}

\begin{proofbox}
	Let $F$ be the cdf of $T$. Then,
	\begin{align*}
		\mathbb{P}_{H_0}(p < u) &= \mathbb{P}_{H_0}(1 - F(T) < u) = \mathbb{P}_{H_0}(F(T) > 1-u) \\
					&= \mathbb{P}_{H_0}(T > F^{-1}(1-u)) = 1 - F(F^{-1}(1-u)) = u,
	\end{align*}
	for all $u \in [0, 1]$. Hence $p \sim U[0,1]$.
\end{proofbox}

\subsection{Composite Hypotheses}
\label{sub:composite_hypotheses}

Let $x \sim F_X(\cdot \mid \theta)$, for $\theta \in \Theta$. Then we can consider composite hypotheses $H_0 : \theta \in \Theta_0$, $H_1 : \theta \in \Theta_1$.

The type 1 and type 2 error probabilities depend on the value of $\theta$ within $\Theta_0$ or $\Theta_1$, respectively.

Let $C$ be some critical region.

\begin{definition}
	The \emph{power function}\index{power function} of the test $C$ is
	\[
	W(\theta) = \mathbb{P}_{\theta}(X \in C)
	.\]
	The \emph{size}\index{size} of $C$ is the worst case type 1 error probability:
	\[
	\alpha = \sup_{\theta \in \Theta_0} W(\theta)
	.\]
	We say that $C$ is \emph{uniformly most powerful}\index{uniformly most powerful} (or UMP) of size $\alpha$ for $H_0$ against $H_1$ if
	\[
	\sup_{\theta \in \Theta_0} W(\theta) = \alpha
	,\]
	and for any other test $C^{\ast}$ of size $\leq \alpha$ with power function $W^{\ast}$, we have
	\[
	W(\theta) \geq W^{\ast}(\theta)
	,\]
	for all $\theta \in \Theta_1$.
\end{definition}

Note that the UMP does not need to exist. But in some simple cases, the LRT is the UMP.

\begin{exbox}
	Again let $X_1, \ldots, X_n$ be $N(\mu, \sigma_0^2)$ with $\sigma_0^2$ known, and we wish to test $H_0 : \mu \leq \mu_0$ against $H_1 : \mu > \mu_0$ for some fixed $\mu_0$.

	We have studied the simple hypothesis where $H_0' : \mu = \mu_0$, $H_1': \mu = \mu_1$, with $\mu_1 > \mu_0$, and found the LRT was
	\[
		C = \biggl\{ x \big| z = \frac{\sqrt n (\bar x - \mu_0)}{\sigma_0} > z_{\alpha} \biggr\}
	.\]
	Now we claim the same test $C$ is the UMP for $H_0$ against $H_1$. Indeed, the power function for $C$ is 
	\begin{align*}
		W(\mu) &= \mathbb{P}_\mu(X \in C) = \mathbb{P}_\mu \biggl( \frac{\sqrt n (\bar x - \mu_0)}{\sigma_0} > z_{\alpha} \biggr) \\
		       &= \mathbb{P}_\mu \biggl( \frac{\sqrt n(\bar x - \mu)}{\sigma_0} > z_\alpha + \sqrt{n} \frac{(\mu_0 - \mu)}{\sigma_0} \biggr) \\
		       &= 1 - \Phi \biggl( z_{\alpha} + \frac{\sqrt n(\mu_0 - \mu)}{\sigma_0} \biggr).
	\end{align*}
	This is monotone increasing in $\mu = (-\infty, \infty)$. Therefore the test $C$ has size $\alpha$ as
	\[
	\sup_{\mu \in \Theta_0} W(\mu) = \alpha
	.\]
	It remains to show that if $C^{\ast}$ is another test of size $\leq \alpha$ with power function $W^{\ast}$, then $W(\mu_1) \geq W^{\ast}(\mu_1)$ for all $\mu_1 > \mu_0$.

	The main observation is that the critical region depends only on $\mu_0$, and $C$ is the LRT for the simple hypotheses $H_0', H_1'$. Hence any other test $C^{\ast}$ of $H_0$ versus $H_1$ of size $\leq \alpha$ also has size $\leq \alpha$ for $H_0'$ versus $H_1'$. Thus, by the Neyman-Pearson lemma, we know that $W(\mu_1) \geq W^{\ast}(\mu_1)$.

	As we can apply this argument for any $\mu_1 > \mu_0$, we have
	\[
	W^{\ast}(\mu_1) \leq W(\mu_1)
	,\]
	for all $\mu_1 > \mu_0$.
\end{exbox}

\subsection{Generalized Likelihood Ratio Tests}
\label{sub:generalized_likelihood_ratio_tests}

Again, let $X \sim f_X(\cdot \mid \theta)$, and $H_0 : \theta \in \Theta_0$, $H_1 : \theta \in \Theta_1$.

The \emph{generalized likelihood ratio statistic}\index{generalized likelihood ratio statistic} is
\[
\Lambda_x(H_0;H_1) = \frac{\sup_{\theta \in \Theta_1}f_X(x \mid \theta)}{\sup_{\theta \in \Theta_0}f_X(x \mid \theta)}.
\]
Large values of $\Lambda_x$ indicate a larger departure from the null $H_0$.
\begin{exbox}
	Let $X_1, \ldots, X_n$ be iid $N(\mu, \sigma_0^2)$ with $\sigma_0$ fixed. We wish to test
	\[
	H_0 : \mu = \mu_0, \qquad H_1 : \mu \neq \mu_0,
	\]
	for fixed $\mu_0$. Here $\Theta_0 = \{\mu_0\}$, $\Theta_1 = \mathbb{R} \setminus \{\mu_0\}$. The generalized likelihood ratio test (GLR) is
	\[
		\Lambda_x(H_0;H_1) = \frac{(2 \pi\sigma_0)^{-n/2} \exp( -\frac{1}{2 \sigma_0^2} \sum (x_i - \bar x)^2)}{(2 \pi \sigma_0)^{-n/2}\exp(-\frac{1}{2\sigma_0^2}\sum(x_i - \mu_0)^2)}.
	\]
	Taking twice the logarithm of $\Lambda_x$,
	\[
	2 \log \Lambda_x = \frac{n}{\sigma_0^2}(\bar x - \mu_0)^2.
	\]
\end{exbox}

The GLR rejects when $\Lambda_x$ is large (or when $2 \log \Lambda_x$ is large), i.e. when
\[
\biggl| \sqrt n \frac{(\bar x - \mu_0)}{\sigma_0} \biggr|
\]
is large. Under $H_0$, this has a $N(0,1)$ distribution. For a test of size $\alpha$, we reject if
\[
\biggl| \sqrt n \frac{(\bar x - \mu_0)}{\sigma_0} \biggr| > z_{\sigma/2} = \Phi^{-1}\biggl(1 - \frac{\alpha}{2} \biggr).
\]
As $2 \log \Lambda_x = n \frac{(\bar x - \mu_0)}{\sigma_0^2} \sim \chi_1^2$ under $H_0$, we can also define the critical region of the GLR test as
\[
	\biggl\{ x \bigm| n \frac{(\bar x - \mu_0)}{\sigma_0^2} > \chi_1^2(\alpha) \biggr\}.
\]
In general, we can approximate the distribution of $2 \log \Lambda_x$ with a $\chi^2$ distribution when $n$ is large:

\subsection{Wilk's theorem}
\label{sub:wilks_theorem}

Suppose $\theta$ is $k$-dimensional, $\theta = (\theta_1, \ldots, \theta_n)$.

The \emph{dimension}\index{dimension} of a hypothesis $H_0 : \theta \in \Theta_0$ is the number of free parameters in $\Theta_0$.

\begin{exbox}
	\begin{enumerate}
		\item If 
		\[
			\Theta_0 = \{ \theta \in \mathbb{R}^{k} \mid \theta_1 = \theta_2 = \cdots = \theta_p = 0\}
		\]
		for some $p < k$, then $\dim(\Theta_0) = k - p$.
	\item Let $A \in \mathbb{R}^{p\times k}$, and $b \in \mathbb{R}^{p}$, with $p < k$. Let
		\[
			\Theta_0 = \{\theta \in \mathbb{R}^{k} \mid A \theta = b\}.
		\]
		Then $\dim(\Theta_0) = k - p$, if the rows of $A$ are linearly independent, and $\Theta_0$ is a hyperplane.
	\item Let 
		\[
			\Theta_0 = \{\theta \in \mathbb{R}^{k} \mid \theta_0 = f_i(\phi), \phi \in \mathbb{R}^{p}\},
		\]
		for $p < k$. Here $\phi$ are the free parameters, and $f_i$ need not be linear. Under these conditions, $\dim(\Theta_0) = p$.
	\end{enumerate}
\end{exbox}

\begin{theorem}[Wilk's theorem]\index{Wilk's theorem}
	Suppose $\Theta_0 \subset \Theta_1$. Let
	\[
	\dim (\Theta_1) - \dim(\Theta_0) = p.
	\]
	If $X_1, \ldots, X_n$ are iid from $f_X(\cdot \mid \theta)$, then as $n \to \infty$, the limiting distribution of $2 \log \Lambda_x$ under $H_0$ is $\chi_p^2$.

	I.e. for any $\theta \in \Theta_0$, and any $l > 0$,
	\[
		(2 \log \Lambda_x \leq l) \overset{n \to \infty}{\to} \mathbb{P}(\Xi \leq l),
	\]
	where $\Xi \sim \chi_p^2$.
\end{theorem}
We can use this as follows: if we reject $H_0$ when $2 \log \Lambda_x \geq \chi_p^2(\alpha)$, then when $n$ is large, the size of the test is approximately $\alpha$.

\begin{exbox}
	In the two-sided normal mean test,
	\[
		\Theta_0 = \{\mu_0\}, \qquad \Theta_1 = \mathbb{R} \setminus \{\mu_0\},
	\]
	we found $2 \log \Lambda_x \sim \chi_1^2$.
	
	If we take $\Theta_1 = \mathbb{R}$, the GRL statistic doesn't change, so $2 \log \Lambda_x \sim \chi_1^2$, and
	\[
	\dim(\Theta_1) - \dim(\Theta_0) = 1 - 0 - 1.
	\]
	Here, the prediction of Wilk's theorem is exact.
\end{exbox}

\subsection{Tests of Goodness-of-fit}
\label{sub:tests_of_goodness_of_fit}

Let $X_1, \ldots, X_n$ be iid samples from a distribution on $\{1, 2, \ldots, k\}$.

Let $p_i = \mathbb{P}(X_1 = i)$, and let $N_i$ be the number of observations equal to $i$. Hence,
\[
\sum_{i = 1}^{n} p_i = 1, \qquad \sum_{i = 1}^{n} N_i = n.
\]
For a goodness-of-fit test, $H_0 : p = \tilde p$, for some fixed distribution $\tilde p$ on $\{1, \ldots, k\}$.

Let $H_1 : p$ is any distribution with
\[
\sum_{i = 1}^{n} p_i = 1, \qquad p_i \geq 0.
\]
\begin{exbox}
	Mendel's experiment involved crossing $n = 556$ smooth yellow peas with wrinkly green peas.

	Each member of the progeny can have any combination of the two features. Let $(p_1, p_2, p_3, p_4)$ be the probabilities of each type, and $(N_1, N_2, N_3, N_4)$ the number of each progeny of each type. Then Mendel's hypothesis is
	\[
	H_0 : p = \biggl( \frac{9}{16}, \frac{3}{16}, \frac{3}{16}, \frac{1}{16} \biggr) = \tilde p.
	\]
	Then we want to see if there is any evidence in $N$ to reject $H_0$. The model can be written as
	\[
		(N_1, N_2, N_3, N_4) \sim \mathrm{Multinomial}(n;p_1, p_2, p_3, p_4).
	\]
	The likelihood is
	\[
	L(p) \propto p_1^{N_1} p_2^{N_2} p_3^{N_3} p_4^{N_4},
	\]
	hence
	\[
	l(p) = C + \sum_{i=1}^{4} N_i \log p_i.
	\]
	We can test $H_0$ against $H_1$ using a GLR test:
	\[
	2 \log \Lambda_x = 2 ( \sup_{p \in \Theta_1} l(p) - \sup_{p \in \Theta_0}l(p)).
	\]
	The latter term is $l(\tilde p)$. In the alternative, $p$ must satisfy $\sum p_i = 1$. So
	\[
	\sup_{p \in \Theta_1} l(p) = \sup_{\sum p_i = 1} \sum_{i = 1}^{4} N_i \log p_i.
	\]
	Use the Lagrangian
	\[
	\mathcal{L}(p, \lambda) = \sum_{i=1}^{4} N_i \log p_i - \lambda\biggl( \sum_{i = 1}^{4} p_i - 1 \biggr).
	\]
	We find that $\hat p_i = \frac{N_i}{n}$, the observed proportion of samples of type $i$. Hence
	\[
	2 \log \Lambda_x = 2 (l(\hat p) - l(\tilde p)) = 2 \sum_{i = 1}^{4} N_i \log \biggl( \frac{N_i}{n \tilde p_i} \biggr).
	\]
	Wilk's theorem tells us that $2 \log \Lambda_x$ is approximately $\chi_p^2$ with
	\[
	p = \dim(\Theta_1) - \dim(\Theta_0) = (k-1) - 0 = k-1.
	\]
	So we can reject $H_0$ with size approximately $\alpha$ when
	\[
	2 \log \Lambda_x > \chi_{k-1}^2(\alpha).
	\]
\end{exbox}

It is common to write
\[
2 \log \Lambda = 2 \sum_{i} o_i \log \biggl( \frac{o_i}{e_i} \biggr),
\]
where $o_i = N_i$ is the observed number of type $i$, and $e_i = n \tilde p_i$ is the expected number of type $i$ under the null hypothesis.

\subsection{Pearson's Statistic}
\label{sub:pearsons_statistic}

Let $\delta_i = o_i - e_i$. Then,
\begin{align*}
	2 \log \Lambda &= 2 \sum_{i}(e_i + o_i) \log \biggl( 1 + \frac{\delta_i}{e_i} \biggr) \approx 2 \sum_{i} \biggl(\delta_i + \frac{\delta_i^2}{e_i} - \frac{\delta_i^2}{2 e_i} \biggr) \\
		       &= \sum_{i} \frac{\delta_i^2}{e_i} = \sum_{i} \frac{(o_i - e_i)^2}{e_i}.
\end{align*}
This is called \emph{Pearson's statistic}\index{Pearson's statistic}. This also tends to a $\chi_{k-1}^2$ distribution when $n$ is large.

\begin{exbox}
	We return to Mendel's experiment, this time with the data
	\[
		(n_1, n_2, n_3, n_4) = (315, 108, 102, 31).
	\]
	Then the GLR and Pearson's statistics are
	\[
	2 \log \Lambda \approx 0.618, \qquad \sum_{i} \frac{(o_i - e_i)^2}{e_i} \approx 0.604.
	\]
	We refer each statistic to a $\chi_{k-1}^2 = \chi_{3}^2$ distribution. We get $\chi_{3}^2(0.05) = 7.815$. Thus we don't reject $H_0$ at size 5\%.

	The $p$-value is $\mathbb{P}(\chi_{3}^2 > 0.6) \approx 0.96$. In fact, the data fits the null model almost too well.
\end{exbox}

We can also have a goodness-of-fit test for a composite null, i.e.
\begin{align*}
	H_0 &: p_i = p_i(\theta), \\
	H_1 &: p \text{ any distribution on } \{1, \ldots, k\}.
\end{align*}

\begin{exbox}
	Individuals can have three genotypes. We have a null-hypothesis
	\[
	H_0 : p_1 = \theta^2, \quad p_2 = 2 \theta(1 - \theta), \quad p_3 = (1 - \theta)^2,
	\]
	for some $\theta \in [0,1]$. Then
	\[
		2 \log \Lambda = 2 \bigl( \sup_{\text{any } p} l(p) - \sup_{\theta} l(p(\theta)) \bigr) = 2 \bigl( l(\hat p) - l(p(\hat \theta)) \bigr),
	\]
	where $\hat p$ is the maximum likelihood estimator in the alternative $H_1$, and $\hat \theta$ is the maximum likelihood estimator in null $H_{\theta}$.

	Last time we found that $\hat p_i = \frac{N_i}{n}$. Then $\hat \theta$ would need to be computed for the null model in question. We get that
	\[
	2 \log \Lambda = 2 \sum_{i} N_i \log \biggl( \frac{N_i}{n p_i(\hat \theta)} \biggr) = 2 \sum_{i} o_i \log \biggl( \frac{o_i}{e_i} \biggr),
	\]
	where again $o_i = N_i$ is the observed number of type $i$, and $e_i = n p_i (\hat \theta)$ is the expected number of type $i$ under the null hypothesis.
\end{exbox}
We can similarly define a Pearson statistic
\[
\sum_{i} \frac{(o_i - e_i)^2}{e_i}
\]
using the same argument as before.

Each statistic can be referred to a $\chi_{d}^2$ when $n$ is large by Wilk's theorem, where
\[
d = \dim (\Theta_1) - \dim (\Theta_0) = k - 1 - \dim(\Theta_0).
\]
\begin{exbox}
	Going back to our example, we have
	\[
	l(\theta) = \sum_{i} N_i \log p_i(\theta) = 2 N_1 \log \theta + N_2 \log(2\theta(1-\theta)) + 2N_3 \log(1-\theta).
	\]
	Maximizing over $\theta \in [0, 1]$ gives
	\[
	\hat \theta = \frac{2N_1 + 2N_2}{2n}.
	\]
	In this model $2 \log \Lambda$ and the Pearson statistic have a $\chi^2_d$ distribution with
	\[
	d = k - 1 - \dim (\Theta_0) = 3 - 1 - 1 = 1.
	\]
\end{exbox}

\subsection{Testing Independence in Contingency Tables}
\label{sub:testing_independence_in_contingency_tables}

Suppose $(X_1, Y_1), \ldots, (X_n, Y_n)$ are independent with $X_i$ taking values in $\{1, \ldots, r\}$, and $Y_i$ taking values in $\{1, \ldots, c\}$.

The entries in a contingency table are
\[
	N_{ij} = | \{l \mid 1 \leq l \leq n , (X_l, Y_l) = (i,j)\}|,
\]
which is the number of samples of type $(i,j)$.

\begin{exbox}
	For COVID-19 deaths, we can take $X_i$ to be the age group of the $i$'th death, and $Y_i$ the week on which it fell.

	From these statistics, we wish to see if deaths are decreasing faster for an older age group that had been vaccinated.
\end{exbox}

Now we can construct the probability model. Assume $n$ is fixed. Then a sample $(X_l, Y_l)$ has probability $p_{ij}$ of having value $(i,j)$. Thus
\[
	(N_{11}, \ldots, N_{1c}, N_{21}, \ldots, N_{rc}) \sim \mathrm{Multinomial}(n;p_{11}, \ldots, p_{1c}, p_{21}, \ldots, p_{rc}).
\]
\begin{remark}
	Fixing $n$ may not be natural; we will consider other models as well.
\end{remark}

The null hypothesis is that $X_i$ is independent of $Y_i$ for each sample. Formalizing, let
\[
p_{i+} = \sum_{j = 1}^{c} p_{ij}, \qquad p_{+j} = \sum_{i = 1}^{r} p_{ij}.
\]
Then the hypotheses are
\begin{align*}
	H_0 &: p_{ij} = p_{i+}p_{+j}, \\
	H_1 &: (p_{ij}) \text{ is unconstrained, except for } p_{ij} \geq 0, \sum p_{ij} = 1.
\end{align*}
The generalized LRT is
\[
2 \log \Lambda = 2 \sum_{i,j} o_{ij} \log \biggl( \frac{o_{ij}}{e_{ij}}\biggr),
\]
where $o_{ij} = N_{ij}$, and $e_{ij} = n \hat p_{ij}$, and $\hat p$ is the maximum likelihood estimator under the independence model $H_0$. Using Lagrange multipliers, we can find $\hat p_{ij} = \hat p_{i+} \hat p_{+j}$, where
\begin{align*}
	\hat p_{i+} = \frac{N_{i+}}{n}, \qquad &\hat p_{+j} = \frac{N_{+j}}{n}, \\
	N_{i+} = \sum_{j} N_{ij}, \qquad &N_{j+} = \sum_{i} N_{ij}.
\end{align*}
Hence the GLR is
\[
2 \log \Lambda = 2 \sum_{i, j} N_{ij} \log \biggl( \frac{N_{ij}}{n \hat p_{i+}\hat p_{+j}} \biggr) \approx \sum_{i,j} \frac{(o_{ij} - e_{ij})^2}{e_{ij}}.
\]
From Wilk's, the asymptotic distribution of these statistic is $\chi_d^2$ with
\[
	d = \dim (\Theta_1) - \dim(\Theta_0) = (rc - 1) - [(r-1) + (c-1)] = (r-1)(c-1).
\]

%lecture 13

Recall: if $X$ is a random vector,
\[
\mathbb{E}[AX + b] = A \mathbb{E}[X] + b, \qquad \Var(AX + b) = A\Var(X)A^{T}.
\]

\begin{definition}
	We say $X$ has multivariate normal distribution\index{multivariate normal} if for any $t \in \mathbb{R}^{n}$, $t^{T}X$ is normal.
\end{definition}

\begin{proposition}
	If $X$ if MVN, then $AX + b$ is MVN.
\end{proposition}

\begin{proofbox}
	Say $AX + b$ is in $\mathbb{R}^{m}$ Take $t \in \mathbb{R}^{m}$, then
	\[
	t^{T}(AX + b) = (A^{T}t)^{t}X + t^{t}b,
	\]
	where the first term is $N(\mu, \sigma^2)$ for some $\mu, \sigma^2$, and the latter term is constant. Thus,
	\[
	t^{T}(AX + b) \sim N(\mu + t^{T}b, \sigma^2).
	\]
\end{proofbox}

\begin{proposition}
	A MVN distribution is fully specified by its mean and variance.
\end{proposition}

\begin{proofbox}
	Take $X_1, X_2$ both MVN with same mean $\mu$, variance $\Sigma$. We will show that their mgf's are the same, hence $X_1, X_2$ have the same distribution:
	\begin{align*}
		\mathbb{E}[e^{t^{T}X_1}] &= M_{t^{T}X_1}(1) = \exp \biggl( 1 = \mathbb{E}[t^{T}X_1] + \frac{1}{2} \Var(t^{T}X_1)\biggr) \\
					 &= \exp\biggl(t^{t}\mu + \frac{1}{2} t^{T}\Sigma t\biggr).
	\end{align*}
	This only depends on $\mu, \Sigma$, so it is the same for $X_1, X_2$.
\end{proofbox}

\subsection{Orthogonal Projection}
\label{sub:orthogonal_projection}

\begin{definition}
	We say $P \in \mathbb{R}^{n \times n}$ is an \emph{orthogonal projection}\index{orthogonal projection}  if it is:
	\begin{itemize}
		\item independent: $PP = P$,
		\item symmetric: $P^{T} = P$.
	\end{itemize}
	Equivalently, $P \in \mathbb{R}^{n \times n}$ is an \emph{orthogonal projection} if for any $v$ in the column space, $Pv = v$, and for any $w$ perpendicular to the column vectors, $Pw = 0$.
\end{definition}

\begin{proposition}
	These two definitions are equivalent.
\end{proposition}

\begin{proofbox}
	To show the first definition equals the second, take $v$ a column vector of $P$, so $v = Pa$ for some $a \in \mathbb{R}^{n}$. Then,
	\[
	Pv = PPa = Pa = v.
	\]
	Take $w$ perpendicular to the column space. Then $P^{T}w = 0$. Then,
	\[
	Pw = P^{T}w = 0.
	\]
	To show the second definition implies the first, we can write any $a \in \mathbb{R}^{n}$ uniquely as $a = v + w$, where $v$ is in the column space, and $w$ is perpendicular to the column space. Then
	\[
	PPa = PP(v+w) = Pv = P(v+w) = Pa.
	\]
	As $a$ was arbitrary, $P = P^2$. For symmetry, take $u_1, u_2 \in \mathbb{R}^{n}$. Then,
	\[
		(Pu_1)^{T}((I-P)u_2) = 0.
	\]
	Hence,
	\[
	u_1^{T}(P^{T} - P^{T}P)u_2 = 0.
	\]
	Since this holds for all $u_1, u_2$, $P^{T} = P^{T}P$. Hence $P^{T} = P$.
\end{proofbox}

\begin{corollary}
	If $P$ is an orthogonal projection, then so is $I-P$.
\end{corollary}

\begin{proofbox}
	We have $(I-P)^{T} = I^{T} - P^{T} = I - P$, and
	\[
		(I-P)^2 = I - 2P + P^2 = I - P.
	\]
\end{proofbox}

\begin{proposition}
	If $P \in \mathbb{R}^{n \times n}$ is an orthogonal projection, then $P = UU^{T}$, where the columns of $U$ form an orthogonal basis for the columns space of $P$.
\end{proposition}

\begin{proposition}
	$UU^{T}$ is clearly symmetric and idempotent,
	\[
	UU^{T}U U^{T} = UU^{T}.
	\]
	So $UU^{T}$ is an orthogonal projection. To show that it is equal to $P$, note the column space is equal to the column space of $P$ by construction.
\end{proposition}

\begin{corollary}
	$n = \rank(P) = \tr(U^{T}U) = \tr(UU^{T}) = \tr(P)$.
\end{corollary}

\begin{theorem}
	If $X$ is MVN, $X \sim N(0, \sigma^2 I)$ and $P$ is an orthogonal projection, then
	\begin{enumerate}[\normalfont1.]
		\item $PX \sim N(0, \sigma^2 P)$, $(I-P)X \sim N(0, \sigma^2(I-P))$, $PX$ and $(I-P)X$ independent.
		\item $\frac{\|PX\|^2}{\sigma^2} \sim \chi_{\rank(P)}^2$.
	\end{enumerate}
\end{theorem}

\begin{proofbox}
	The vector
	\[
	\begin{pmatrix}
		P \\ (I-P)
	\end{pmatrix}
	X = AX
	\]
	is MVN, because it is a linear function of $X$. The distribution is specified by the mean and variance:
	\[
	\mathbb{E}[AX] =
	\begin{pmatrix}
		P \\ I-P
	\end{pmatrix}
	\mathbb{E}[X] = 0,
	\]
	and
	\begin{align*}
		\Var(AX) &=
	\begin{pmatrix}
		P \\ I-P
	\end{pmatrix}
	X
	\begin{pmatrix}
		P \\ I-P
	\end{pmatrix}^{T}
	=
	\begin{pmatrix}
		P \\ I-P
	\end{pmatrix}
	\sigma^2 I
	\begin{pmatrix}
		P \\ I-P
	\end{pmatrix}^{T}
	\\
			 &= \sigma^2
	\begin{pmatrix}
		P & 0 \\ 0 & I-P
	\end{pmatrix}.
	\end{align*}
	Let $Z \sim N(0, \sigma^2P)$, and $Z' \sim N(0, \sigma^2(I-P))$, with $Z, Z'$ independent. Then,
	\[
	\begin{pmatrix}
		Z \\ Z'
	\end{pmatrix}
	\sim
	N \Biggl( 0, \sigma^2
		\begin{pmatrix}
			P & 0 \\ 0 & I-P
		\end{pmatrix}\biggr).
	\]
	Hence we have
	\[
	\begin{pmatrix}
		PX \\ (I-P)X
	\end{pmatrix}
	=
	\begin{pmatrix}
		Z \\ Z'
	\end{pmatrix}.
	\]
	Therefore $PX, (1-P)X$ independent.

	To show 2, note
	\[
	\frac{\|PX\|^2}{\sigma^2} = \frac{(PX)^{T}PX}{\sigma^2} = \frac{X^{T}(UU^{T})^{T}UU^{T}X}{\sigma^2} = \frac{X^{T}UU^{T}X}{\sigma^2}.
	\]
	The columns of $U$ form an orthogonal basis for the column space of $P$, so
	\[
	\frac{\|PX\|^2}{\sigma^2} = \frac{\|U^{T}X\|^2}{\sigma^2} = \sum_{i = 1}^{\rank(P)} \frac{(U^{T}X)_i}{\sigma^2}.
	\]
	But $U^{T}X \sim N(0, \sigma^2I)$, so
	\[
	\Var(U^{T}X) = U^{T}\Var(X) U = \sigma^2 U^{T}U = \sigma^2 I.
	\]
	Therefore $(U^{T}X)_i$, for $i = 1, \ldots, \rank(P)$, and iid $N(0, \sigma^2)$. Thus, $\frac{\|PX\|^2}{\sigma^2}$ is the sum of $\rank(P)$ squared independent $N(0,1)$ variables, meaning it is $\chi_{\rank(P)}^2$.
\end{proofbox}

Let's look at an example. Suppose $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$, with $\mu, \sigma^2$ unknown. Recall that the MLE for $\mu$ is
\[
\bar x = \frac{1}{n} \sum_{i = 1}^{n} x_i.
\]
The MLE for $\sigma^2$ is
\[
\hat \sigma^2 = \frac{S_{xx}}{n},
\]
where
\[
S_{xx} = \sum_{i = 1}^{n} (x_i - \bar x)^2.
\]
\begin{theorem}
	\begin{enumerate}[\normalfont(i)]
		\item[]
		\item $\bar X \sim N(\mu, \sigma^2/n)$,
		\item $\frac{S_{xx}}{\sigma^2} \sim \chi_{n-1}^2$,
		\item $\bar x$, $S_{xx}$ independent.
	\end{enumerate}
\end{theorem}

\begin{proofbox}
	Let $\mathbf{1} = (1, \ldots, 1)^{T}$. Let $P= \frac{1}{n} \mathbf{1} \mathbf{1}^{T}$ be an orthogonal projection. It is easy to check that $P = P^{T} = P^2$. We can write
	\[
	X =
	\begin{pmatrix}
		X_1 \\ X_2 \\ \vdots \\ X_n
	\end{pmatrix}
	 = \mu \mathbf{1} + \eps,
	\]
	where $\eps \sim N(0, \sigma^2 I)$. Now note,
	\begin{itemize}
		\item $\bar X$ is a function of $PX$,
			\[
			PX = \mu \mathbf{1} + P\eps.
			\]
			Because $\bar X = (PX)_1$. In particular, $\bar X$ is a function of $P\eps$.
		\item We also have
			\[
			S_{x x} = \sum_{i = 1}^{n} (X_i - \bar X)^2 = \|X - \mathbf{1}\bar X \|^2 = \|(I-P)X\|^2 = \|(I-P)\eps\|^2,
			\]
			so $S_{x x}$ is a function of $(I-P)\eps$.
	\end{itemize}
	By the previous theorem, $P\eps$ and $(I-P)\eps$ are independent, so $\bar X$ and $S_{xx}$ are independent. Also,
	\[
	\frac{S_{x x}}{\sigma^2} = \frac{\|(I-P)\eps\|^2}{\sigma^2} \sim \chi_{n-1}^2.
	\]
\end{proofbox}

%lecture 15

Consider a linear model $Y = X \beta + \eps$, where
\[
	\hat \beta = \mathrm{argmin}_{\beta} \|Y = X \beta\|^2 = (X^{T}X)^{-1}X^{T}Y.
\]
Then $P = X(X^{T}X)^{-1}X^{T}$ is an orthogonal projection onto the column space of $X$, and also
\[
\hat Y = X \hat \beta = PY, \qquad Y - \hat Y = (I - P)Y.
\]
\begin{theorem}
	Let $\eps \sim N(0, \sigma^2 I)$. Then,
	\begin{enumerate}[\normalfont1.]
		\item $P \eps \sim N(0, \sigma^2 P)$, $(I-P)\eps \sim N(0, \sigma^2(I-P))$,
		\item $P \eps$ independent of $(I - P)\epsilon$.
		\item We have
			\[
				\frac{\|P \epsilon\|^2}{\sigma^2} \sim \chi^2_{\rank(P)}.
			\]
	\end{enumerate}
\end{theorem}

\subsection{Normal Linear Model}
\label{sub:normal_linear_model}

Take $Y = X \beta + \eps$, where $\epsilon \sim N(0, \sigma^2 I)$. The MLE has two parameters: $\beta \in \mathbb{R}^{p}$ and $\sigma^2 \in \mathbb{R}_{+}$. The log-likelihood is
\[
l(\beta, \sigma^2) = c + \frac{n}{2} \log \sigma^2 - \frac{1}{2 \sigma^2} \|Y - X \beta\|^2.
\]
For any $\sigma^2 > 0$, we can see that $l(\sigma^2, \beta)$ is maximized as a function of $\beta$ at the minimizer of $\|Y - X \beta\|^2$, i.e. the least-squares estimator $\hat \beta$. Now we find
\[
\frac{\partial l}{\partial \sigma^2}(\hat \beta, \sigma^2) = - \frac{n}{2 \sigma^2} + \frac{1}{2(\sigma^2)^2} \|Y - X \hat \beta\|^2.
\]
As $\sigma^2 \mapsto l(\hat B, \sigma^2)$ is unique, there is a unique maximizer when the derivative is $0$, i.e.
\[
\hat \sigma^2 = \frac{\|Y - X \hat \beta\|^2}{n} = \frac{\|(I-P)Y\|^2}{n}.
\]
\begin{theorem}
	\begin{enumerate}[\normalfont1.]
		\item[]
	\item $\hat \beta \sim N(\beta, \sigma^2(X^{T}X)^{-1})$,
	\item $\frac{\hat \sigma^2}{\sigma} n \sim \chi^2_{n-p}$,
	\item $\hat \beta, \hat \sigma^2$ are independent.
\end{enumerate}
\end{theorem}

\begin{proofbox}
	As $\hat \beta$ is linear in $Y$, it is a multivariate normal. We already know $\mathbb{E}[\hat \beta] = \beta$, and $\Var(\hat \beta) = \sigma^2(X^{T}X)^{-1}$, so this proves (1).

	For (2), note
	\[
	\frac{n \hat \sigma^2}{\sigma^2} = \frac{\|(I-P)Y\|^2}{\sigma^2}= \frac{\|(I-P)(X\beta+\epsilon)\|^2}{\sigma^2} = \frac{\|(I-P)\epsilon\|^2}{\sigma^2} \sim \chi^2_{\rank(I-P)}.
	\]
	As $\rank(I-P) = n - p$, this proves (2).

	Finally for (3), note $\hat \sigma^2$ is a function of $(I-P)\epsilon$. We will show that $\hat \beta$ is a function of $P \epsilon$, which implies they are independent. Indeed,
	\begin{align*}
		\hat \beta &= (X^{T}X)^{-1}X^{T}Y = (X^{T}X)^{-1}X^{T}(X\beta + \eps) = \beta + (X^{T}X)^{-1}X^{T}\eps \\
			   &= \beta + (X^{T}X)^{-1}X^{T}P\eps,
	\end{align*}
	as $X^{T}P = X^{T}$
\end{proofbox}

\begin{corollary}
	$\hat \sigma^2$ is biased:
	\[
	\mathbb{E}\biggl[ \frac{\hat \sigma^2 n}{\sigma^2} \biggr] = n-p \implies \mathbb{E}[\hat \sigma^2] = \biggl( \frac{n - p}{n} \biggr) \sigma^2.
	\]
\end{corollary}

\subsubsection{Student's t-distribution}
\label{subsub:students_t_distribution}

If $U \sim N(0, 1)$, $V \sim \chi^2_{n}$ and $U, V$ independent, then we say that
\[
	T = \frac{U}{\sqrt{V/n}}
\]
has a $t_n$ distribution\index{t-distribution}.

\subsubsection{The F distribution}
\label{subsub:f_distribution}

If $V \sim \chi^2_{n}$, $W \sim \chi^2_{m}$ and $V, W$ independent, then we say
\[
F = \frac{V/n}{W/m}
\]
has a $F_{n,m}$ distribution.\index{F-distribution}

\subsubsection{Confidence Sets for \texorpdfstring{$\beta$}{Beta}}
\label{subsub:confidence_sets_for_beta}

Suppose we want a $100(1-\alpha)$\% confidence interval for one of the coefficients, say $\beta_1$. Note that
\[
	\frac{\beta_1 - \hat \beta_1}{\sqrt{\sigma^2(X^{T}X)^{-1}_{11}}} \sim N(0, 1),
\]
because $\hat \beta_1 = N(\beta_1, \sigma^2(X^{T}X)^{-1}_{11})$. Also,
\[
\frac{\hat \sigma^2}{\sigma^2} n \sim \chi^2_{n-p},
\]
and these two statistic are independent. Hence,
\[
	\frac{\frac{\hat \beta_1 - \beta_1}{\sqrt{\sigma^2 (X^{T}X)^{-1}_{11}}}}{\sqrt{\frac{\hat \sigma^2}{\sigma^2} \frac{n}{n-p}}} \sim \frac{N(0,1)}{\sqrt{\chi^2_{n-p}/(n-p)}} \sim t_{n-p},
\]
which depends only on $\beta_1$ and not on $\sigma^2$. Hence we can use this as a pivot:
\[
\mathbb{P}_{\beta, \sigma^2} \biggl( - t_{n-p}(\alpha/2) \leq \frac{\hat \beta_1 - \beta_1}{\sqrt{(X^{T}X)^{-1}_{11}}} \sqrt{\frac{n-p}{n\hat \sigma^2}} \leq t_{n-p}(\alpha/2) \biggr) = 1 - \alpha.
\]
We can use the fact that $t_n$ distribution is symmetric around $0$. Rearranging the inequalities, we get
\[
\mathbb{P}_{\beta, \sigma^2} (\hat \beta_1 - M \leq \beta_1 \leq \hat \beta_1 + M) = 1- \alpha,
\]
where
\[
	M = t_{n-p}(\alpha/2) \sqrt{\frac{(X^{T}X)^{-1}_{11} \hat \sigma^2}{(n-p)/n}}.
\]
We conclude that $[\hat \beta_1 \pm M]$ is a $(1 - \alpha)100$\% confidence interval for $\beta_1$.

Note that this is not asymptotic.

By the duality between tests of significance and confidence intervals, we can find a size $\alpha$ test for
\begin{align*}
	H_0 &: \beta_1 = \beta^{\ast}, \\
	H_1 &: \beta_1 \neq \beta^{\ast}.
\end{align*}
We simply reject $H_0$ if $\beta^{\ast}$ is not contained in the $100(1-\alpha)$\% confidence interval for $\beta_1$.

\subsection{Confidence Ellipsoids for \texorpdfstring{$\beta$}{Beta}}
\label{sub:confidence_ellipsoids_for_beta}

Note that $\hat \beta - \beta \sim N(0, \sigma^2(X^{T}X)^{-1})$. As $X$ has full rank $X^{T}X$ is positive definite. So it has eigendecomposition
\[
X^{T}X = U D U^{T},
\]
where $D$ is diagonal and $U$ is unitary. Define
\[
	(X^{T}X)^{\alpha} = U D^{\alpha} U^{T},
\]
where
\[
D^{\alpha} =
\begin{pmatrix}
	D_{11}^{\alpha} & \cdots & 0 \\
			\vdots & \ddots & \vdots \\
			0 & \cdots & D_{pp}^{\alpha}
\end{pmatrix}.
\]
Then
\[
	(X^{T}X)^{1/2}(\hat \beta - \beta) \sim N(0, \sigma^2 I).
\]
Hence,
\[
\frac{\|X(\hat \beta - \beta)\|^2}{\sigma^2} = \frac{\|(X^{T}X)^{1/2}(\hat \beta - \beta)\|^2}{\sigma^2} \sim \chi^2_{p}.
\]
This is a function of $\hat \beta$, so is independent of
\[
\frac{\hat \sigma^2 n}{\sigma^2} \sim \chi^2_{n-p}.
\]
Hence,
\[
\frac{\|X(\hat \beta - \beta)\|^2/p}{\hat \sigma^2 n/(n-p)} \sim F_{p, n-p}.
\]
This only depends on $\beta$, and not on $\sigma^2$, so it can be used as a pivot. For all $\beta, \sigma^2$
\[
\mathbb{P}_{\sigma^2, \beta} \biggl( \frac{\|X(\hat \beta - \beta)\|^2/p}{\hat \sigma^2 n/(n-p)} \leq F_{p,n-p}(\alpha) \biggr) = 1 - \alpha.
\]
So we can say that the set
\[
	\biggl\{\beta \in \mathbb{R}^{p} \bigm| \frac{\|X(\hat \beta - \beta)\|^2/p}{\hat \sigma^2 n/(n-p)} \leq F_{p,n-p}(\alpha) \biggr\}
\]
is a $100(1 - \alpha)$\% confidence set for $\beta$. The principal axes are given by the eigenvectors of $X^{T}X$.

\newpage

\printindex

\end{document}
