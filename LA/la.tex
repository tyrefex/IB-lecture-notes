\documentclass[12pt]{article}

\usepackage{ishn}

\makeindex[intoc]

\begin{document}

\hypersetup{pageanchor=false}
\begin{titlepage}
	\begin{center}
		\vspace*{1em}
		\Huge
		\textbf{IB Linear Algebra}

		\vspace{1em}
		\large
		Ishan Nath, Michaelmas 2022

		\vspace{1.5em}

		\Large

		Based on Lectures by Prof. Pierre Raphael

		\vspace{1em}

		\large
		\today
	\end{center}
	
\end{titlepage}
\hypersetup{pageanchor=true}

\tableofcontents

\newpage

\section{Vector Spaces and Subspaces}%
\label{sec:vector_spaces_and_subspaces}

Let $F$ be an arbitrary field.

\begin{definition}[$F$ vector space]\index{vector space}
	A $F$ vector space is an abelian group $(V, +)$ equipped with a function
	\begin{align*}
		F \times V &\to V \\
		(\lambda, v) &\mapsto \lambda v
	\end{align*}
	such that
	\begin{itemize}
		\item $\lambda(v_1 + v_2) = \lambda v_1 + \lambda v_2$, 
		\item $(\lambda_1 + \lambda_2)v = \lambda_1 v + \lambda_2 v$,
		\item $\lambda(\mu v) = (\lambda \mu) v$,
		\item $1 \cdot v = v$.
	\end{itemize}
\end{definition}
We know how to
\begin{itemize}
	\item Sum two vectors
	\item Multiply a vector $v \in V$ by a scalar $\lambda \in F$.
\end{itemize}

\begin{exbox}
	\begin{enumerate}[(i)]
		\item Take $n \in \mathbb{N}$, then $F^{n}$ is the set of column vectors of length $n$ with elements in $F$. We have
			\[
			v \in F^{n}, v =
			\begin{pmatrix}
				x_1 \\
				\vdots \\
				x_n
			\end{pmatrix}
			, x_i \in F
			,\]
			\[
			v + w =
			\begin{pmatrix}
				v_1 \\
				\vdots \\
				v_n
			\end{pmatrix}
			+
			\begin{pmatrix}
				w_1 \\
				\vdots \\
				w_n
			\end{pmatrix}
			=
			\begin{pmatrix}
				v_1 + w_1 \\
				\vdots \\
				v_n + w_n
			\end{pmatrix}
			,\]
			\[
			\lambda v =
			\begin{pmatrix}
				\lambda v_1 \\
				\vdots \\
				\lambda v_n
			\end{pmatrix}
			.\]
			Then $F^{n}$ is a $F$ vector space.
		\item For any set $X$, take
			\[
				\mathbb{R}^{X} = \{f : X \to \mathbb{R}\}
			.\]
			Then $\mathbb{R}^{X}$ is an $\mathbb{R}$ vector space.
		\item Take $\mathcal{M}_{n, m}(F)$, the set of $n \times m$ $F$ valued matrices. Then $\mathcal{M}_{n, m}(F)$ is a $F$ vector space.
\end{enumerate}
\end{exbox}

\begin{remark}
	The axiom of scalar multiplication implies that for all $v \in V$, $0 \cdot v = 0$.
\end{remark}

\begin{definition}[Subspace]\index{subspace}
	Let $V$ be a vector space over $F$. A subset $U$ of $V$ is a vector subspace of $V$ (denoted $U \leq V$) if
	\begin{itemize}
		\item $0 \in U$,
		\item $(u_1, u_2) \in U \times U$ implies $u_1 + u_2 \in U$,
		\item $(\lambda, u) \in F \times U$ implies $\lambda u \in U$.
	\end{itemize}
\end{definition}
Note if $V$ is an $F$ vector space, and $U \leq V$, then $U$ is an $F$ vector space.

\begin{exbox}
	\begin{enumerate}[(i)]
		\item Take $V = \mathbb{R}^{\mathbb{R}}$, the space of functions $f : \mathbb{R} \to \mathbb{R}$. Let $\mathcal{C}(\mathbb{R})$ be the space of continuous functions $f : \mathbb{R} \to \mathbb{R}$. Then $\mathcal{C}(\mathbb{R}) \leq \mathbb{R}^{\mathbb{R}}$.
		\item Take the elements of $\mathbb{R}^3$ which sum up to $t$. This is a subspace if and only if $t = 0$.
	\end{enumerate}	
\end{exbox}

Note that the union of two subspaces is generally not a subspace, as it is usually not closed under addition.

\begin{proposition}
	Let $V$ be an $F$ vector space, and $U, W \leq V$. Then $U \cap W \leq V$.
\end{proposition}

\begin{proofbox}
	Since $0 \in U, 0 \in W$, $0 \in U \cap W$. Now consider $(\lambda, \mu) \in F^2$, and $(v_1, v_2) \in (U \cap W)^2$. Take $\lambda_1 v_1 + \lambda_2 v_2$. Since $u_1,v_1 \in U$, this is in $U$. Similarly, it is in $W$. So it is in $U \cap W$, and $U \cap W \leq V$.
\end{proofbox}

\begin{definition}[Sum of subspaces]\index{subspace sum}
	Let $V$ be an $F$ vector space. Let $U, W \leq V$. Then the \textit{sum} of $U$ and $W$ is the set
	\[
		U + W = \left\{ u + w \mid (u, w) \in U \times W \right\}
	.\]
\end{definition}

\begin{proofbox}
	Note $0 = 0 + 0 \in U + W$. Take $\lambda_1 f + \lambda_2 g$, where $f, g \in U + W$. Then we can write $f = f_1 + f_2, g = g_1 + g_2$, where $f_1, g_1 \in U$, $f_2, g_2 \in W$. Then 
\[
	\lambda_1 f + \lambda_2 g = \lambda_1 (f_1 + f_2) + \lambda_2(g_1 + g_2) = (\lambda_1 f_1 + \lambda_2 g_1) + (\lambda_1 f_2 + \lambda_2 g_2) \in U + W
.\]
\end{proofbox}

\begin{remark}
	$U + W$ is the smallest subspace of $V$ which contains both $U$ and $W$.
\end{remark}

\subsection{Subspaces and Quotients}%
\label{sub:subspaces_and_quotients}

\begin{definition}[Quotient]\index{quotient}
	Let $V$ be an $F$ vector space. Let $U \leq V$. The quotient space $V / U$ is the abelian group $V/U$ equipped with the scalar product multiplication
	\begin{align*}
		F \times V/U &\to V/U \\
		(\lambda, v + U) &\mapsto \lambda v + U
	\end{align*}
\end{definition}

\begin{proposition}
	$V/U$ is an $F$ vector space.
\end{proposition}

\subsection{Spans, Linear Independence and the Steinitz Exchange Lemma}%
\label{sub:spans_linear_independence_and_the_steinitz_exchange_lemma}

\begin{definition}[Span of a family of vectors]\index{span}
	Let $V$ be a $F$ vector space. Let $S \subset B$ be a subset. We define
	\begin{align*}
		\langle S \rangle &= \left\{ \text{finite linear combinations of elements of } S \right\} \\
				  &= \left\{ \sum_{\delta \in J} \lambda_{\delta} v_{\delta},  v_{\delta} \in S,  \lambda_{\delta} \in F, J \text{ finite} \right\}.
	\end{align*}
\end{definition}

By convention, we let $\langle \emptyset \rangle = \{0\}$.

\begin{remark}
	$\langle S' \rangle$ is the smallest vector subspace which contains $S$.
\end{remark}

\begin{exbox}
	Take $V = \mathbb{R}^3$, and
	\[
	S = \left\{
		\begin{pmatrix}
			1 \\
			0 \\
			0
		\end{pmatrix}
		,
		\begin{pmatrix}
			0 \\
			1\\
			2
		\end{pmatrix}
		,
		\begin{pmatrix}
			3 \\
			-2 \\
			4
		\end{pmatrix}
	\right\}
	.\]
	Then we have
	\[
		\langle S' \rangle = \left\{
			\begin{pmatrix}
				a \\
				b \\
				2b
			\end{pmatrix}
			,
		(a, b) \in \mathbb{R}^2\right\}
	.\]
	Take $V = \mathbb{R}^{n}$, and let $e_i$ be the $i$'th basis vector. Then $V = \langle e_1, \ldots, e_n \rangle$.

	Take $X$ a set, and $V = \mathbb{R}^{X}$. Let $S_x : X \to \mathbb{R}$, such that $y \mapsto 1$ if $x = y$, otherwise $y \mapsto 0$. Then
	\begin{align*}
		\langle (S_x)_{x \in X} \rangle &= \{f \in \mathbb{R}^{X} \mid f \text{ has finite support}\}.
	\end{align*}
\end{exbox}

\begin{definition}
	Let $V$ be a $F$ vector space. Let $S'$ be a subset of $V$. We may say that $S$ \textit{spans} $V$ if $\langle S \rangle = V$.
\end{definition}

\begin{definition}[Finite dimension]\index{finite dimension}
	Let $V$ be a $F$ vector space. We say that $V$ is \textit{finite dimensional} if it is spanned by a finite set.
\end{definition}

\begin{exbox}
	Consider $P[x]$, the polynomials over $\mathbb{R}$, and $P_n[x]$, the polynomials over $\mathbb{R}$ with degree $\leq n$. Then since
	\[
		\langle 1, x, \ldots, x^{n}\rangle = P_n[x]
	,\]
	$P_n[x]$ is finite dimensional, however $P[x]$ is not.
\end{exbox}

\begin{definition}[Independence]\index{linear independence}
	We say that $(v_1, \ldots, v_n)$, elements of $V$ are \textit{linearly independent} if
	\[
	\sum_{i = 1}^{n} \lambda_i v_i = 0 \implies \lambda_i = 0 \, \forall i
	.\]
\end{definition}

\begin{remark}
	\begin{enumerate}[1.]
		\item[]
		\item We also say that the family $(v_1, \ldots, v_n)$ is \textit{free}.
		\item Equivalently, $(v_1, \ldots, v_n)$ are not linearly independent if one of these vectors is a linear combination of the remaining $(n-1)$.
		\item If $(v_i)$ is free, then $v_i = 0$ for all $i$.
	\end{enumerate}
	
\end{remark}

\begin{definition}[Basis]\index{basis}
	A subset $S$ of $V$ is a \textit{basis} of $V$ if and only if
	\begin{enumerate}[(i)]
		\item $\langle S' \rangle = V$,
		\item $S$ is linearly independent.
	\end{enumerate}
	
\end{definition}

\begin{remark}
	A subset $S$ that generates $V$ is a generating family, so a basis $S$ is a free generating family.
\end{remark}

\begin{exbox}
	For $V = \mathbb{R}^{n}$, then $(e_i)$ is a basis of $V$.

	If $V = \mathbb{C}$, then for $F = \mathbb{C}$, $\{1\}$ is a basis.

	If $V = P[x]$, then $S = \{x^{n}, n \geq 0\}$ is a basis for $V$.

\end{exbox}

\begin{lemma}
	$V$ is a $F$ vector space. Then $(v_1, \ldots, v_n)$ is a basis of $V$ if and only if any vector $v \in V$ has a unique decomposition
	\[
	v = \sum_{i = 1}^{n} \lambda_i v_i
	.\]
\end{lemma}

\begin{remark}
	We call $(\lambda_1, \ldots, \lambda_n)$ the coordinates of $v$ in the basis $(v_1, \ldots, v_n)$.
\end{remark}

\begin{proofbox}
	Since $\langle v_1, \ldots, v_n \rangle = V$, we must have 
	\[
	v = \sum_{i = 1}^{n} \lambda_i v_i
	\]
	for some $\lambda_i$. Now assume
	\begin{align*}
		v = \sum_{i = 1}^{n}\lambda_i v_i = \sum_{i = 1}^{n}\lambda_i' v_i, \\
		\implies \sum_{i = 1}^{n}(\lambda_i - \lambda_i') v_i = 0.
	\end{align*}
	Since $v_i$ are free, $\lambda_i = \lambda_i'$.
\end{proofbox}

\begin{lemma}
	If $(v_1, \ldots, v_n)$ spans $V$, then some subset of this family is a basis of $V$.
\end{lemma}

\begin{proofbox}
	If $(v_1, \ldots, v_n)$ are linearly independent, we are done. Otherwise assume they are not independent, then by possibly reordering the vectors, we have
	\[
		v_n \in \langle v_1, \ldots, v_{n-1}\rangle
	.\]
	Then we have $V = \langle v_1, \ldots, v_n \rangle = \langle v_1, \ldots, v_{n-1}\rangle$.
	By iterating, we must eventually get to an independent set.
\end{proofbox}

\begin{theorem}[Steinitz Exchange Lemma]\index{Steinitz exchange lemma}
	Let $V$ be a finite dimensional vector space over $F$. Take
	\begin{enumerate}[\normalfont(i)]
		\item $(v_1, \ldots, v_m)$ free,
		\item $(w_1, \ldots, w_n)$ generating.
	\end{enumerate}
	Then $m \leq n$, and up to reordering, $(v_1, \ldots, v_m, w_{m+1}, \ldots, w_n)$ spans $V$.
\end{theorem}

\begin{proofbox}
	We use induction. Suppose that we have replaced $l$ of the $w_i$, reordering if necessary, so
\[
	\langle v_1, \ldots, v_{l}, w_{l+1}, \ldots, w_n \rangle = V
.\]
If $m = l$, we are done. Otherwise, $l < m$. Then since these vectors span $V$, we have
\[
v_{l+1} = \sum_{i \leq l}a_i v_i + \sum_{i > l}\beta_i w_i
.\]
	Since $(v_1, \ldots, v_{l+1})$ is free, some of the $\beta_i$ are non-zero. Upon reordering, we may let $\beta_{l+1} \neq 0$. Then,
\[
	w_{l+1} = \frac{1}{\beta_{l+1}} \left[ v_{l+1} - \sum_{i \leq l}\alpha_i v_i - \sum_{i > l+1} \beta_i w_i \right]
.\]
Hence, 
\begin{align*}
	V &= \langle v_1, \ldots, v_l, w_{l+1}, \ldots, w_n \rangle = \langle v_1, \ldots, v_l, v_{l+1}, w_{l+1}, \ldots, w_n \rangle \\
	  &= \langle v_1, \ldots, v_{l+1}, w_{l+2}, \ldots, w_n \rangle.
\end{align*}
Iterating this process, we eventually get $l = m$, which then proves $m \leq n$.
\end{proofbox}

\subsection{Basis, Dimension and Direct Sums}%
\label{sub:basis_dimension_and_direct_sums}

\begin{corollary}
	Let $V$ be a finite dimensional vector space over $F$. Then any two bases of $V$ have the same number of vectors, called the \textit{dimension}\index{dimension} of $V$.
\end{corollary}

\begin{proofbox}
	Take $(v_1, \ldots, v_n), (w_1, \ldots, w_m)$ bases of $V$.
	\begin{enumerate}[(i)]
		\item As $(v_i)$ is free and $(w_i)$ is generating, $n \leq m$.
		\item As $(w_i)$ is free and $(v_i)$ is generating, $m \leq n.$
	\end{enumerate}
	So $m = n$.
\end{proofbox}

\begin{corollary}
	Let $V$ be a vector space over $F$ with dimension $n \in \mathbb{N}$.
	\begin{enumerate}[\normalfont(i)]
		\item Any set of independent vectors has at most $n$ elements, with equality if and only if it is a basis.
		\item Any spanning set of vectors has at least $n$ elements, with equality if and only if it is a basis.
	\end{enumerate}
	
\end{corollary}

\begin{proofbox}
	Take a basis $\mathcal{B}$ of $V$. Then as $V$ has dimension $n$, $\mathcal{B}$ has $n$ elements.
	\begin{enumerate}[(i)]
		\item If $(v_1, \ldots, v_m)$ is free, then from the Steinitz Exchange lemma, as $\mathcal{B}$ is generating, $m \leq n$. Moreover, if we have equality, then $(v_1, \ldots, v_n)$ spans $V$, so $(v_1, \ldots, v_m)$ is a basis.
		\item If $(w_1, \ldots, w_k)$ is spanning, then from the Steinitz Exchange lemma, as $\mathcal{B}$ is free, $n \leq k$.

			If we have equality, assume that $(w_1, \ldots, w_n)$ is not a basis; hence they are not free, so there exist $\lambda_i \in F$, not all $0$, such that
			\[
			\lambda_1 w_1 + \cdots + \lambda_n w_n = 0
			.\]
			Reorder such that $\lambda_1 \neq 0$. Then, $\langle w_1, w_2, \ldots, w_n \rangle = \langle w_2, \ldots, w_n \rangle$, so $(w_2, \ldots, w_n)$ is spanning. However, this is a spanning set of size $n-1$, which is a contradiction.
	\end{enumerate}
\end{proofbox}

\begin{proposition}
	Let $U, W$ be finite dimensional subspaces of $V$. If $U$ and $W$ are finite dimensional, then so is $U + W$, and
	\[
		\dim (U + W) = \dim U + \dim W - \dim(U \cap W)
	.\]
\end{proposition}

\begin{proofbox}
	Pick $(v_1, \ldots, v_l)$ a basis of $U \cap W$. Since $U \cap W \leq U$, we can extend this to a basis $(v_1, \ldots, v_l, u_1, \ldots, u_m)$ of $U$, and a basis $(v_1, \ldots, v_l, w_1, \ldots, w_n)$ of $W$. Then we show $(v_1, \ldots, v_l, u_1, \ldots, u_m, w_1, \ldots, w_n)$ is a basis of $U + W$.

	It is clearly a generating family, so we will show it is free. Suppose
	\[
		\sum_{i = 1}^{l} \alpha_i v_i + \sum_{ = 1}^{m} \beta_i u_i + \sum_{i = 1}^{n} \gamma_i w_i = 0
	.\]
	Then we get
	\[
	\sum_{i = 1}^{n} \gamma_i w_i \in U \cap W
	,\]
	implying that
	\[
	\sum_{i = 1}^{l} s_i v_i = \sum_{i = 1}^{n} \gamma_i w_i
	.\]
	But since $(v_1, \ldots, w_n)$ is a basis of $W$, we get $\gamma_i = 0$. Similarly, $\beta_i = 0$. Thus,
	\[
	\sum_{i = 1}^{l} \alpha_i v_i = 0
	.\]
	Since $(v_i)$ is a basis of $U \cap W$, $\alpha_i = 0$.
\end{proofbox}

\begin{proposition}
	Let $V$ be a finite dimensional vector space over $F$. Let $U \leq V$. Then $U$ and $V/U$ are both finite dimensional and
	\[
		\dim V = \dim U + \dim(V/U)
	.\]
\end{proposition}

\begin{proofbox}
	Let $(u_1, u_2, \ldots, u_l)$ be a basis of $U$. As $U \leq V$, we can extend this to a basis $(u_1, \ldots, u_l, w_{l+1}, \ldots, w_{n})$ of $V$. Then we show that $(w_{l+1} + U, \ldots, w_{n} + U)$ is a basis of $V/U$.

	Indeed, they are free: if $\sum_{i = l+1}^{n} \lambda_i (w_i + U) = 0 + U$, then
	\[
	\sum_{i = l+1}^{n} (\lambda_i w_i) + U = 0 + U,
	\]
	so $\sum_{i = l+1}^{n} (\lambda_i w_i) \in U$. But if $W = \langle w_{l+1}, \ldots, w_n\rangle$, then $U \oplus W = V$, so $U \cap W = 0$. Hence $\sum_{i=l+1}^{n} (\lambda_i w_i) = 0$, giving $\lambda_i = 0$ for all $i$, as $w_i$ is a basis of $W$.

	Moreover, they span $V/U$: if $v + U \in V/U$, then we can write
	\[
	v = \sum_{i = 1}^{l} \lambda_i u_i + \sum_{i = l+1}^{n} \lambda_i w_i
	.\]
	Then, as $u_i \in U$,
	\[
	v + U = \Biggl(v - \sum_{i = 1}^{l} \lambda_i u_i \Biggr) + U = \Biggl( \sum_{i = l+1}^{n} \lambda_i w_i\Biggr) + U = \sum_{i = l+1}^{n} \lambda_i (w_i + U)
	,\]
	proving $(w_{l+1} + U, \ldots, w_n + U)$ span $V/U$.

	Hence $\dim(V/U) = n - l = \dim V - \dim U$, as desired.
\end{proofbox}

\begin{remark}
	If $U \leq V$, then we say $U$ is proper if $U \neq V$. Then for finite dimensions, $U$ proper\index{proper subspace} implies $\dim U < \dim V$, as $\dim(V/U) > 0$.
\end{remark}

\begin{definition}[Direct sum]\index{direct sum}
	Let $V$ be a vector space over $F$, and $U, W \leq V$. We say $V = U \oplus W$ if and only if any element of $v \in V$ can be uniquely decomposed as $v = u + w$ for $u \in U, w \in W$.
\end{definition}

\begin{remark}
	If $V = U \oplus W$, we say that $W$ is a complement\index{complement} of $U$ in $V$. There is no uniqueness of such a complement.
\end{remark}

In the sequel, we use the following notation. Let $\mathcal{B}_1 = \{u_1, \ldots, u_l\}$ and $\mathcal{B}_2 = \{w_1, \ldots, w_m\}$ be collections of vectors. Then
\[
	\mathcal{B}_1 \cup \mathcal{B}_2 = \{u_1, \ldots, u_l, w_1, \ldots, w_m\}
\]
with the convention that $\{v\} \cup \{v\} = \{v, v\}$.

\begin{lemma}
	Let $U, W \leq V$. Then the following are equivalent:
	\begin{enumerate}[\normalfont(i)]
		\item $V = U \oplus W$;
		\item $V = U + W$ and $U \cap W = \{0\}$;
		\item For any basis $\mathcal{B}_1$ of $U$, $\mathcal{B}_2$ of $W$, the union $\mathcal{B} = \mathcal{B}_1 \cup \mathcal{B}_2$ is a basis of $V$.
	\end{enumerate}
\end{lemma}

\begin{proofbox}
	We show (ii) implies (i). Let $V = U + W$, then clearly $U, W$ generate $V$. We only need to show uniqueness. Suppose $u_1 + w_1 = u_2 + w_2$. Then
	\[
		u_1 - u_2 = w_2 - w_1 \in U \cap W = \{0\}
	.\]
	Hence $u_1 = u_2$ and $w_1 = w_2$, as required.

	Now we show (i) implies (iii). Let $\mathcal{B}_1$ be a basis of $U$, and $\mathcal{B}_2$ a basis of $W$. Then $\mathcal{B} = \mathcal{B}_1 \cup \mathcal{B}_2$ generates $U + W = V$, and $\mathcal{B}$ is free, as if $\sum \lambda_i v_i = u + w = 0$, then $0 = 0 + 0$ uniquely, so $u = 0, w = 0$, giving $\lambda_i = 0$ for all $i$.

	Finally, we show (iii) implies (ii). Let $\mathcal{B} = \mathcal{B}_1 \cup \mathcal{B}_2$. Then since $\mathcal{B}$ is a basis of $V$,
	\[
	v = \sum_{u_i \in \mathcal{B}_1}\lambda_i u_i + \sum_{w_i \in \mathcal{B}_2}\lambda_i w_i = u + w
	.\]
	Now if $v \in U \cap W$,
	\[
	v = \sum_{u \in \mathcal{B}_1} \lambda_u u = \sum_{w \in \mathcal{B}_2} \lambda_w w
	.\]
	This gives
	\[
	\sum_{u \in \mathcal{B}_1}\lambda_u u - \sum_{w \in \mathcal{B}_2} \lambda_w w = 0
	.\]
	Since $\mathcal{B}_1 \cup \mathcal{B}_2$ is free, we get $\lambda_u = \lambda_w = 0$, so $U \cap W = \{0\}$.
\end{proofbox}

\begin{definition}
	Let $V$ be a vector space over $F$, and $V_1, \ldots, V_l \leq V$. Then
	\begin{enumerate}[\normalfont(i)]
		\item The sum of the subspaces is
			\[
				\sum_{i = 1}^{l} V_i = \{v_1 + \cdots + v_l \mid v_j \in V_J, 1 \leq j \leq l \}
			.\]
		\item The sum is direct:
			\[
			\sum_{i = 1}^{l} V_i = \bigoplus_{i = 1}^{l} V_i
			\]
			if and only if
			\[
			v_1 + \cdots + v_l = v_1' + \cdots + v_l' \implies v_1 = v_1', \ldots, v_l = v_l'
			.\]
	\end{enumerate}
	
\end{definition}

\begin{proofbox}
	Exercise.
\end{proofbox}

\begin{proposition}
	The following are equivalent:
	\begin{enumerate}[\normalfont(i)]
		\item
			\[
			\sum_{i = 1}^{l} V_i = \bigoplus_{i = 1}^{l} V_i
			,\]
		\item 
			\[
				\forall i, V_i \cap \left( \sum_{j < i} V_i \right) = \{0\}
			,\]
		\item For any basis $\mathcal{B}_i$ of $V_i$,
			\[
				\mathcal{B} = \bigcup_{i = 1}^{l} \mathcal{B}_i \text{ is a basis of } \sum_{i = 1}^{l} V_l
			.\]
	\end{enumerate}
	
\end{proposition}

\newpage

\section{Linear Maps}%
\label{sec:linear_maps}

\begin{definition}[Linear map]\index{linear map}
	Let $V, W$ be vector spaces over $F$. A map $\alpha : V \to W$ is \textit{linear} if and only if for all $\lambda_1, \lambda_2 \in F$ and $v_1, v_2 \in V$, we have
	\[
		\alpha(\lambda_1 v_1 + \lambda_2 v_2) = \lambda_1 \alpha(v_1) + \lambda_2 \alpha(v_2)
	.\]
\end{definition}

\begin{exbox}
	\begin{enumerate}[(i)]
		\item Take an $m \times n$ matrix $M$, Then we can take the linear map $\alpha : \mathbb{R}^{m} \to \mathbb{R}^{n}$ defined by $X \mapsto M X$.
		\item Take the linear map $\alpha : \mathcal{C}[0, 1] \to \mathcal{C}[0, 1]$ by
			\[
				f \mapsto \alpha(f)(x) = \int_{0}^{x}f(t)\diff t
			.\]
		\item Fix $x \in [a, b]$. Then we can take a linear map $\mathcal{C}[a, b] \to \mathbb{R}$ by $f \mapsto f(x)$.
	\end{enumerate}

\end{exbox}

\begin{remark}
	Let $U, V, W$ be $F$-vector spaces.
	\begin{enumerate}[(i)]
		\item The identity map $\id_V : V \to V$ by $x \mapsto x$ is a linear map.
		\item If $U \to V$ is $\beta$ linear, and $V \to W$ is $\alpha$ linear, then $U \to W$ is linear by $\alpha \circ \beta$.
	\end{enumerate}
\end{remark}

\begin{lemma}
	Let $V, W$ be $F$-vector spaces, and $\mathcal{B}$ a basis of $V$. Let $\alpha_0 : \mathcal{B} \to W$ be any map, then there is a unique linear map $\alpha : V \to W$ extending $\alpha_0$.
\end{lemma}

\begin{proofbox}
	For $v \in V$, we can write
\[
v = \sum_{i = 1}^{n} \lambda_i v_i
,\]
where $\mathcal{B} = (v_1, \ldots, v_n)$. Then by linearity, we must have
\[
	\alpha(v) = \alpha \left( \sum_{i = 1}^{n} \lambda_i v_i \right) = \sum_{i = 1}^{}\lambda_i \alpha_0 (v_i)
.\]
This is unique as $\mathcal{B}$ is a basis.
\end{proofbox}

\begin{remark}
	This is true in infinite dimensions as well.
\end{remark}

Often, to define a linear map, we define its value on a basis and extend by linearity. As a corollary, if $\alpha_1, \alpha_2 : V \to W$ are linear and agree on a basis of $V$, they are equal.

\begin{definition}[Isomorphism]\index{isomorphism}
	Let $V, W$ be vector spaces over $F$. A map $\alpha : V \to W$ is called an \textit{isomorphism} if and only if $\alpha$ is linear and bijective. If such an $\alpha$ exists, we say $V \cong W$.
\end{definition}

\begin{remark}
	If $\alpha : V \to W$ is an isomorphism, then $\alpha^{-1} : W \to V$ is linear. Indeed, for $w_1, w_2 \in W \times W$, let $w_1 = \alpha(v_1), w_2 = \alpha(v_2)$. Then,
	\begin{align*}
		\alpha^{-1}(\lambda_1 w_1 + \lambda_2 w_2) &= \alpha^{-1}(\lambda_1 \alpha(v_1) + \lambda_2 \alpha(v_2)) \\
							   &= \alpha^{-1} ( \alpha(\lambda_1 v_1 + \lambda_2 v_2) ) \\
							   &= \lambda_1 v_1 + \lambda_2 v_2 \\
							   &= \lambda_1 \alpha^{-1}(v_1) + \lambda_2 \alpha^{-1}(v_2).
	\end{align*}
\end{remark}

\begin{lemma}
	Congruence is an equivalence relation on the class of all vector spaces of $F$:
	\begin{enumerate}[\normalfont(i)]
		\item $\id_V : V \to V$ is an isomorphism.
		\item $\alpha : V \to W$ is an isomorphism implies $\alpha^{-1} : W \to V$ is an isomorphism.
		\item If $\alpha : U \to V$ is an isomorphism, $\beta : V \to W$ is an isomorphism, then $\beta \circ \alpha : U \to W$ is an isomorphism.
	\end{enumerate}
\end{lemma}

\begin{proofbox}
	Exercise.
\end{proofbox}

\begin{theorem}
	If $V$ is a vector space over $F$ of dimension $n$, then $V \cong F^{n}$.
\end{theorem}

\begin{proofbox}
	Let $\mathcal{B} = (v_1, \ldots, v_n)$ be a basis of $V$. Then take
	\begin{align*}
		\alpha : V &\to F^{n} \\
		v = \sum_{i = 1}^{n} \lambda_i v_i &\mapsto 
		\begin{pmatrix}
			\lambda_1 \\
			\vdots \\
			\lambda_n
		\end{pmatrix}
	\end{align*}
	as an isomorphism.
\end{proofbox}

\begin{remark}
	In this way, choosing a basis of $V$ is like choosing an isomorphism from $V$ to $F^{n}$.
\end{remark}

\begin{theorem}
	Let $V, W$ be vector spaces over $F$ with finite dimension. Then $V \cong W$ if and only if $\dim V = \dim W$.
\end{theorem}

\begin{proofbox}
	If $\dim V = \dim W$, then $V \cong F^{n} \cong W$, so $V \cong W$.

Otherwise, let $\alpha : V \to W$ be an isomorphism, and $\mathcal{B}$ a basis of $V$. Then we show $\alpha(\mathcal{B})$ is a basis of $W$.
\begin{itemize}
	\item $\alpha(\mathcal{B})$ spans $V$ from the surjectivity of $\alpha$.
	\item $\alpha(\mathcal{B})$ is free from the injectivity of $\alpha$.
\end{itemize}
Hence $\dim V = \dim W$.
\end{proofbox}

\begin{definition}[Kernal and Image]\index{kernel}\index{image}
	Let $V, W$ be vector spaces over $F$. Let $\alpha : V \to W$ be a linear map. We define
	\begin{enumerate}[(i)]
	\item $\Ker \alpha = \{v \in V \mid \alpha(v) = 0\}$, the kernel of $\alpha$.
	\item  $\Img(\alpha = \{w \in W \mid \exists v \in V, \alpha(v) = w\}$, the image of $\alpha$.
	\end{enumerate}
\end{definition}

\begin{lemma}
	$\Ker \alpha$ is a subspace of $V$, and $\Img \alpha$ is a subspace of $W$.
\end{lemma}

\begin{proofbox}
	Let $\lambda_1, \lambda_2 \in F$, and $v_1, v_2 \in \Ker \alpha$. Then
\[
	\alpha (\lambda_1 v_1 + \lambda_2 v_2) = \lambda_1 \alpha(v_1) + \lambda_2 \alpha(v_2) = 0
.\]
So $\lambda_1 v_1 + \lambda_2 v_2 \in \Ker \alpha$.

Now if $w_1 = \alpha(v_1), w_2 = \alpha (v_2)$, then
 \[
	 \lambda_1 w_1 + \lambda_2 w_2 = \lambda_1 \alpha(v_1) + \lambda_2 \alpha(v_2) = \alpha(\lambda_1 v_1 + \lambda_2 v_2)
.\]
Hence $\lambda_1 w_1 + \lambda_2 w_2 \in \Img \alpha$.
\end{proofbox}

\begin{exbox}
	Consider $\alpha : \mathcal{C}^{\infty}(\mathbb{R}) \to \mathcal{C}^{\infty}(\mathbb{R}),$ given by
	\[
		f \mapsto \alpha(f) = f'' + f
	.\]
	Then $\alpha$ is linear, and
	\[
		\Ker \alpha = \{f \in \mathcal{C}^{\infty}(\mathbb{R}) \mid f'' + f = 0\} = \langle \sin t, \cos t \rangle
	.\]
\end{exbox}

\begin{remark}
	If $\alpha : V \to W$ is linear, then $\alpha$ is injective if and only if $\Ker \alpha = \{0\}$, as
	\[
		\alpha(v_1) = \alpha(v_2) \iff \alpha(v_1 - v_2) = 0
	.\]
\end{remark}

\begin{theorem}
	Let $V, W$ be vector spaces over $F$, and $\alpha : V \to W$ linear. Then
	\begin{align*}
		V/\Ker \alpha &\to \Img \alpha \\
		v + \Ker \alpha &\mapsto \alpha(v)
	\end{align*}
	is an isomorphism.
\end{theorem}

\begin{proofbox}
	We proceed in steps.
\begin{itemize}
	\item $\bar \alpha$ is well defined: Note if $v + \Ker \alpha = v' + \Ker \alpha$, then $v - v' \in \Ker \alpha$, so $\alpha(v - v') = 0$. Hence $\alpha(v) = \alpha(v')$.
	\item $\bar \alpha$ is linear: This follows from linearity of $\alpha$.
	\item $\bar \alpha$ is a bijection: First, if $\bar \alpha (v + \Ker \alpha) = 0$, then $\alpha(v) = 0$, so $v \in \Ker \alpha$, hence $v + \Ker \alpha = 0 + \Ker \alpha$, so $\alpha$ is injective. Then $\bar \alpha$ is surjective from the definition of the image.
\end{itemize}
\end{proofbox}

\begin{definition}[Rank and Nullity]\index{rank}\index{nullity}
	We define the rank $r(\alpha) = \rank(\alpha) = \dim \Img \alpha$, and the nullity $n(\alpha) = \nullity(\alpha) = \dim \Ker \alpha$.
\end{definition}

\begin{theorem}[Rank-nullity theorem]\index{rank-nullity theorem}
	Let $U, V$ be vector spaces over $F$, with $\dim U < \infty$, and let $\alpha : U \to V$ be a lninear map. Then,
	\[
		\dim U = r(\alpha) + n(\alpha)
	.\]
\end{theorem}

\begin{proofbox}
	We have proven that $U/\Ker \alpha \cong \Img \alpha$, but we have already proven $\dim U/\Ker \alpha = \dim U - r(\alpha)$, which proves the theorem.
\end{proofbox}

\begin{lemma}
	Let $V, W$ be vector spaces over $F$ of equal finite dimension. Let $\alpha : V \to W$ be a linear map. Then the following are equivalent:
	\begin{itemize}
		\item $\alpha$ is injective,
		\item $\alpha$ is surjective,
		\item $\alpha$ is an isomorphism.
	\end{itemize}
\end{lemma}

This follows immediately from the rank-nullity theorem.

\subsection{Linear maps and Matrices}%
\label{sub:linear_maps_and_matrices}

\begin{definition}
	If $V, W$ are vector spaces over $F$, then
	\[
		\mathcal{L}(V, W) = \{\alpha : V \to W \text{ linear}\}
	.\]
\end{definition}

\begin{proposition}
	$\mathcal{L}(V, W)$ is a vector space over $F$ with
	\[
		(\alpha_1 + \alpha_2)(v) = \alpha_1(v) + \alpha_2(v)
	,\]
	\[
		(\lambda \alpha)(v) = \lambda \alpha 9v)
	.\]
	Moreover, if $V$ and $W$ are finite dimensional, then so is $\mathcal{L}(V, W)$, and
	\[
		\dim \mathcal{L}(V, W) = \dim V \dim W
	.\]
\end{proposition}

\begin{definition}
	An $m \times n$ matrix\index{matrix} over $F$ is an array with $m$ rows and $n$ columns with entries in $F$, $A = (a_{ij})$. Define
	\[
		\mathcal{M}_{m, n}(F) = \{\text{set of } m \times n \text{ matrices over } F\}
	.\]
\end{definition}

\begin{proposition}
	$\mathcal{M}_{m, n}(F)$ is a vector space over $F$, and $\dim \mathcal{M}_{m, n}(F) = mn$
\end{proposition}

\begin{proofbox}
	Let $E_{ij}$ be the matrix with $a_{xy} = \delta_{xi}\delta_{yj}$. Then $(E_{ij})$ is a basis of $\mathcal{M}_{m, n}(F)$, as
	\[
		N = (a_{ij}) = \sum_{i, j}a_{ij} E_{ij}
	,\]
	and $(E_{ij})$ is free.
\end{proofbox}

If $V, W$ are vector spaces over $F$, and $\alpha : V \to W$ is a linear map, we take a basis $\mathcal{B} = (v_1, \ldots, v_n)$ of $V$, and $\mathcal{C} = (w_1, \ldots, w_m)$ of $W$. Let $v \in V$, then
\[
v = \sum_{i = 1}^{n} \lambda_i v_i \sim
\begin{pmatrix}
	\lambda_1 \\
	\vdots \\
	\lambda_n
\end{pmatrix}
 \in F^{n}
.\]
We let this isomorphism from $V$ to $F^{n}$ be $[v]_{\mathcal{B}}$. Similarly, we can obtain $[w]_{\mathcal{B}}$ for $w \in W$.

\begin{definition}
	We define a matrix of $\alpha$ with respect to a basis $\mathcal{B}, \mathcal{C}$ as
	\[
		[\alpha]_{\mathcal{B}, \mathcal{C}} = ([\alpha(v_1)]_{\mathcal{C}}, [\alpha(v_2)]_{\mathcal{C}}, \ldots, [\alpha(v_n)]_{\mathcal{C}})
	.\]
\end{definition}

By definition, if $[\alpha]_{\mathcal{B}, \mathcal{C}} = (a_{ij})$, then
\[
	\alpha(v_j) = \sum_{i = 1}^{m} a_{ij} w_i
.\]

\begin{lemma}
	If $v \in V$, then
	\[
		[\alpha(v)]_{\mathcal{C}} = [\alpha]_{\mathcal{B}, \mathcal{C}} \cdot [v]_{\mathcal{B}}
	,\]
	or equivalently,
	\[
		(\alpha (v))_{i} = \sum_{j = 1}^{n} a_{ij} \lambda_j
	.\] 
\end{lemma}

\begin{proofbox}
	Let $v \in V$, then
\[
v = \sum_{j = 1}^{n} \lambda_j v_j
.\]
Then
 \begin{align*}
	 \alpha(v) &= \alpha \left( \sum_{j = 1}^{n} \lambda_j v_j \right) = \sum_{j = 1}^{n} \lambda_j \alpha(v_j) \\
		   &= \sum_{j = 1}^{n} \lambda_j \sum_{i = 1}^{n} a_{ij} w_i = \sum_{i = 1}^{m} \left( \sum_{j = 1}^{n} a_{ij} \lambda_j \right) w_i.
\end{align*}
\end{proofbox}

\begin{lemma}
	If $U \to V$ is linear under $\beta$, $V \to W$ linear under $\alpha$, then $U \to W$ is linear under $\alpha \to W$. Let $\mathcal{A}$ be a basis of $U$, $\mathcal{B}$ a basis of $V$, and $\mathcal{C}$ a basis of $W$. Then
	\[
		[\alpha \circ \beta]_{\mathcal{A}, \mathcal{C}} = [\alpha]_{\mathcal{B}, \mathcal{C}} \cdot [\beta]_{\mathcal{A}, \mathcal{B}}
	.\] 
\end{lemma}

\begin{proofbox}
	Let $A = [\alpha]_{\mathcal{B}, \mathcal{C}}$, $B = [\beta]_{\mathcal{A}, \mathcal{B}}$. Pick $u_l \in A$. Then
	\begin{align*}
		(\alpha \circ \beta)(u_l) &= \alpha(\beta(u_l)) = \alpha\left( \sum_{j} b_{jl} v_j \right) \\
					  &= \sum_{j} b_{jl} \alpha(v_j) = \sum_{j}b_{jl}\sum_{i}a_{ij}w_i \\
					  &= \sum_{i}\left( \sum_{j} a_{ij} b_{jl} \right) w_i.
	\end{align*}
\end{proofbox}

\begin{proposition}
If $V$ and $W$ are vector spaces over $F$, and $\dim V = n$, $\dim W = m$, then $\mathcal{L}(V, W) \cong \mathcal{M}_{m, n}(F)$, so $\dim \mathcal{L}(V, W) = m \times n$.
\end{proposition}

\begin{proofbox}
	Fix $\mathcal{B}, \mathcal{C}$ bases of $V$ and $W$. We show
\begin{align*}
	\theta : \mathcal{L}(V, W) &\to \mathcal{M}_{m, n}(F) \\
	\alpha &\mapsto [\alpha]_{\mathcal{B}, \mathcal{C}}
\end{align*}
is an isomorphism.
\begin{itemize}
	\item $\theta$ is linear:
		$[\lambda_1 \alpha_1 + \lambda_2 \alpha_2]_{\mathcal{B}, \mathcal{C}} = \lambda_1 [\alpha_1]_{\mathcal{B}, \mathcal{C}} + \lambda_2[\alpha_2]_{\mathcal{B}, \mathcal{C}}$.
	\item $\theta$ is surjective: Consider $A = (a_{ij})$. Consider the map
		\[
		\alpha : v_j \mapsto \sum_{i = 1}^{m} a_{ij}w_i
		.\]
		This can be extended by linearity, and $[\alpha]_{\mathcal{B}, \mathcal{C}} = A$.
	\item $\theta$ is injective: If $[\alpha]_{\mathcal{B}, \mathcal{C}} = 0$, then $\alpha = 0$ for all $v$.
\end{itemize}
\end{proofbox}

\begin{remark}
	If $\mathcal{B}, \mathcal{C}$ are bases of $V, W$ and $\varepsilon_{\mathcal{B}} : v \mapsto [v]_{\mathcal{B}}$, $\varepsilon_{\mathcal{C}} : w \mapsto [w]_{\mathcal{C}}$, then the following diagram commutes:
	\[
		\begin{tikzcd}
			V \arrow[r, "\alpha"] \arrow[d, "\varepsilon_{\mathcal{B}}"] & W \arrow[d, "\varepsilon_{\mathcal{C}}"] \\
			F^{n} \arrow[r, "{[\alpha]_{\mathcal{B},\mathcal{C}}}"] & F^{m}
		\end{tikzcd}
	\] 
\end{remark}

\subsection{Change of Basis and Equivalent Matrices}%
\label{sub:change_of_basis_and_equivalent_matrices}

Let $\alpha : V \to W$ with $\mathcal{B}$ and $\mathcal{C}$ bases of $V, W$. Then
\[
	[\alpha(v)]_{\mathcal{C}} = [\alpha]_{\mathcal{B}, \mathcal{C}} \cdot [v]_{\mathcal{B}}
.\]
If $Y \leq V$, we can take $\mathcal{B}$ a basis of $V$, such that $(v_1, \ldots, v_k, v_{k+1}, \ldots, v_n)$ is a basis of $V$, and $(v_1, \ldots, v_k)$ is a basis $\mathcal{B}'$ of $Y$, and $(v_{k+1}, \ldots, v_n)$ is a basis $\mathcal{B}''$.

Then if $Z \leq W$, we can take a basis $\mathcal{C}$ of $W$ $(w_1, \ldots, w_l, w_{l+1}, \ldots, w_m)$, such that $(w_1, \ldots, w_l)$ is a basis $\mathcal{C}'$ of $Z$, and $(w_{l+1}, \ldots, w_m)$ is a basis $\mathcal{C}''$. Then
\[
	[\alpha]_{\mathcal{B}, \mathcal{C}} =
	\begin{pmatrix}
		A & B \\
		0 & C
	\end{pmatrix}
.\]
Then we can show that
\[
	A = [\alpha|_{Y}]_{\mathcal{B}', \mathcal{C}'}
,\]
if $\alpha(Y) \leq Z$. Moreover, we can show $\alpha$ induces a homomorphism
\begin{align*}
	\bar \alpha : V / Y &\to W / Z \\
	v + Y &\mapsto \alpha(v) + Z
\end{align*}
This is well-defined as $\alpha(v) \in Z$ for $v \in Y$, and $[\bar \alpha]_{\mathcal{B}'', \mathcal{C}''} = C$.

Consider $\alpha : V \to W$, where $V$ has two bases $\mathcal{B} = \{v_1, \ldots, v_n\}$ and $\mathcal{B}' = \{v_1', \ldots, v_n'\}$ and $W$ has two bases $\mathcal{C} = \{w_1, \ldots, w_n\}$ and $\mathcal{C}' = \{w_1', \ldots, w_m'\}$. We aim to find the relation between $[\alpha]_{\mathcal{B}, \mathcal{C}}$ and $[\alpha]_{\mathcal{B}', \mathcal{C}'}$.

\begin{definition}
	The \textit{change of basis matrix}\index{change of basis matrix} from $\mathcal{B}'$ to $\mathcal{B}$ is $P = (p_{ij})$ given by
	\[
		P = ([v_1']_{\mathcal{B}}, \ldots, [v_n']_{\mathcal{B}}) = [\id]_{\mathcal{B}', \mathcal{B}}
	.\]
\end{definition}

\begin{lemma}
	$[v]_{\mathcal{B}} = P[v]_{\mathcal{B}'}$.
\end{lemma}

\begin{proofbox}
	In general $[\alpha(v)]_{\mathcal{C}} = [\alpha]_{\mathcal{B}, \mathcal{C}} [v]_{\mathcal{B}}$. If $P = [\id]_{\mathcal{B}', \mathcal{B}}$, then
	\[
		[v]_{\mathcal{B}} = [\id(v)]_{\mathcal{B}} = [\id]_{\mathcal{B}', \mathcal{B}} [v]_{\mathcal{B}'} = P[v]_{\mathcal{B}'}
	.\]
\end{proofbox}

\begin{remark} 
	$P$ is an $n \times n$ invertible matrix, and $P^{-1}$ is the change of basis matrix from $B$ to $B'$. Indeed,
	\[
		[\id]_{\mathcal{B}, \mathcal{B}'} [\id]_{\mathcal{B}', \mathcal{B}} = [\id]_{\mathcal{B}', \mathcal{B}'} = \id
	,\]
	and similarly.
\end{remark}

Note while we know $[v]_{\mathcal{B}} = P [v]_{\mathcal{B}'}$, to compute a vector in $\mathcal{B}'$, we have $[v]_{\mathcal{B}'} = P^{-1}[v]_{\mathcal{B}}$. This is hard to do.

Similarly, we can also change basis $\mathcal{C}$ to $\mathcal{C}'$ in $W$. In this case, the change of basis matrix $Q = [\id]_{\mathcal{C}', \mathcal{C}}$ is $m \times m$ and invertible.

Now given $\alpha : V \to W$, we wish to find how $[\alpha]_{\mathcal{B}, \mathcal{C}}$ and $[\alpha]_{\mathcal{B}', \mathcal{C}'}$.

\begin{proposition}
	If $A = [\alpha]_{\mathcal{B}, \mathcal{C}}$, $A' = [\alpha]_{\mathcal{B}', \mathcal{C}'}$, $P = [\id]_{\mathcal{B}', \mathcal{B}}$, $Q = [\id]_{\mathcal{C}'}, \mathcal{C}$, then
	\[
	A' = Q^{-1}AP
	.\]
\end{proposition}

\begin{proofbox}
	Combining the facts we know, we get
\[
	[\alpha(v)]_{\mathcal{C}} = Q[\alpha(v)]_{\mathcal{C}'} = Q[a]_{\mathcal{B}', \mathcal{C}'} [v]_{\mathcal{B}'} = Q A' [v]_{\mathcal{B}'}.
\]
But we also know
\[
	[\alpha(v)]_{\mathcal{C}} = [\alpha]_{\mathcal{B}, \mathcal{C}}[v]_{\mathcal{B}} = A P [v]_{\mathcal{B}'}
.\]
But since this is true for any $v \in V$, we get $QA' = AP$, so $A' = Q^{-1}AP$.
\end{proofbox}

\begin{definition}[Equivalent matrices]\index{equivalent matrices}
	Two matrices $A, B \in \mathcal{M}_{m, n}(F)$ are equivalent if $A' = Q^{-1}AP$, where $Q \in \mathcal{M}_{m, m}$ and $P \in \mathcal{M}_{n, n}$ are invertible.
\end{definition}

\begin{remark}
	This defines an equivalence relation on $\mathcal{M}_{m, n}(F)$, as
	\begin{itemize}
		\item $A = I_m^{-1}AI_n$,
		\item If $A' = Q^{-1}AP$, then $A = (Q^{-1})^{-1}A' P^{-1}$,
		\item If $A' = Q^{-1}AP$, $A'' = (Q')^{-1}A'P'$, then $A'' = (QQ')^{-1}A(PP')$.
	\end{itemize}
	
\end{remark}

\begin{proposition}
	Let $V, W$ be vector spaces over $F$, with $\dim_F V = n$, $\dim_F W = m$. Let $\alpha : V \to W$ be a linear map. Then there exists $\mathcal{B}$, $\mathcal{C}$ bases of $V, W$ such that
	\[
		[\alpha]_{\mathcal{B}, \mathcal{C}} =
		\begin{pmatrix}
			I_r & 0 \\
			0 & 0
		\end{pmatrix}
	.\]
\end{proposition}

\begin{proofbox}
	Choose $\mathcal{B}$ and $\mathcal{C}$ wisely. Fix $r \in \mathbb{N}$ such that $\dim \Ker \alpha = n - r$. Let $N(\alpha) = \Ker(\alpha) = \{x \in V \mid \alpha(x) = 0\}$. Fix any basis of $N(x)$, $(v_{r+1}, \ldots, v_n)$, and extend it to a basis $\mathcal{B} = (v_1, \ldots, v_r, v_{r+1}, \ldots, v_n)$.

	We claim that $(\alpha(v_1), \ldots, \alpha(v_r))$ is a basis of $\Img \alpha$.
	\begin{itemize}
		\item First, if $v = \sum \lambda_i v_i$, then
			\[
				\alpha(v) = \sum_{i = 1}^{n} \lambda_i \alpha(v_i) = \sum_{i = 1}^{r} \lambda_i \alpha(v_i)
			.\]
			Let $y \in \Img \alpha$, so then
			\[
				y = \sum_{i = 1}^{r} \lambda_i \alpha(v_i)
			.\]
			So $y \in \langle \alpha(v_1), \ldots, \alpha(v_r)\rangle$.
		\item Now, suppose that it is not free, so
			\[
				\sum_{i = 1}^{r} \lambda_i \alpha(v_i) = 0
			.\]
			Then we get
			\[
				\alpha\left( \sum_{i = 1}^{r} \lambda_i v_i \right) = 0
			,\]
			so
			\[
			\sum_{i = 1}^{r} \lambda_i v_i \in \Ker \alpha
			.\]
			Hence, we get that
			\[
			\sum_{i = 1}^{r} \lambda_i v_i = \sum_{i = 1}^{n} \mu_i v_i
			.\]
			But since $(v_1, \ldots, v_n)$ is a basis, $\lambda_i = \mu_i = 0$.
	\end{itemize}
	So we have $(\alpha(v_1), \ldots, \alpha(v_r))$ is a basis of $\Img \alpha$, and $(v_{r+1}, \ldots, v_{n})$ is a basis of $\Ker \alpha$. Let $\mathcal{C} = (\alpha(v_1), \ldots, \alpha(v_r), w_{r+1}, \ldots, w_{m})$. We get that
	\[
		[\alpha]_{\mathcal{B}, \mathcal{C}} = (\alpha(v_1), \ldots, \alpha(v_r), \alpha(v_{r+1}), \ldots, \alpha(v_n)) =
		\begin{pmatrix}
			I_r & 0 \\
			0 & 0
		\end{pmatrix}	
	.\]
\end{proofbox}

\begin{remark}
	This proves another proof of the rank-nullity theorem: $r(\alpha) + n(\alpha) = n$.
\end{remark}

\begin{corollary}
	Any $m \times n$ matrix is equivalent to
	\[
	\begin{pmatrix}
		I_r & 0 \\
		0 & 0
	\end{pmatrix}
	,\]
	where $r = \rank(\alpha)$.
\end{corollary}

\begin{definition}
	For $a \in \mathcal{M}_{m, n}(F)$, the column rank\index{column rank} $r_c(A)$ of $A$ is the dimension of the span of the column vectors of $A$ in $F^{m}$. Similarly, the row rank\index{row rank} is the column rank of $A^{T}$.
\end{definition}

\begin{remark}
	If $\alpha$ is a linear map represented by $A$ with respect to one basis, the column rank $A$ equals the rank of $\alpha$.
\end{remark}

\begin{proposition}
	Two matrices are equivalent if and only if $r_c(A) = r_c(A')$.
\end{proposition}

\begin{proofbox}
	If $A$ and $A'$ are equivalent then they coorespond to the same linear map $\alpha$ except in two different bases.

Conversely, if $r_c(A) = r_c(A') = r$, then both $A$ and $A'$ are equivalent to
\[
\begin{pmatrix}
	I_r & 0 \\
	0 & 0
\end{pmatrix}
,\]
hence are equivalent.
\end{proofbox}

\begin{theorem}
	$r_c(A) = r_c(A^{T})$, so column rank equals row rank.
\end{theorem}

\begin{proofbox}
	If $r = r_c(A)$, then
	\[
	Q^{-1}AP =
	\begin{pmatrix}
		I_r & 0 \\
		0 & 0
	\end{pmatrix}
	.\]
	Take the transpose, to get
	\[
		(Q^{-1}AP)^{T} = P^{T}A^{T}(Q^{-1})^{T} = P^{T}A^{T}(Q^{T})^{-1} =
		\begin{pmatrix}
			I_r & 0 \\
			0 & 0
		\end{pmatrix}
	.\]
	Hence $r_c(A^{T}) = r = r_c(A)$.
\end{proofbox}

\subsection{Elementary operations and Elementary matrices}%
\label{sub:elementary_operations_and_elementary_matrices}

This is a special case of the change of basis formula, when $\alpha : V \to V$ is a map from a vector space to itself, called an endomorphism.\index{endomorphism} Suppose $\mathcal{B} = \mathcal{C}$ and $\mathcal{B}' = \mathcal{C}'$, and $P$ is the change of basis matrix from $\mathcal{B}'$ to $\mathcal{B}$. Then
\[
	[\alpha]_{\mathcal{B'},  \mathcal{B}'} = P^{-1} [\alpha]_{\mathcal{B}, \mathcal{B}} P
.\]

\begin{definition}
	Let $A, A'$ be $n \times n$ matrices. We say that $A$ and $A'$ are similar\index{similar matrices} if and only if $A' = P^{-1}AP$ for a square invertible matrix $P$.
\end{definition}

\begin{definition}
	The elementary column operations\index{elementary column operation} on an $m \times n$ matrix $A$ are:
	\begin{enumerate}[(i)]
		\item Swap columns $i$ and $j$;
		\item Replace column $i$ by $\lambda$ times column $i$;
		\item Add $\lambda$ times column $i$ to column $j$, for $i \neq j$.
	\end{enumerate}
	The elementary row operations are analogously defined\index{elementary row operation}.
\end{definition}

Note elementary operations are invertible, and all operations can be realized through the action of elementary matrices\index{elementary matrices}:
\begin{enumerate}[(i)]
	\item For swapping columns $i$ and $j$, we can take an identity matrix, but with $a_{ij} = a_{ji} = 1$, and $a_{ii} = a_{jj} = 0$.
	\item For multiplying column $i$ by $\lambda$, we can take an identity matrix but with $a_{ii} = \lambda$.
	\item For adding $\lambda$ times columns $i$ to column $j$, we can take an identity matrix but with $a_{ij} = \lambda$.
\end{enumerate}
An elementary columns (resp. row) operation can be done by multiplying $A$ by the corresponding elementary matrix from the right (resp. left).

We will now show that any $m \times n$ matrix is equivalent to
\[
\begin{pmatrix}
	I_r & 0 \\
	0 & 0
\end{pmatrix}
.\]
Start with a matrix $A$. If all entries are zero, we are done. Otherwise, pick $a_{ij} = \lambda \neq 0$. By swapping columns and rows, we can ensure $a_{11} = \lambda$. Multiplying column 1 by $1/\lambda$, we get $a_{11} = 1$. We can then clean out row 1 by subtracting a suitable multiply of column 1 from every row, and similarly from column 1. This gives us a matrix
\[
\begin{pmatrix}
	1 & 0 & \cdots & 0 \\
	0 & & & \\
	\vdots & & \tilde A & \\
	0 & & &
\end{pmatrix}
.\]
Iterating with $\tilde A$, a strictly smaller matrix, eventually gives
\[
\begin{pmatrix}
	I_r & 0 \\
	0 & 0
\end{pmatrix} = Q^{-1}AP
.\]
A variation of this is known as \textit{Gauss' pivot algorithm}\index{Gauss' pivot algorithm}. If we only use row operations, we can reach the row-echelon form of the matrix\index{row echelon form}:
\begin{itemize}
	\item Assume that $a_{i1} \neq 0$ for some $i$.
	\item Swap rows $i$ and $1$.
	\item Divide first row by $\lambda = a_{i1}$.
	\item Use $1$ in $a_{11}$ to clean the first column.
	\item Iterate over all columns.
\end{itemize}
This procedure is what is usually done when solving a system of linear equations.

\begin{lemma}
	If $A$ is an $n \times n$ square invertible matrix, then we can obtain $I_n$ using either only row or column elementary operations.
\end{lemma}

\begin{proofbox}
	We prove for column operations; row operations are analogous. We proceed by induction on the number of rows.
\begin{itemize}
	\item Suppose that we could write $A$ in the form
		\[
		\begin{pmatrix}
			I_h & 0 \\
			\ast & \ast
		\end{pmatrix}
		.\]
		Then we want to obtain the same structure as we go from $h$ to $h + 1$.
	\item We show there exists $j > h$ such that $\lambda = a_{h+1, j} \neq 0$. Otherwise, the row rank is less than $n$, as the first $h+1$ rows are linearly dependent. Hence $\rank A < n$.
	\item We swap columns $h + 1$ and $j$, so $\lambda = a_{h+1,h+1} \neq 0$, and then divide by $\lambda$.
	\item Finally, we can use the $1$ in $a_{h+1, h+1}$ to clear out the rest of the $(h+1)$'st row.
\end{itemize}
\end{proofbox}

This gives $A E_1 \ldots E_c = I_n$, or $A^{-1} = E_1 \ldots E_c$. This is an algorithm for computing $A^{-1}$.

\begin{proposition}
	Any invertible square matrix is a product of elementary matrices.
\end{proposition}

\newpage

\section{Dual Spaces}%
\label{sec:dual_spaces}

\begin{definition}
	$V$ is a $F$-vector space. We say $V^{\ast}$ is the dual of $V$ if
	\[
		V^{\ast} = \mathcal{L}(V, F) = \{ \alpha : V \to F \text{ linear}\}
	.\]\index{dual space}
	If $\alpha : V \to F$ is linear, then we say $\alpha$ is a linear form.\index{linear form}
\end{definition}

\begin{exbox}
	\begin{enumerate}[(i)]
		\item $\tr : \mathcal{M}_{n, n}(F) \to F$ is a linear map, so $\tr \in \mathcal{M}_{n,n}^{\ast}(F)$.
		\item Let $f : [0, 1] \to \mathbb{R}$ by $x \mapsto f(x)$, and $Tf : \mathcal{C}^{\infty}([0,1], \mathbb{R}) \to \mathbb{R}$ by
			\[
				\phi \mapsto \int_{0}^{1} f(x)\phi(x)\diff x
			.\]
			Then $Tf$ is a linear form.
	\end{enumerate}
\end{exbox}

\begin{lemma}
	Let $V$ be a vector space over $F$ with a finite basis $\mathcal{B} = \{e_1, \ldots, e_n\}$. Then there exists a basis for $V^{\ast}$ given by $\mathcal{B}^{\ast} = \{\varepsilon_1, \ldots, \varepsilon_n\}$, with
	\[
		\varepsilon_j \Biggl( \sum_{i = 1}^{n} a_i e_i \Biggr) = a_j
	.\]
	Then $\mathcal{B}^{\ast}$ is the dual basis of $\mathcal{B}$.\index{dual basis}
\end{lemma}

\begin{remark}
	If we define the Kronecker symbols
	\[
	\delta_{ij} =
	\begin{cases}
		1 & i = j, \\
		0 & \text{otherwise},
	\end{cases}
	\]
	then we can equivalently define
	\[
		\varepsilon_j \Biggl( \sum_{i = 1}^{n} a_i e_i \Biggr) = a_j \iff \varepsilon_j(e_i) = \delta_{ij}
	.\]
\end{remark}

\begin{proofbox}
	Let $(\varepsilon_1, \ldots, \varepsilon_n)$ be defined as above.

	We prove $(\varepsilon_i)$ are free. Indeed, suppose
	\[
		\sum_{j = 1}^{n} \lambda_j \varepsilon_j = 0 \implies \sum_{j = 1}^{n} \lambda_j e_j(e_i) = 0 \implies \lambda_i = 0
	.\]
	Now we show $(\varepsilon_i)$ generates $V^{\ast}$. Pick $\alpha \in V^{\ast}$, then for $x \in V$, we have
	\[
		\alpha(x) = \alpha \Biggl( \sum_{j = 1}^{n} \lambda_j e_j \Biggr) = \sum_{j = 1}^{n} \lambda_j \alpha(e_j)
	.\]
	On the other hand, consider the linear form
	\[
		\sum_{j = 1}^{n} \alpha(e_j) \varepsilon_j \in V^{\ast}
	.\]
	Then we have
	\begin{align*}
		\sum_{j = 1}^{n} \alpha(e_j) \varepsilon_j(x) &= \sum_{j = 1}^{n} \alpha(e_j) \varepsilon_j\Biggl( \sum_{k = 1}^{n} \lambda_k e_k \Biggr) = \sum_{j = 1}^{n} \alpha(e_j) \sum_{k = 1}^{n} \lambda_k \varepsilon_j (e_k) \\
							      &= \sum_{j = 1}^{n} \alpha(e_j) \lambda_j = \alpha(x).
	\end{align*}
	Hence $(\varepsilon_i)$ generates $V^{\ast}$.
\end{proofbox}


\begin{corollary}
	If $V$ is finite dimensional, then $\dim V^{\ast} = \dim V$.
\end{corollary}

This is very different in infinite dimensions.

\begin{remark}
	It is sometimes convenient to think of $V^{\ast}$ as the space of row vector of length $n$ over $F$. If $(e_1, \ldots, e_n)$ is a basis of $v$ such that $x = \sum x_i e_i$ and $(\varepsilon_1, \ldots, \varepsilon_n)$ is a basis of $V^{\ast}$ such that $\alpha = \sum \alpha_i \varepsilon_i$, then
	\begin{align*}
		\alpha(x) &= \sum_{i = 1}^{n} \alpha_i \varepsilon_i \Biggl( \sum_{j = 1}^{n} x_j e_j \Biggr) = \sum_{i = 1}^{n} \alpha_i \sum_{j = 1}^{n} x_j \varepsilon_i(e_j) = \sum_{i = 1}^{n} \alpha_i x_i \\
			  &= 
			  \begin{pmatrix}
				  \alpha_1 & \cdots & \alpha_n
			  \end{pmatrix}
			  \begin{pmatrix}
			  	x_1 \\
				\vdots \\
				x_n
			  \end{pmatrix}.
	\end{align*}
	This gives a scalar product structure on $V^{\ast}$.
\end{remark}

\begin{definition}
	If $U \leq V$, we define the annihilator\index{annihilator} of $U$ by
	\[
		U^{\circ} = \{\alpha \in V^{\ast} \mid \alpha(u) = 0 \; \forall u \in U\}
	.\]
\end{definition}

\begin{lemma}
	\begin{enumerate}[\normalfont(i)]
		\item[]
		\item $U^{\circ} \le V^{\ast}$.
		\item If $U \leq V$ and $\dim V < \infty$, then $\dim V = \dim U + \dim U^{\circ}$.
	\end{enumerate}
\end{lemma}

\begin{proofbox}
	Suppose $\alpha, \alpha' \in U^{\circ}$. Then for all $u \in U$,
	\[
		(\alpha + \alpha')(u) = \alpha(u) + \alpha'(u) = 0
	,\]
	and for all $\lambda \in F$, $(\lambda \alpha)(u) = \lambda \alpha(u) = 0$. Hence $U^{\circ} \le V^{\ast}$.

	Now let $U \leq V$, and $\dim V = n$. Let $(e_1, \ldots, e_k)$ be a basis of $U$ and complete it to a basis $\mathcal{B} = (e_1, \ldots, e_k, e_{k+1}, \ldots, e_n)$ of $V$. Let $(\varepsilon_1, \ldots, \varepsilon_n)$ be the dual basis of $\mathcal{B}$. Then I claim $U^{\circ} = \langle \varepsilon_{k+1}, \ldots, \varepsilon_n\rangle$.

	Indeed, pick $i > k$, then $\varepsilon_i(e_k) = \delta_{ik} = 0$, so $\varepsilon_i \in U^{\circ}$. Now let $\alpha \in U^{\circ}$. Then $(\varepsilon_1, \ldots, \varepsilon_n)$ is a basis of $V^{\ast}$ implies $\alpha = \sum \alpha_i \varepsilon_i$. But $\alpha \in U^{\circ} \implies \alpha(e_i) = 0$, which gives $\alpha_i = 0$ for $i \leq k$. Hence $\alpha \in \langle \varepsilon_{k+1}, \ldots, \varepsilon_n\rangle$.
\end{proofbox}

\begin{definition}
	Let $V, W$ be vector spaces over $F$, and let $\alpha \in \mathcal{L}(V, W)$. Then the map
	\begin{align*}
		\alpha^{\ast} : W^{\ast} &\to V^{\ast} \\
		\varepsilon &\mapsto \varepsilon \circ \alpha
	\end{align*}
	is an element of $\mathcal{L}(W^{\ast}, V^{\ast})$. This is known as the \textit{dual map} of $\alpha$.\index{dual map}
\end{definition}

\begin{proofbox}
	$\varepsilon \circ \alpha : V \to F$ is linear due to the linearity of $\varepsilon$ and $\alpha$. Hence $\varepsilon \circ \alpha \in V^{\ast}$.

We show $\alpha^{\ast}$ is linear. Let $\theta_1, \theta_2 \in W^{\ast}$. Then,
\[
	\alpha^{\ast}(\theta_1 + \theta_2) = (\theta_1 + \theta_2)(\alpha) = \theta_1 \circ \alpha + \theta_2 \circ \alpha = \alpha^{\ast} (\theta_1) + \alpha^{\ast}(\theta_2)
.\]
Similarly, if $\lambda \in F$, then
\[
	\alpha^{\ast}(\lambda \theta) = \lambda \alpha^{\ast}(\theta)
.\]
Hence $\alpha^{\ast} \in \mathcal{L}(W^{\ast}, V^{\ast})$.
\end{proofbox}

\begin{proposition}
	Let $V, W$ be finite dimensional spaces over $F$ with bases $\mathcal{B}, \mathcal{C}$. Let $\mathcal{B}^{\ast}, \mathcal{C}^{\ast}$ be the dual bases for $V^{\ast}, W^{\ast}$. Then
	\[
		[\alpha^{\ast}]_{\mathcal{C}^{\ast}, \mathcal{B}^{\ast}} = [\alpha]_{\mathcal{B},\mathcal{C}}^{T}
	.\]
\end{proposition}

\begin{proofbox}
	Let $\mathcal{B} = (b_1, \ldots, b_n), \mathcal{C} = (c_1, \ldots, c_m)$, $\mathcal{B}^{\ast} = (\beta_1, \ldots, \beta_n), \mathcal{C}^{\ast} = (\gamma_1, \ldots, \gamma_m)$. Say $[\alpha]_{\mathcal{B}, \mathcal{C}} = A = (a_{ij})$. Recall $\alpha^{\ast} : W^{\ast} \to V^{\ast}$, so let us compute
	\begin{align*}
		\alpha^{\ast}(\gamma_r)(b_s) = \gamma_r \circ \alpha(b_s) = \gamma_r \Biggl( \sum_{t} a_{ts} c_t \Biggr) = \sum_{t} a_{ts} \gamma_r(c_t) = a_{rs}.
	\end{align*}
	Say that
	\[
		[\alpha^{\ast}]_{\mathcal{C}^{\ast}, \mathcal{B}^{\ast}} = 
		\begin{pmatrix}
			\alpha^{\ast}(\gamma_1) & \cdots & \alpha^{\ast}(\gamma_m)
		\end{pmatrix}
		= (m_{ij})
	.\]
	Then we can find that
	\[
		\alpha^{\ast}(\gamma_r) = \sum_{i = 1}^{n} m_{ir}\beta_i
	,\]
	so
	\[
		\alpha^{\ast}(\gamma_r)(b_s) = m_{sr}
	.\]
	This gives $a_{rs} = m_{sr}$, as desired.
\end{proofbox}

\subsection{Properties of the Dual Map}%
\label{sub:properties_of_the_dual_map}

Recall if $V, W$ are vector spaces over $F$, and $\alpha \in \mathcal{L}(V, W)$, then we can construct a dual map
\begin{align*}
	\alpha^{\ast} : W^{\ast} &\to V^{\ast} \\
	\varepsilon &\mapsto \varepsilon \circ \alpha
\end{align*}
Moreover, if $\mathcal{B}, \mathcal{C}$ are bases of $V$ and $W$, and $\mathcal{B}^{\ast}$, $\mathcal{C}^{\ast}$ are the dual bases of $\mathcal{B}$ and $\mathcal{C}$ respectively, then
\[
	[\alpha^{\ast}]_{\mathcal{C}^{\ast}, \mathcal{B}^{\ast}} = [\alpha]_{\mathcal{B}, \mathcal{C}}^{T}
.\]
Now if $\mathcal{E} = (e_1, \ldots, e_n)$ is a basis of $V$ and $\mathcal{F} = (f_1, \ldots, f_n)$ is another basis of $V$, then consider the change of basis matrix
\[
	P = [\id]_{\mathcal{F}, \mathcal{E}}
.\]
Consider $\mathcal{E}^{\ast} = (\varepsilon_1, \ldots, \varepsilon_n)$ and $\mathcal{F}^{\ast} = (\eta_1, \ldots, \eta_n)$.

\begin{lemma}
	The change of basis matrix from $\mathcal{F}^{\ast}$ to $\mathcal{E}^{\ast}$ is
	\[
		(P^{-1})^{T}
	.\]
\end{lemma}
\begin{proofbox}
	We have
\[
	[\id]_{\mathcal{F}^{\ast}, \mathcal{E}^{\ast}} = [\id]_{\mathcal{E}, \mathcal{F}}^{T} = ([\id]_{\mathcal{F}, \mathcal{E}}^{-1})^{T}
.\]
\end{proofbox}

\begin{lemma}
	Let $V, W $ be vector spaces over $F$. Let $\alpha \in \mathcal{L}(V, W)$ and $\alpha^{\ast} \in \mathcal{L}(W^{\ast}, V^{\ast})$. Then
	\begin{enumerate}[\normalfont(i)]
		\item $\Ker(\alpha^{\ast}) = (\Img \alpha)^{\circ}$. Hence $\alpha^{\ast}$ is injective if and only if $\alpha$ is surjective.
		\item $\Img \alpha^{\ast} \leq (\Ker \alpha)^{\circ}$ with equality if $V, W$ are finite dimensional. Hence in this case, $\alpha^{\ast}$ is injective if and only if $\alpha$ is injective.
	\end{enumerate}
\end{lemma}

There are many problems where the understanding of $\alpha^{\ast}$ is simpler than the understanding of $\alpha$.

\begin{proofbox}
\begin{enumerate}[(i)]
	\item Let $\varepsilon \in W^{\ast}$. Then $\varepsilon \in \Ker \alpha^{\ast} \iff \alpha^{\ast} (\varepsilon) = 0$. But $\alpha^{\ast}(\varepsilon) = \varepsilon(\alpha)$, so for all $x$,
			\[
				\varepsilon(\alpha)(x) = \varepsilon(\alpha(x)) = 0
			.\]
			This holds if and only if $\varepsilon \in (\Img \alpha)^{\circ}$.
		\item We will first show that
			 \[
				 \Img \alpha^{\ast} \leq (\Ker \alpha)^{\circ}
			.\]
			Indeed, if $\varepsilon \in \Img \alpha^{\ast}$, then $\varepsilon = \alpha^{\ast}(\phi)$, so for all $u \in \Ker \alpha$,
			\[
				\varepsilon(u) = \alpha^{\ast}(\phi) (u) = \phi \circ \alpha(u) = \phi(0) = 0
			.\]
			Hence $\varepsilon \in (\Ker \alpha)^{\circ}$. In finite dimension, we can compare the dimension of $\Img \alpha^{\ast}$ and $(\Ker \alpha)^{\circ}$. Indeed,
			\[
				\dim (\Img \alpha^{\ast}) = r(\alpha^{\ast}) = r([\alpha^{\ast}]_{\mathcal{C}^{\ast}, \mathcal{B}^{\ast}}) = r([\alpha]_{\mathcal{B},\mathcal{C}}^{T}) = r([\alpha]_{\mathcal{B}, \mathcal{C}}) = r(\alpha)
			.\]
			Hence, we get
			\[
				\dim (\Img \alpha^{\ast}) = r(\alpha^{\ast}) = r(\alpha) = \dim V - \dim \Ker \alpha = \dim [(\Ker \alpha)^{\circ}]
			.\]
			Since the dimensions are the same, we get $\Img \alpha^{\ast} = (\Ker \alpha)^{\circ}$.
\end{enumerate}
\end{proofbox}

\subsection{Double Dual}%
\label{sub:double_dual}

If $V$ is a vector space over $F$, then $V^{\ast} = \mathcal{L}(V, F)$.

We define the \textit{bidual}\index{bidual}\index{double dual} as
\[
	V^{\ast \ast} = (V^{\ast})^{\ast} = \mathcal{L}(V^{\ast}, F)
.\]
This is a very important space in infinite dimension. In general, there is no obvious connection between $V$ and $V^{\ast}$. However, there is a large class of function spaces such that
\[
V \cong V^{\ast \ast}
.\]
This is known as a reflexive space\index{reflexive space}.

\begin{exbox}
	For $p > 2$, define
	\[
		L^{p}(\mathbb{R}) = \biggl\{ f : \mathbb{R} \to \mathbb{R} \,\bigg\vert\, \int_{\mathbb{R}} |f(x)|^{p} \diff x < \infty \biggr\}
	.\]
	This is an example of a reflexive space.
\end{exbox}

In general, there is a canonical embedding of $V$ into $V^{\ast \ast}$. Indeed, pick $v \in V$. We define
\begin{align*}
	\hat v : V^{\ast} &\to F \\
	\varepsilon &\mapsto \varepsilon(v)
\end{align*}
Then this is linear, as
\[
	\hat v(\lambda_1 \varepsilon_1 + \lambda_2 \varepsilon_2) = (\lambda_1 \varepsilon_1 + \lambda_2 + \varepsilon_2)(v) = \lambda_1 \varepsilon_1(v) + \lambda_2 \varepsilon_2(v) = \lambda_1 \hat v(\varepsilon_1) + \lambda_2 \hat v(\varepsilon_2)
.\]

\begin{theorem}
	If $V$ is a finite dimensional vector space over $F$, then the hat map $v \mapsto \hat v$ is an isomorphism.
\end{theorem}

In infinite dimension, under certain assumption (e.g. Banach space) we can show that the hat map is injective.

\begin{proofbox}
	If $V$ is finite dimensional, then first note that for $v \in V$, $\hat v \in V^{\ast \ast}$. We show the hat map is linear: for $v_1, v_2 \in V$, $\lambda_1, \lambda_2 \in F$ and $\varepsilon \in V^{\ast}$,
	\[
		\widehat{\lambda_1 v_1 + \lambda_2 v_2} (\varepsilon) = \varepsilon(\lambda_1 v_1 + \lambda_2 v_2) = \lambda_1 \varepsilon(v_1) + \lambda_1 \varepsilon_2(v_2) = \lambda_1 \hat v_1(\varepsilon) + \lambda_2 \hat v_2 (\varepsilon)
	.\]
	Now we show the hat map is injective. Let $e \in V \setminus \{0\}$. Then extend to a basis $(e, e_2, \ldots, e_n)$. Let $(\varepsilon, \varepsilon_2, \ldots, \varepsilon_n)$ be the dual basis. Then
	\[
		\hat e (\varepsilon) = \varepsilon(e) = 1
	.\]
	Hence $\hat e \neq \{0\}$, so the hat map is injective.

	Finally, we show the hat map is an isomorphism. We already know $\dim V = \dim V^{\ast}$, and as a result $\dim V^{\ast} = \dim V^{\ast \ast}$. Thus, since the hat map is injective, it is an isomorphism.
\end{proofbox}

\begin{lemma}
	Let $V$ be a finite dimensional vector space over $F$, and let $U \leq V$. Then
	\[
	\hat U = U^{\circ \circ}
	.\]
	Hence after identification of $V$ and $V^{\ast \ast}$, we get
	\[
	U = U^{\circ \circ}
	.\]
\end{lemma}

\begin{proofbox}
	We will show $U \leq U^{\circ \circ}$. Indeed, let $u \in U$. Then for all $\varepsilon \in U^{\circ}$, $\varepsilon(u) = 0$. So for all $\varepsilon \in U^{\circ}$, $\hat u(\varepsilon) = \varepsilon(u) = 0$. Hence $\hat u \in U^{\circ \circ}$, so $\hat U \subset U^{\circ \circ}$.

	But then we can compute dimension to find
	\[
	\dim U^{\circ \circ} = \dim V - \dim U^{\circ} = \dim U
	,\]
	proving this lemma.
\end{proofbox}

\begin{remark}
	If $T \leq V^{\ast}$, then
	\[
		T^{\circ} = \{v \in V \mid \theta(v) = 0, \, \forall \theta \in T\}
	.\]
\end{remark}

\begin{lemma}
	Let $V$ be a finite dimensional vector space over $F$. Let $U_1, U_2 \leq V$. Then,
	\begin{enumerate}[\normalfont(i)]
		\item $(U_1 + U_2)^{\circ} = U_1^{\circ} \cap U_2^{\circ}$,
		\item $(U_1 \cap U_2)^{\circ} = U_1^{\circ} + U_2^{\circ}$.
	\end{enumerate}
\end{lemma}

\begin{proofbox}
\begin{enumerate}[(i)]
	\item Let $\theta \in V^{\ast}$, then
		\begin{align*}
			\theta \in (U_1 + U_2)^{\circ} &\iff \theta(u_1 + u_2) = 0 \iff \theta(u) = 0 \, \forall u \in U_1 \cup U_2 \\
						       &\iff \theta \in U_1^{\circ} \cap U_2^{\circ}.
		\end{align*}
		Hence $(U_1 + U_2)^{\circ} = U_1^{\circ} \cap U_2^{\circ}$.
	\item Looking at (i), we can take the annihilator of everything to get
		\[
			(U_1 \cap U_2)^{\circ} = (U_1^{\circ} + U_2^{\circ})^{\circ \circ} = U_1^{\circ} + U_2^{\circ}
		.\]
\end{enumerate}
\end{proofbox}

\newpage

\section{Determinant and Traces}%
\label{sec:determinant_and_traces}

\begin{definition}
	If $A \in \mathcal{M}_{n}(F)$, we define the trace\index{trace} of $A$ as
	\[
	\tr A = \sum_{i = 1}^{n} A_{ii}
	.\]
\end{definition}

\begin{remark}
	The map $\mathcal{M}_n(F) \to F$ by $A \mapsto \tr A$ is linear.
\end{remark}

\begin{lemma}
	$\tr (AB) = \tr (BA)$.
\end{lemma}

\begin{proofbox}
\[
	\tr (AB) = \sum_{i = 1}^{n} \Biggl( \sum_{j = 1}^{n} a_{ij} b_{ji} \Biggr) = \sum_{j = 1}^{n} \sum_{i = 1}^{n} b_{ji} a_{ij} = \tr (BA)
.\]
\end{proofbox}

\begin{corollary}
	Similar matrices have the same trace, as
	\[
		\tr(P^{-1}AP) = \tr(APP^{-1}) = \tr(A)
	.\] 
\end{corollary}

\begin{definition}
	If $\alpha : V \to V$ is linear, we can define
	\[
		\tr \alpha = \tr([\alpha]_{\mathcal{B}})
	\]
	in any basis $\mathcal{B}$.\index{trace of endomorphism}
\end{definition}

\begin{lemma}
	If $\alpha : V \to V$ with $\alpha^{\ast} : V^{\ast} \to V^{\ast}$ the dual map,
\end{lemma}

\begin{proofbox}
	\[
		\tr \alpha = \tr([\alpha]_{\mathcal{B}}) = \tr([\alpha]_{\mathcal{B}}^{T}) = \tr([\alpha^{\ast}]_{\mathcal{B}^{\ast}}
	.\]
\end{proofbox}

\subsection{Permutations and Transposition}%
\label{sub:permutations_and_transposition}

We define $S_n$ as the symmetric group\index{symmetric group}, the permutations of $X = \{1, \ldots, n\}$.

The transposition $\tau_{k, \ell} \in S_n$ for $k \neq l$\index{transposition} is $\tau_{k, \ell} = (k, \ell)$.

Then we know any permutation $\sigma$ can be decomposed as a product of transpositions:
\[
\sigma = \prod_{i = 1}^{n_{\sigma}} \tau_i
.\]
The signature\index{signature}\index{sign} is a map
\begin{align*}
	\varepsilon : S_n &\to \{\pm 1\} \\
	\sigma &\mapsto
	\begin{cases}
		1 & n_{\sigma} \text{ even},\\
		-1 & n_{\sigma} \text{ odd}.
	\end{cases}
\end{align*}

\begin{definition}
	For $A \in \mathcal{M}_{n}(F)$, and $A = (a_{ij})$, we define the determinant of $A$\index{determinant} as
	\[
		\det A = \sum_{\sigma S_n}\varepsilon(\sigma)a_{\sigma(1) 1} a_{\sigma(2) 2} \cdots a_{\sigma(n) n}
	.\]
\end{definition}

\begin{exbox}
	For $n = 2$, we have
	\[
	\det
	\begin{pmatrix}
		a_{11} & a_{12} \\
		a_{21} & a_{22}
	\end{pmatrix}
	= a_{11}a_{22} - a_{12}a_{21}
	.\]
\end{exbox}

\begin{lemma}
	If $A = (a_{ij})$ is an upper (or lower) triangular matrix with $0$ on the diagonal, then $\det A = 0$.
\end{lemma}

\begin{proofbox}
	\[
		\det A = \sum_{\sigma \in S_n} a_{\sigma(1) 1} \cdots a_{\sigma(n) n}
	\]
	For the summands not to be $0$, we need $\sigma(j) < j$ for all $j \in \{1, \ldots, n\}$. But this is impossible for all $\sigma \in S_n$, so all summands are $0$, and $\det A = 0$.

	Similarly, if $A$ is upper-triangular, not necessarily with $0$'s on the diagonal, then the summands are non-zero only if $\sigma(j) \leq j$ for all $j \in \{1, \ldots, n\}$. By induction and the fact $\sigma$ is a permutation, we get $\sigma(j) = j$ for all $j \in \{1, \ldots, n\}$ and the only term that doesn't vanish is $a_{11}a_{22}\cdots a_{nn}$. Hence
	\[
	\det
	\begin{pmatrix}
		\lambda_1 & & \ast \\
		 & \ddots & \\
		0 &  & \lambda_n
	\end{pmatrix}
	= \prod_{i = 1}^{n} \lambda_i = \det
	\begin{pmatrix}
		\lambda_1 & & 0 \\
			  & \ddots & \\
		\ast & &\lambda_n
	\end{pmatrix}
	.\]
\end{proofbox}

\begin{lemma}
	$\det A = \det(A^{T})$.
\end{lemma}

\begin{proofbox}
	\begin{align*}
		\det A &= \sum_{\sigma \in S_n} \eps(\sigma) a_{\sigma(1) 1} \cdots a_{\sigma(n) n} = \sum_{\sigma \in S_n} \eps(\sigma) \prod_{i = 1}^{n} a_{\sigma(i) i} \\
		       &= \sum_{\sigma \in S_n} \eps(\sigma) \prod_{i = 1}^{n} a_{i \sigma^{-1}(i)}.
	\end{align*}
	Now remember $1 = \eps(\sigma \sigma^{-1}) = \eps(\sigma) \eps(\sigma^{-1})$, so $\eps(\sigma^{-1}) = \eps(\sigma)$. Hence
	\begin{align*}
		\det A &= \sum_{\sigma \in S_n} \eps(\sigma) \prod_{i = 1}^{n} a_{i \sigma^{-1}(i)} = \sum_{\sigma \in S_n} \eps(\sigma^{-1}) \prod_{i = 1}^{n}a_{i \sigma^{-1}(i)} \\
		       &= \sum_{\sigma \in S_n} \eps(\sigma) \prod_{i = 1}^{n} a_{i \sigma(i)} = \det (A^{T}).
	\end{align*}
\end{proofbox}

Our definition of $\det A$ has seemingly come out of nowhere. We want some reason to take this as our definition.

\begin{definition}
	A volume form\index{volume form} on $F^{n}$ is a function
	\[
		\underbrace{F^{n} \times \cdots \times F^{n}}_{n \text{ times}} \to F
	,\]
	such that
	\begin{enumerate}[(i)]
		\item It is multilinear,\index{multilinear} so for any $1 \leq i \leq n$, and all $(v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_n) \in (F^{n})^{n-1}$, we want the map
			\begin{align*}
				F^{n} &\to F \\
				v &\mapsto d(v_1, \ldots, v_{i-1}, v, v_{i+1}, \ldots, v_n)
			\end{align*}
			to be linear.
		\item It is alternate,\index{alternate} so if $v_i = v_j$ for some $i \neq j$, then
			\[
				d(v_1, \ldots, v_n) = 0
			.\]
	\end{enumerate}
\end{definition}

Then we want to show that there is in fact only one volume form on $F^{n} \times \cdots \times F^{n}$ given by the determinant: If $A = (a_{ij}) = (A^{(1)} \mid \ldots \mid A^{(n)})$, then we denote
\[
	\det A = \det(A^{(1)}, \ldots, A^{(n)})
.\]
\begin{lemma}
	$F^{n} \times \cdots \times F^{n} \to F$ by $(A^{(1)}, \ldots, A^{(n)}) \mapsto \det A$ is a volume form.
\end{lemma}

\begin{proofbox}
	\begin{enumerate}[(i)]
		\item Firstly, this map is multilinear. Pick $\sigma \in S_n$. Then the individual summands $\prod_{i = 1}^{n} a_{\sigma(i)i}$ are multilinear, as there is only one term from each column appearing in this expression.

			Since the sum of multilinear maps is multilinear, $\det$ is multilinear.
		\item Now we show the map is alternate. Assume $k \neq \ell$, and $A^{(k)} = A^{(\ell)}$. Then we want to show $\det A = 0$. Let $\tau = (k, \ell)$ be a transposition. Then note $A^{(k)} = A^{(\ell)} \iff a_{ij} = a_{i \tau(j)}$ for all $i \in \{1, \ldots, n\}$.

				We can decompose $S_n = A_n \cup \tau A_n$. Here $A_n$ is the alternating group, which are the permutations with an even number of transpositions, and $\tau A_n$ are the permutations with an odd number of transpositions. Thus,
				\begin{align*}
					\det A &= \sum_{\sigma \in S_n} \eps(\sigma) \prod_{i = 1}^{n} a_{i \sigma(i)} = \sum_{\sigma \in A_n} \prod_{i = 1}^{n} a_{i \sigma(i)} + \sum_{\sigma \in \tau A_n} - \prod_{i = 1}^{n} a_{i \sigma(i)} \\
					       &= \sum_{\sigma \in A_n} \prod_{i = 1}^{n}a_{i \sigma(i)} - \sum_{\sigma \in A_n} a_{i \tau \sigma(i)} = \sum_{\sigma \in A_n} \Biggl( \prod_{i = 1}^{n} a_{i \sigma(i)} - \prod_{j = 1}^{n} a_{i \sigma(i)} \Biggr) \\
					       &= 0.
				\end{align*}
	\end{enumerate}
\end{proofbox}

\begin{lemma}
	Let $d$ be a volume form. Then swapping two entries changes the sign, so
	\[
		d(v_1, \ldots, v_i, \ldots, v_j, \ldots, v_n) = - d(v_1, \ldots, v_j, \ldots, v_i, \ldots, v_n)
	.\]
\end{lemma}

\begin{proofbox}
	\begin{align*}
		0 &= d(v_1, \ldots, v_i + v_j, \ldots, v_i + v_j, \ldots, v_n) \\
		  &= d(v_1, \ldots, v_i, \ldots, v_i, \ldots, v_n) + \quad + d(v_1, \ldots, v_i, \ldots, v_j, \ldots, v_n) \\
		  & \qquad + d(v_1, \ldots, v_j, \ldots, v_i, \ldots, v_n) + d(v_1, \ldots, v_j, \ldots, v_j, \ldots, v_n) \\
		  &= d(v_1, \ldots, v_i, \ldots, v_j, \ldots, v_n) + d(v_1, \ldots, v_j, \ldots, v_i, \ldots, n).
	\end{align*}
\end{proofbox}

\begin{corollary}
	If $\sigma \in S_n$, and $d$ is a volume form, then
	\[
		d(v_{\sigma(1)}, \ldots, v_{\sigma(n)}) = \eps(\sigma) d(v_1, \ldots, v_n)
	.\]
\end{corollary}

This follows as $\sigma$ is a product of transpositions.

\begin{theorem}
	Let $d$ be a volume form on $F^{n}$, and let $A = (A^{(1)}, \ldots, A^{(n)})$. Then,
	\[
		d(A^{(1)}, \ldots, A^{(n)}) = d(e_1, \ldots, e_n) \det A
	.\]
\end{theorem}

Hence, up to a constant, $\det$ is the only volume form on $F^{n}$.

\begin{proofbox}
	\begin{align*}
		d(A^{(1)}, \ldots, A^{(n)}) &= d \Biggl( \sum_{i = 1}^{n} a_{i1}e_1, \ldots, A^{(n)} \Biggr) = \sum_{i = 1}^{n} a_{i1} d(e_1, A^{(2)}, \ldots, A^{(n)}) \\
					    &= \sum_{i = 1}^{n} a_{i1} \Biggl( e_1, \sum_{j = 1}^{n} a_{j2}e_2, \ldots, A^{(n)}\Biggr) \\
					    &= \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{i1}a_{j2} d(e_i, e_j, \ldots, A^{(n)}) \\
					    &= \sum_{1 \leq i_k \leq n} \Biggl( \prod_{k = 1}^{n} a_{i_k k} \Biggr) d(e_{i_1}, e_{i_2}, \ldots, e_{i_n}).
	\end{align*}
	This last term is non-zero if and only if all $i_k$ are distinct, meaning there exists $\sigma \in S_n$ such that $i_k = \sigma(k)$. This means
	\begin{align*}
		d(A^{(1)}, \ldots, A^{(n)}) &= \sum_{\sigma \in S_n} \prod_{k = 1}^{n} a_{\sigma(k) k}d(e_{\sigma(1)}, \ldots, e_{\sigma(n)}) \\
					    &= \sum_{\sigma \in S_n} \Biggl[ \prod_{k = 1}^{n} a_{\sigma(k) k} \Biggr] \eps(\sigma) d(e_1, \ldots, e_n) \\
					    &= d(e_1, \ldots, e_n) \det A.
	\end{align*}
\end{proofbox}

\begin{corollary}
	$\det$ is the only volume form such that $d(e_1, \ldots, e_n) = 1$.
\end{corollary}

\subsection{Some Properties of Determinants}%
\label{sub:some_properties_of_determinants}

\begin{lemma}
	If $A, B \in \mathcal{M}_{n}(F)$, then 
	\[
		\det (AB) = (\det A)(\det B)
	.\]
\end{lemma}

\begin{proofbox}
	Pick $A$. Consider the map $d_a : \underbrace{F^{n} \times \cdots \times F^{n}}_{n} \to F$ defined by
	\[
		d_A(v_1, \ldots, v_n) = \det(Av_1, \ldots, Av_n)
	.\]
	Then $d_A$ is multilinear and alternate, as $v_i \mapsto A v_i$ is linear, and $v_i = v_j \implies A v_i = A v_j$. Thus, there exists $C$ such that
	\[
		d_A(v_1, \ldots, v_n) = C \det(v_1, \ldots, v_n)
	.\]
	Computing on the canonical basis,
	\[
		d_A(e_1, \ldots, e_n) = \det(Ae_1, \ldots, Ae_n) = \det(A_1, \ldots, A_n) = \det A
	.\]
	Hence, $C = \det A$.

	Now observe $AB = ((AB)_1, \ldots, (AB)_n)$, so
	\[
		\det(AB) = \det(AB_1, \ldots, AB_n) = (\det A)\det(B_1, \ldots, B_n) = (\det A)(\det B)
	.\]
\end{proofbox}

\begin{definition}
	For $A \in \mathcal{M}_n(F)$, we say that
	\begin{enumerate}[(i)]
		\item $A$ is singular if $\det A = 0$,\index{singular}
		\item $A$ is non-singular if $\det A \neq 0$.\index{non-singular}
	\end{enumerate}
\end{definition}

\begin{lemma}
	$A$ is invertible implies $A$ is non-singular.
\end{lemma}

\begin{proofbox}
	If $A$ is invertible, then there exists $A^{-1}$ such that $AA^{-1} = A^{-1}A = I_n$. Thus
	\[
		(\det A)(\det A^{-1}) = \det (AA^{-1}) = \det I_n = 1
	,\]
	so $\det A \neq 0$.
\end{proofbox}

\begin{remark}
	This also prove $\det A^{-1} = (\det A)^{-1}$.
\end{remark}

\begin{theorem}
	Let $A \in \mathcal{M}_n(F)$. Then the following are equivalent:
	\begin{enumerate}[\normalfont(i)]
		\item $A$ is invertible;
		\item $A$ is non-singular;
		\item $r(A) = n$.
	\end{enumerate}
\end{theorem}

\begin{proofbox}
	We have already seen (i) $\iff$ (iii), from rank nullity, and we have just shown (i) $\implies$ (ii). Thus it suffices to show (ii) $\implies$ (iii). Indeed, assume $r(a) < n$.

		Then $r(A) < n \iff \dim \spn \{c_1, \ldots, c_n\} < n$, so there exists $(\lambda_1, \ldots, \lambda_n) \neq (0, \ldots, 0)$ such that
			\[
			\sum_{i = 1}^{n} \lambda_i c_i = 0
			.\]
			Pick $j$ with $\lambda_j \neq 0$. Then,
			\[
			c_j = \frac{1}{\lambda_j} \sum_{i \neq j}\lambda_i c_i
			.\]
			This gives
			\[
				\det A = \det(c_1, \ldots, c_j, \ldots, c_n) = \det \Biggl( c_1, \ldots, \frac{-1}{\lambda_j} \sum_{i \neq j}\lambda_i c_i, \ldots, c_n \Biggr) = 0
			.\]
			Hence by contrapositive, (ii) $\implies$ (iii).
\end{proofbox}

\begin{remark}
	This gives a sharp criterion for invertibility of a linear system of $n$ equations with $n$ unknowns.
\end{remark}

\subsection{Determinant of linear maps}%
\label{sub:determinant_of_linear_maps}

\begin{lemma}
	Conjugate matrices have the same determinant.
\end{lemma}

\begin{proofbox}
	\[
		\det(P^{-1}AP) = \det P^{-1} \det A \det P = \det A \det (P^{-1}P) = \det A
	.\]
\end{proofbox}

\begin{definition}
	For $\alpha : V \to V$ linear, we define $\det \alpha = \det([\alpha]_{\mathcal{B}})$.\index{determinant of a linear map}
\end{definition}

\begin{theorem}
	$\det : \mathcal{L}(V, V) \to F$ satisfies
	\begin{enumerate}[\normalfont(i)]
		\item $\det \id = 1$;
		\item $\det(\alpha \circ \beta) = (\det \alpha) (\det \beta)$;
		\item $\det \alpha \neq 0$ if and only if $\alpha$ is invertible, and then $\det(\alpha^{-1}) = (\det \alpha)^{-1}$.
	\end{enumerate}
\end{theorem}

\begin{proofbox}
	Pick a basis $\mathcal{B}$ and express in terms of $[\alpha]_{\mathcal{B}}, [\beta]_{\mathcal{B}}$.
\end{proofbox}

\subsection{Determinant of Block Matrices}%
\label{sub:determinant_of_block_matrices}

\begin{lemma}
	For $A \in \mathcal{M}_{k}(F)$, $B \in \mathcal{M}_{\ell}(F)$, and $C \in \mathcal{M}_{k, \ell}(F)$, let
	\[
	M =
	\begin{pmatrix}
		A & C \\
		0 & B
	\end{pmatrix} \in \mathcal{M}_n(F)
	.\]
	Then, $\det M = (\det A)(\det B)$.
\end{lemma}

\begin{proofbox}
	We know that
	\[
		\det M = \sum_{\sigma \in S_n} \eps(\sigma) \prod_{i = 1}^{n} m_{\sigma(i)i}
	.\]
	Observe, that $m_{\sigma(i)i} = 0$ if $i \leq k$, and $\sigma(i) > k$. Hence,we only need to sum over $\sigma \in S_n$ such that
	\begin{enumerate}[(i)]
		\item For all $j \in [1, k]$, $\sigma(j) \ni [1, k]$;
		\item For all $j \in [k+1, n], \sigma(j) \in [k+1,n]$.
	\end{enumerate}
	In other words, we restrict $\sigma$ to $\sigma_1 : \{1, \ldots, k\} \to \{1, \ldots, k\}$ and  $\sigma_2 : \{k+1, \ldots, n\} \to \{k+1, \ldots, n\}$. Hence
	\[
		m_{\sigma(j)j} =
		\begin{cases}
			a_{\sigma_1(j)j} & j \leq k, \\
			b_{\sigma_2(j)(j)} & j \geq k.
		\end{cases}
	.\]
	We know $\eps(\sigma) = \eps(\sigma_1) \eps(\sigma_2)$. So
	\begin{align*}
		\det M &= \sum_{\sigma \ni S_n}\eps(\sigma) \prod_{i = 1}^{n} m_{\sigma(i)i} = \sum_{\substack{\sigma_1 \in S_k, \\ \sigma_2 \in S_{\ell}}}\eps(\sigma_1 \circ \sigma_2) \prod_{i = 1}^{k} a_{\sigma_1(i)i} \prod_{j = k+1}^{n} b_{\sigma_2(j)j} \\
		       &= \Biggl( \sum_{\sigma_1 \in S_k} \eps(\sigma_1) \prod_{i = 1}^{k} a_{\sigma_1(i)i} \Biggr) \Biggl( \sum_{\sigma_2 \in S_{\ell}} \eps(\sigma_2) \prod_{j = k+1}^{n} b_{\sigma_2(j)j} \Biggr) \\
		       &= (\det A)(\det B).
	\end{align*}
\end{proofbox}

\begin{corollary}
	If $A_1, \ldots, A_k$ are square matrices, then
	\[
		\det
	\begin{pmatrix}
		A_1 & \ast & \cdots & \ast \\
		0 & A_2 & \cdots & \ast \\
		 \vdots & \vdots& \ddots & \vdots \\
		 0 & 0 & \cdots & A_k
	\end{pmatrix}
	= (\det A_1) \cdots (\det A_k)
	.\]
\end{corollary}
This follows from induction on the number of matrices. In particular, if $A$ is upper-triangular with $\lambda_i$ on the diagonals, then $\det A = \prod \lambda_i$.

However, note that in general,
\[
\det
\begin{pmatrix}
	A & B \\
	C & D
\end{pmatrix}
\neq \det A \det D - \det B \det C
.\]

\begin{remark}
	In 3 dimensions, $(a, b, c) \mapsto (a \times b) \cdot c$ is a volume form. Thus we can show $\det(a, b, c) = (a \times b) \cdot c$.
\end{remark}

\subsection{Adjugate Matrix}%
\label{sub:adjugate_matrix}

Observe by swapping two columns in $A = (A^{(1)}, \ldots, A^{(n)})$, the determinant alternates parity. Using the fact that $\det A = \det (A^{T})$, we can see that swapping two rows also changes the parity of the determinant.

It is hard to compute the determinant using our current definitions. Using column expansion, we can reduce the computation of $n \times n$ determinants to $(n-1) \times (n-1)$ determinants.

\begin{definition}
	Let $A \in \mathcal{M}_n(F)$. Pick $i, j \in \{1, \ldots, n\}$. We define $A_{\widehat{ij}} \in \mathcal{M}_{n-1}(F)$, obtained by removing the $i$'th row and $j$'th column from $A$.
\end{definition}

\begin{lemma}
	Let $A \in \mathcal{M}_n(F)$.
	\begin{enumerate}[\normalfont(i)]
		\item Pick $1 \leq j \leq n$, then:
			\[
				\det A = \sum_{i = 1}^{n}(-1)^{i+j} a_{ij} \det A_{\widehat{ij}}
			.\]
		\item Pick $1 \leq i \leq n$, then:
			\[
				\det A = \sum_{j = 1}^{n} (-1)^{i+j} a_{ij} \det A_{\widehat{ij}}
			.\]
	\end{enumerate}
\end{lemma}

\begin{proofbox}
	We prove expansion with respect to the $j$'th column. Then row expansion will follow by taking the transpose. First, we can write $A = (A^{(1)}, \ldots, A^{(j)}, \ldots, A^{(n)})$. Then,
	\[
		A^{(j)} = \sum_{i = 1}^{n} a_{ij}e_i
	.\]
	Hence we get
	\[
		\det \Biggl( A^{(1)}, \ldots, \sum_{i = 1}^{n} a_{ij}e_i, \ldots, A^{(n)} \Biggr) = \sum_{i= 1}^{n} a_{ij} \det (A^{(1)}, \ldots, e_i, \ldots, A^{(n)})
	.\]
	Now, we can compute:

	\begin{align*}
		\det(A^{(1)}, &\ldots, e_i, \ldots, A^{(n)}) = \det
		\begin{pmatrix}
			& & 0 & & \\
			& & \vdots & & \\
			A^{(1)} & \cdots & 1 & \cdots & A^{(n)} \\
			& & \vdots & & \\
			& & 0 & &
		\end{pmatrix}
		\\
							    &= (-1)^{j-1} \det
							    \begin{pmatrix}
								    0 & & & & & & \\
								    \vdots & & & & & & \\
								    1 & A^{(1)} & \cdots & A^{(j-1)} & A^{(j+1)} & \cdots & A^{(n)} \\
								    \vdots & & & & & & \\
								    0 & & & & & &
							    \end{pmatrix} \\
							    &= (-1)^{i-1}(-1)^{j-1} \det
							    \begin{pmatrix}
								    1 & a_{i1} & \cdots & a_{ij-1} & a_{ij+1} & \cdots & a_{in} \\
								    0 && & & & & \\
								    \vdots & & & A_{\widehat{ij}} & & & \\
								    0 & & & & & &
							    \end{pmatrix}
							    \\
							    &= (-1)^{i+j}\det(A_{\widehat{ij}}).
	\end{align*}

	Combining these facts,
	\[
	\det A = \sum_{i = 1}^{n} a_{ij} \det(A^{(1)}, \ldots, a_i, \ldots, A^{(n)}) = \sum_{i = 1}^{n} a_{ij}(-1)^{i+j}\det_{\widehat{ij}}
	.\]
\end{proofbox}

\begin{definition}
	Let $A \in \mathcal{M}_n(F)$. The adjugate matrix $\adj A$ is the $n \times n$ matrix with $(i, j)$ entry given by $(-1)^{i+j}\det(A_{\widehat{ji}})$.\index{adjugate}
\end{definition}

\begin{theorem}
	Let $A \in \mathcal{M}_n(F)$. Then,
	\[
		(\adj A) A = (\det A)I_n
	.\]
	In particular, when $A$ is invertible,
	\[
	A^{-1} = \frac{1}{\det A} \adj A
	.\]
\end{theorem}

\begin{proofbox}
	From what we have just proven,
	\[
		\det A = \sum_{i = 1}^{n} (-1)^{i+j}(\det A_{\widehat{ij}})a_{ij} = \sum_{i = 1}^{n}(\adj A)_{ji} a_{ij} = (\adj (A) A)_{jj}
	.\]
	Now for $j \neq k$, we have
	\begin{align*}
		0 &= \det(A^{(1)}, \ldots, A^{(k)}, \ldots, A^{(k)}, \ldots, A^{(n)}) \\
		  &= \det\Biggl(A^{(1)}, \ldots, \sum_{i = 1}^{n}a_{ik} e_i, \ldots, A^{(k)}, \ldots, A^{(n)} \Biggr) \\
		  &= \sum_{i = 1}^{n} a_{ik} \det (A^{(1)}, \ldots, e_i, \ldots, A^{(n)}) \\
		  &= \sum_{i = 1}^{n} (\adj A)_{ji} a_{ik} = ((\adj A)A)_{jk}.
	\end{align*}
\end{proofbox}

\begin{proposition}[Cramer's rule]\index{Cramer's rule}
	Let $A \in \mathcal{M}_n(F)$ be invertible, and $b \in F^{n}$. Then the unique solution to $Ax = b$ is given by
	\[
		x_i = \frac{1}{\det A} \det(A_{\hat i b})
	,\]
	where $A_{\hat i b}$ is obtained by replacing the $i$'th column of $A$ by $b$.
\end{proposition}

Algorithmically, this avoids computing $A^{-1}$.

\begin{proofbox}
	If $A$ is invertible, then there exists unique $x \in F^{n}$ with $Ax = b$. Let $x$ be this solution, then
	\begin{align*}
		\det(A_{\hat i b}) &= \det(A^{(1)}, \ldots, A^{(i-1)}, b, A^{(i+1)}, \ldots, A^{(n)}) \\
				   &= \det (A^{(1)}, \ldots, A^{(i-1)}, Ax, A^{(i+1)}, \ldots, A^{(n)}) \\
				   &= \det \Biggl(A^{(1)}, \ldots, A^{(i-1)}, \sum_{j = 1}^{n} x_j A^{(j)}, A^{(i+1)}, \ldots, A^{(n)} \Biggr) \\
				   &= x_i \det (A^{(1)}, \ldots, A^{(i-1)}, A^{(i)}, A^{(i+1)}, \ldots, A^{(n)}) = x_i \det A.
	\end{align*}
	Inverting, this gives
	\[
	x_i = \frac{\det A_{\hat i b}}{\det A}
	.\]
\end{proofbox}

\newpage

\section{Eigenvectors and Eigenvalues}%
\label{sec:eigenvectors_and_eigenvalues}

Here, we set up towards our goal of the diagonalization of endomorphisms. Let $V$ be a vector space over $F$, and $\dim V = n < \infty$. Then recall $\alpha : V \to V$ linear is an endomorphism of $V$.

We want to find a basis $\mathcal{B}$ of $V$ such that in this basis,
\[
	[\alpha]_{\mathcal{B}} = [\alpha]_{\mathcal{B}, \mathcal{B}}
\]
has a ``nice'' form.

Recall that for another basis $\mathcal{B}'$ of $V$, the change of basis matrix satisfies
\[
	[\alpha]_{\mathcal{B}'} = P^{-1}[\alpha]_{\mathcal{B}}P
.\]
Equivalently, given a matrix $A \in \mathcal{M}_n(F)$, we want to find whether it is conjugate to a matrix with a ``simple'' form.

\begin{definition}
	\begin{enumerate}[(i)]
		\item[]
		\item $\alpha \in \mathcal{L}(V)$ is \textit{diagonalizable}\index{diagonalisable} if there exists a basis $\mathcal{B}$ of $V$ such that $[\alpha]_{\mathcal{B}}$ is diagonal.
		\item $\alpha \in \mathcal{L}(V)$ is \textit{triangulable}\index{triangulable} if there exists a basis $\mathcal{B}$ of $V$ such that $[\alpha]_{\mathcal{B}}$ is triangular:
			\[
				[\alpha]_{\mathcal{B}}=
				\begin{pmatrix}
					\lambda_1 & \ast & \cdots & \ast \\
					0 & \lambda_2 & \cdots & \ast \\
					\vdots & \vdots & \ddots & \vdots \\
					0 & 0 & \cdots & \lambda_n
				\end{pmatrix}
			\]
	\end{enumerate}
\end{definition}

\begin{remark}
	A matrix is diagonalizable (resp. triangulable) if and only if it is conjugate to a diagonal (resp. triangular) matrix.
\end{remark}

\begin{definition}
	\begin{enumerate}[(i)]
		\item[]
		\item $\lambda \in F$ is an \textit{eigenvalue}\index{eigenvalue} of $\alpha \in \mathcal{L}(V)$ if there exists $v \in V\setminus\{0\}$ such that $\alpha(v) = \lambda v$.
		\item $v \in V$ is an \textit{eigenvector}\index{eigenvector} of $\alpha \in \mathcal{L}(V)$ if and only if $v \neq 0$ and there exists $\lambda \in F$ such that $\alpha(v) = \lambda v$.
		\item $V_{\lambda} = \{v \in V \mid \alpha(v) = \lambda v\} \leq V$ is the \textit{eigenspace}\index{eigenspace} associated to $\lambda \in F$.
	\end{enumerate}
\end{definition}

\begin{lemma}
	Let $\alpha \in \mathcal{L}(V)$ and $\lambda \in F$. Then
	\[
		\lambda \text{ is an eigenvalue of } \alpha \iff \det(\alpha - \lambda \id) = 0
	.\]
\end{lemma}

\begin{proofbox}
	If $\lambda$ is an eigenvalue, then we have a chain of equalities
	\begin{align*}
		& \lambda \text{ eigenvalue} \\
		\iff& \exists v \in V\setminus\{0\}, \alpha(v) = \lambda v \\
			\iff& \exists v \in V \setminus\{0\}, (\alpha - \lambda \id)(v) = 0 \\
			\iff& \ker(\alpha - \lambda \id) \neq \{0\} \\
				\iff& r(\alpha - \lambda \id) < n \\
					\iff& \det(\alpha - \lambda \id) = 0.
	\end{align*}
\end{proofbox}

\begin{remark}
	If $\alpha(v_j) = \lambda v_j$, and $v_j \neq 0$, then we can complete to a basis of $V$ $(v_1, \ldots, v_j, \ldots, v_n)$, such that
	\[
		[\alpha]_{\mathcal{B}} (A^{(1)}, \ldots, e_j, \ldots, A^{(n)})
	.\]
\end{remark}

\subsection{Polynomials}%
\label{sub:polynomials}

We will look at how polynomials interact with $\alpha \in \mathcal{L}(V)$. First, if $F$ is a field, and
\[
	f(t) = a_n t^{n} + a_{n-1} t^{n-1} + \cdots + a_1 t + a_0
,\]
with $a_i \in F$, then $n$ is the largest exponent such that $a_n \neq 0$. We say $n = \deg f$. Then, we can easily show
\[
	\deg(f + g) \leq \max\{\deg f, \deg g\}, \quad \deg(fg) = \deg f + \deg g
.\]
Define $F[t]$ as the ring of polynomials with coefficients in $F$. Then $\lambda$ is a root of $f(t) \iff f(\lambda) = 0$.

\begin{lemma}
	If $\lambda$ is a root of $f$, then $t - \lambda$ divides $f$.
\end{lemma}

\begin{proofbox}
	Write $f(t) = a_nt^{n} + \cdots + a_1 t + a_0$, then $f(\lambda) = a_n \lambda^{n} + \cdots + a_1 \lambda + a_0 = 0$. Hence,
	\begin{align*}
		f(t) &= f(t) - f(\lambda) = a_n(t^{n} - \lambda^{n}) + \cdots + a_1(t - \lambda) \\
		     &= a_n(t - \lambda)(t^{n-1} + \cdots + \lambda^{n-1}) + \cdots + a_1(t - \lambda) \\
		     &= (t - \lambda)g(t).
	\end{align*}
\end{proofbox}

\begin{corollary}
	A non-zero polynomial of degree $n$ has at most $n$ roots.
\end{corollary}

This follows from induction of degree, and the above lemma.

\begin{corollary}
	If $f_1, f_2$ are polynomials of degree less than $n$, such that $f_1(t_i) = f_2(t_i)$ for at least $n$ values $(t_i)$, then $f_1 = f_2$.
\end{corollary}

This follows from the above corollary on $f_1 - f_2$.

\begin{theorem}
	Any $f \in \mathbb{C}[t]$ of positive degree has a complex root (hence exactly $\deg f$ roots when counted with multiplicity).
\end{theorem}

This will be proved in complex analysis.

\begin{definition}
	Let $\alpha \in \mathcal{L}(V)$. The \textit{characteristic polynomial}\index{characteristic polynomial} of $\alpha$ is
	\[
		\chi_{\alpha}(t) = \det(A - t \id)
	.\]
\end{definition}

\begin{remark}
	We can visualise
	\[
	A - t \id =
	\begin{pmatrix}
		a_{11} - t & a_{12} & \cdots & a_{1n} \\
		a_{21} & a_{22} - t & \cdots & a_{2n} \\
		\vdots & \vdots & \ddots & \vdots \\
		a_{n1} & a_{n2} & \cdots & a_{nn} - t
	\end{pmatrix}
	.\]
	The fact that $\det(A - t \id)$ is a polynomial of degree $n$ comes from the definition of $\det$.

	Moreover, notice that conjugate matrices have the same characteristic polynomial:
	\[
		\det(P^{-1}AP - \lambda \id) = \det(P^{-1}(A - \lambda \id)P) = \det(A - \lambda \id)
	.\]
	Hence $\chi_\alpha(t) = \det(A - \lambda \id)$ does not depend on the basis $\mathcal{B}$ in which we express $\alpha$.
\end{remark}

\begin{theorem} 
	$\alpha \in \mathcal{L}(V)$ is triangulable if and only if $\chi_\alpha$ can eb written as a product of linear factors over $F$:
	\[
		\chi_\alpha(t) = c \prod_{i = 1}^{n} (t - \lambda_i)
	.\]
	In particular, over $F = \mathbb{C}$, any matrix is triangulable.
\end{theorem}

\begin{proofbox}
	Suppose $\alpha$ is triangulable. Then in some basis, we have
	\[
		[\alpha]_{\mathcal{B}}=
		\begin{pmatrix}
			\lambda_1 & \ast & \cdots & \ast \\
			0 & \lambda_2 & \cdots & \ast \\
			\vdots & \vdots & \ddots & \vdots \\
			0 & 0 & \cdots & \lambda_n
		\end{pmatrix}
	.\]
	Then we can expand
	 \[
		 \chi_{\alpha}(t) =
			\begin{pmatrix}
				\lambda_1 - t & \ast & \cdots & \ast \\
				0 & \lambda_2 - t & \cdots & \ast \\
				\vdots & \vdots & \ddots & \vdots \\
				0 & 0 & \cdots & \lambda_n - t
			\end{pmatrix}
			= \prod_{i = 1}^{n}(\lambda_i - t)
	.\]
	For the backwards direction, we argue by induction on $n = \dim V$. If $n = 1$, then the conclusion is obvious. So suppose $n > 1$.

	By assumption let $\chi_{\alpha}(t)$ have a root $\lambda$. Then note $\chi_{\alpha}(\lambda) = 0 \iff \lambda$ is an eigenvalue of $\alpha$. Let $U=  V_{\lambda}$ be the associated eigenspace, and note $\{0\} \subsetneq U$.

		Let $(v_1, \ldots, v_k)$ be a basis of $U$, and complete to basis $(v_1, \ldots, v_k, v_{k+1}, \ldots, v_n)$ of $V$. Let $\spn(v_{k+1}, \ldots, v_n) = W$, then $V = U \oplus W$. In $\mathcal{B}$, we have
		\[
			[\alpha]_{\mathcal{B}} =
			\begin{pmatrix}
				\lambda I_k & \ast \\
				0 & C
			\end{pmatrix}
		.\]
		$\alpha$ induces an endomorphism
		\[
		\bar \alpha : V / U \to V/U
		.\]
		Then $C = [\bar \alpha]_{\bar{\mathcal{B}}}$, where $\bar{\mathcal{B}} = (v_{k+1} + U, \ldots, v_n + U)$. Then, as this is a block product,
		\begin{align*}
			\det(\alpha - t \id) &= \det
			\begin{pmatrix}
				(\lambda - t)\id & \ast \\
				0 & C - t \id
			\end{pmatrix}
			\\
					     &= (\lambda-t)^{k}\det(C - t\id) = c \prod_{i= 1}^{n} (t - \lambda_i).
		\end{align*}
		From uniqueness of factorisation, we can determine
		\[
			\det(C - t \id) = \tilde c \prod_{i = k+1}^{n} (t - \tilde \lambda_i)
		.\]
		Hence, by induction (as $\dim V/U < \dim V$), there is a basis $\check{\mathcal{B}} = (\check{v}_{k+1}, \ldots, \check v_{n})$ of $W$ where $[\mathcal{C}]_{\check{\mathcal{B}}}$ is triangular.

		Hence letting $\hat{\mathcal{B}} = (v_1, \ldots, v_k, \check v_{k+1}, \ldots, \check v_n)$, $[\alpha]_{\hat{\mathcal{B}}}$ is triangular.
\end{proofbox}

\begin{lemma}
	If $V$ is $n$-dimensional over $F = \mathbb{R}, \mathbb{C}$, and $\alpha \in \mathcal{L}(V)$, then if $\chi_{\alpha}(t) = (-1)^{n}t^{n} + c_{n-1}t^{n-1} + \cdots + c_0$, we have
	\[
		c_0 = \det A = \det \alpha, \quad c_{n-1} = (-1)^{n-1} \tr A
	.\]
\end{lemma}

\begin{proofbox}
	We known $\chi_{\alpha}(t) = \det(a - t \id)$, so $\chi_{\alpha}(0) = \det \alpha = c_0$.

	Say that $F = \mathbb{C}$. We known $\alpha$ is triangulable over $\mathbb{C}$, so
	\begin{align*}
		\chi_{\alpha}(t) &= \det
		\begin{pmatrix}
			a_1 - t & \cdots & \ast \\
			\vdots & \ddots & \vdots \\
			0 & \cdots & a_n - t
		\end{pmatrix} = \prod_{i = 1}^{n}(a_i - t) \\
				 &= (-1)^{n}t^{n} + c_{n-1}t^{n-1} + \cdots + c_0, \\
		c_{n-1} &= (-1)^{n-1} \sum_{i = 1}^{n} a_i = (-1)^{n-1} \tr \alpha.
	\end{align*}
\end{proofbox}

\subsection{Diagonalization Matrix}%
\label{sub:diagonalization_matrix}

\begin{definition}
	Pick $p(t)$, a polynomial over $F$, with $p(t) = a_n t^{n} + \cdots + a_1 t + a_0$ Hence if $A \in \mathcal{M}_n(F)$, then $A^{m} \in \mathcal{M}_n(F)$, and we define
	\[
		p(A) = a_n A^{n} + \cdots + a_1 A + a_n \id \in \mathcal{M}_n(F)
	.\]
	Similarly, for $\alpha \in \mathcal{L}(V)$, we can define
	\[
		p(\alpha) = a_n \alpha^{n} + \cdots + a_1 \alpha + a_n \id
	.\]
\end{definition}

\begin{theorem}
	Let $V$ be a vector space over $F$, with $\dim V < \infty$, and $\alpha \in \mathcal{L}(V)$.

	Then $\alpha$ is diagonalizable if and only if there exists a polynomial which is a product of distinct linear factors such that $p(\alpha) = 0$.
\end{theorem}

In other words $\alpha$ is diagonalizable if and only if there exist distinct $(\lambda_1, \ldots, \lambda_k)$, with $\lambda_j \in F$, such that
\[
	p(\alpha) = (\alpha - \lambda_1 \id) \cdots (\alpha - \lambda_k \id) = 0
.\]

\begin{proofbox}
	Suppose $\alpha$ is diagonalizable with $\lambda_1, \ldots, \lambda_k$ the distinct polynomial. Let $p(t) = \prod (t - \lambda_i)$, and let $\mathcal{B}$ be a basis of $V$ made of eigenvectors of $\alpha$. Then for $v \in \mathcal{B}$, we have $\alpha(v) = \lambda_i v$ for some $i \in \{1, \ldots, k\}$, so
	\[
		(\alpha - \lambda_i \id)(v) = 0
	.\]
	Hence
	\[
		p(\alpha) = \Biggl[ \prod_{j = 1}^{k} (\alpha - \lambda_j \id) \Biggr] (v) = 0
	\]
	for all $v \in \mathcal{B}$. But since $\mathcal{B}$ is a basis, then by linearity, for all $v \in F$, $p(\alpha)(v) = 0$, so $p(\alpha) = 0$.

	Now suppose $p(\alpha) = 0$ for 
	\[
		p(t) = \prod_{i = 1}^{k}(t - \lambda_i)
	,\]
	where $\lambda_i \neq \lambda_j$. Let $V_{\lambda_i} = \Ker(\alpha - \lambda_i \id)$. Then we claim
	\[
	V = \bigoplus_{i = 1}^{k} V_{\lambda_i}
	.\]
	Indeed, let
	\[
		q_j(t) = \prod_{\substack{i = 1 \\i \neq j}}^{k} \biggl( \frac{t - \lambda_i}{\lambda_j- \lambda_i} \biggr)
	.\]
	Then we have
	\[
		q_j(\lambda_i) =
		\begin{cases}
			1 & i = j, \\
			0 & i \neq j.
		\end{cases}
	\]
	Consider the polynomial
	\[
		q(t) = \sum_{j = 1}^{k} q_j(t)
	.\]
	Then $\deg q_j \leq j-1$, so $\deg q \leq k-1$. On the other hand, $q(\lambda_i) = 1$ for all $i$. Hence the polynomial $[q(t) - 1]$ degree less than or equal to $k-1$, and has at least $k$ roots, so for all $t$, $q(t) = 1$. Thus, we have
	\[
		q_1(t) + \cdots + q_k(t) = 1
	.\]
	Define the projector\index{projector}
	\[
		\pi_j = q_j(\alpha) \in \mathcal{L}(V)
	.\]
	Then we have
	\begin{align*}
		\sum_{j = 1}^{k} \pi_j = \sum_{j = 1}^{k} q_j(\alpha) = \Biggl( \sum_{j = 1}^{k} q_j \Biggr) (\alpha) = \id
	\end{align*}
	This means for any vector $v \in V$,
	\[
		v = q(\alpha)(v) = \sum_{j = 1}^{k} \pi_j(v) = \sum_{j = 1}^{k} q_j(\alpha)(v)
	.\]
	Now if we pick $j \in \{1, \ldots, k\}$, then
	 \[
		 (\alpha - \lambda_j \id) q_j(\alpha)(v) = \frac{1}{\prod_{i \neq j}(\lambda_j - \lambda_i)} p(\alpha)(v) = 0
	.\]
	Thus for all $j \in \{1, \ldots, k\}$, $(\alpha - \lambda_j \id)\pi_j(v) = 0$, so $\pi_j(v) \in V_{\lambda_j}$ for all $v$. Now for all $v \in V$,
	\[
		v = \sum_{j = 1}^{k} \pi_j(v) \implies V = \sum_{j = 1}^{k}V_{\lambda_j}.
	\]
	Now we prove the sum is direct. Indeed, let $v \in V_{\lambda_j} \cap (\sum_{i \neq j} V_{\lambda_i})$. Then since $v \in V_{\lambda_j}$,
	\[
		\pi_j(v) = q_j(\alpha)(v) = \prod_{i \neq j} \frac{\alpha - \lambda_i \id}{\lambda_i - \lambda_j} (v) = \prod_{i \neq j} \frac{(\lambda_j - \lambda_i)v}{\lambda_j - \lambda_i} = v
	.\]
	Now if $v \in \sum_{i \neq j} V_{\lambda_i}$, then note for $v \in V_{\lambda_i}$, then $\alpha(v) = \lambda_i v$ so
	\[
		\pi_j(\alpha) = q_j(\alpha)(v) = \prod_{i \neq j} \frac{\alpha - \lambda_i \id}{\lambda_j - \lambda_i}(v) = 0
	.\]
	Hence if $v \in V_{\lambda_j}\cap (\sum_{i \neq j} V_{\lambda_i})$, then $v = 0$. Hence $V$ is a direct sum of eigenspaces, meaning it can be diagonalized.
\end{proofbox}

\begin{remark}
	We have actually proved the following: If $\lambda_1, \ldots, \lambda_k$ are distinct eigenvalues of $\alpha$ then
	\[
	\sum_{i = 1}^{k}V_{\lambda_i} = \bigoplus_{i = 1}^{k} V_{\lambda_i}
	.\]
	This means that the only way digitalization fails is if the sum of the eigenspaces is a proper subspace of $V$.
\end{remark}

\begin{exbox}
	For $A \in \mathcal{M}_n(F)$, with $A$ having finite order $m$, then $A$ is diagonalizable, as $A$ is a root of
	\[
		t^{m}- 1 = \prod_{j = 1}^{m}(t - \zeta_m^{j})
	.\]
\end{exbox}

\begin{theorem}
	For $\dim V < +\infty$ and $\alpha, \beta \in \mathcal{L}(V)$ diagonalizable, then $\alpha, \beta$ are simultaneously diagonalizable if and only if $\alpha$ and $\beta$ commute.
\end{theorem}

\begin{proofbox}
	First, if $\alpha, \beta$ are simultaneously diagonalizable, then there is a basis of $V$ in which
	\[
		[\alpha]_{\mathcal{B}} = D_1, \quad [\beta]_{\mathcal{B}} = D_2
	.\]
	Since $D_1$ and $D_2$ diagonal, $D_1D_2 = D_2D_1$, so $\alpha \beta = \beta \alpha$.

	Now suppose $\alpha, \beta$ are both diagonalizable and $\alpha \beta = \beta \alpha$. Let $\lambda_1, \ldots, \lambda_k$ be the $k$ distinct eigenvalues of $\alpha$. Then we can write
	\[
	V = \bigoplus_{i = 1}^{k} V_{\lambda_i}
	\]
	where $V_{\lambda_i}$ is the eigenspace associated to $\lambda_i$. We claim that $V_{\lambda_i}$ is stable by $\beta$. Indeed, if $v \in V_{\lambda_i}$, then
	\[
		\alpha (\beta(v)) = \beta (\alpha(v)) = \beta (\lambda_i v) = \lambda_i \beta(v)
	.\]
	Hence $\beta(v) \in V_{\lambda_i}$. Now we use the criterion for diagonalizability: if $\beta$ is diagonalizable, then there exists a polynomial with distinct linear factors such that $p(\beta) = 0$.

	Since $\beta|_{V_{\lambda_j}}$ is an endomorphism and $p(\beta|_{V_{\lambda_j}}) = 0$, $B|_{V_{\lambda_j}}$ is diagonalizable. Let $\mathcal{B}_j$ be a basis for which $\beta|_{V_{\lambda_j}}$ is diagonal.

	Then, since $V$ is the sum of $V_{\lambda_j}$, $(\mathcal{B}_1 , \ldots, \mathcal{B}_k) = \mathcal{B}$ is a basis of $V$ in which both $\alpha$ and $\beta$ are in diagonal form.
\end{proofbox}

\subsection{Minimal Polynomials}%
\label{sub:minimal_polynomials}

\begin{proposition}[Euclidean Algorithm for Polynomials]
	Let $a, b$ be polynomials over $F$, with $b \neq 0$. Then there exist polynomials $q, r$ over $F$ with $\deg r < \deg b$ and $a = qb + r$.
\end{proposition}

\begin{definition}
	Let $V$ be a finite-dimensional vector space over $F$, and let $\alpha \in \mathcal{L}(V)$. The \textit{minimal polynomial} $m_{\alpha}$ of $\alpha$ is the unique non-zero polynomial with smallest degree such that $m_{\alpha}(\alpha) = 0$.
\end{definition}

The existence and uniqueness of a minimal polynomial can be seen as such. If $\dim V = n$, then we known $\dim \mathcal{L}(V) = n^2$, so $(\id, \alpha, \ldots, \alpha^{n^2})$ cannot be free. Hence there is some combination
\[
a_{n^2}\alpha^{n^2} + \cdots + a_1 \alpha + a_0 = 0
.\]
Hence the existence of a minimal polynomial is shown.

Now suppose $p(\alpha) = 0$. We show that $m_{\alpha} \mid p$. Indeed, from the Euclidean algorithm, we can find $q, r$ such that $p = m_{\alpha}q + r$, with $\deg r < \deg m_{\alpha}$. Since $p(\alpha) = m_{\alpha}(\alpha) = 0$, we must have $r(\alpha) = 0$.

Hence by minimality of $m_{\alpha}$, $r = 0$, and $m_{\alpha} \mid p$. But this implies uniqueness, as if $m_1, m_2$ are both polynomials of smallest degree, then $m_1 \mid m_2$ and $m_2 \mid m_1$, so they are equal up to a constant factor.

\begin{exbox}
	If $V = \mathbb{R}^2$, take
	\[
	A =
	\begin{pmatrix}
		1 & 0 \\
		0 & 1
	\end{pmatrix}
	, \quad B =
	\begin{pmatrix}
		1 & 1 \\
		0 & 1
	\end{pmatrix}
	.\]
	Let $p(t) = (t - 1)^2$, then $p(A) = p(B) = 0$. So their minimal polynomial is only $t - 1$ or $(t - 1)^2$. From this, we can check $m_{A} = t - 1$and $m_{B} = (t - 1)^2$. In particular, $B$ is not diagonalizable.
\end{exbox}

\subsection{Cayley-Hamilton Theorem}%
\label{sub:cayley_hamilton_theorem}

\begin{theorem}[Cayley-Hamilton Theorem]\index{Cayley-Hamilton theorem}
	Let $V$ be a finite dimensional $F$ vector space, and $\alpha \in \mathcal{L}(V)$ with characteristic polynomial $\chi_{\alpha}(t) = \det(\alpha - t \id)$. Then
	\[
		\chi_{\alpha}(\alpha) = 0
	.\]
\end{theorem}

As a corollary, $m_{\alpha} \mid \chi_{\alpha}$.

\begin{proofbox}
	We solve over $F = \mathbb{C}$. Take a basis $\mathcal{B} = \{v_1, \ldots, v_n\}$ for which $[\alpha]_{\mathcal{B}}$ is triangular, i.e.
	\[
		[\alpha]_{\mathcal{B}} = 
		\begin{pmatrix}
			a_1 &  & \ast \\
			    & \ddots & \\
			0 & & a_n
		\end{pmatrix}
	,\]
	and let $U_j = \langle v_1, \ldots, v_j\rangle$. Then, $(\alpha - a_j \id)U_j \leq U_{j-1}$, due to the triangular form. Now we known $\chi_{\alpha}(t) = \prod (a_i - t)$, so
	\begin{align*}
		&(\alpha - a_1 \id) \cdots (\alpha - a_{n-1} \id) (\alpha - a_n \id) V \\
		\leq& (\alpha - a_1 \id) \cdots (\alpha - a_{n-1} \id) U_{n-1} \\
		    & \,\vdots \\
		\leq& (\alpha - a_1 \id) U_1 \\
		=& 0.
	\end{align*}
	Hence $\chi_{\alpha}(\alpha) = 0$.
\end{proofbox}

\begin{definition}[Multiplicity]
	For a finite-dimensional vector space $V$ and $\alpha \in \mathcal{L}(V)$, let $\lambda$ be an eigenvalue of $\lambda$. Then
	\[
		\chi_{\alpha}(t) = (t - \lambda)^{a_{\lambda}}q(t)
	,\]
	where $a_{\lambda}$ is the \textit{algebraic multiplicity}\index{algebraic multiplicity} of $\lambda$, and the \textit{geometric multiplicity}\index{geometric multiplicity} of $\lambda$ is $\dim \Ker (\alpha - \lambda \id)$.
\end{definition}

\begin{remark}
	If $\lambda$ is an eigenvalue, then $\alpha - \lambda \id$ is singular, so $\det(\alpha - \lambda \id) = \chi_{\alpha}(\lambda) = 0$.
\end{remark}

\begin{lemma}
	For an eigenvalue $\lambda$ of $\alpha \in \mathcal{L}(V)$, then $1 \leq g_{\lambda} \leq a_{\lambda}$.
\end{lemma}

\begin{proofbox}
	Immediately, $g_{\lambda} = \dim \Ker(\alpha - \lambda \id) \geq 1$, as $\alpha - \lambda \id$ is singular. So we show $g_{\lambda} \leq a_{\lambda}$.

	Indeed, let $(v_1, \ldots, v_{g_{\lambda}})$ be a basis of $V_{\lambda} = \Ker(\alpha - \lambda \id)$, and complete to a basis $\mathcal{B} = (v_1, \ldots, v_{g_{\lambda}}, v_{g_{\lambda} + 1}, \ldots, v_n)$ of $V$. Then,
	\[
		[\alpha]_{\mathcal{B}} =
		\begin{pmatrix}
			\lambda \id_{g_{\lambda}} & \ast \\
			0 & A_1
		\end{pmatrix}
	,\]
	\[
		\implies \det[\alpha - t \id] = \det
		\begin{pmatrix}
			(\lambda - t) \id_{g_{\lambda}} & \ast \\
			0 & A_1 - t \id
		\end{pmatrix}
		= (\lambda - t)^{g_t} \chi_{A_1}(t)
	.\]
	Hence $g_{\lambda} \leq a_{\lambda}$.
\end{proofbox}

\begin{lemma}
	For $\lambda$ an eigenvalue of $\alpha \in \mathcal{L}(V)$, let $c_{\lambda}$ be the multiplicity of $\lambda$ as a root of $m_{\alpha}$. Then $1 \leq c_{\lambda} \leq a_{\lambda}$.
\end{lemma}

\begin{proofbox}
	From Cayley-Hamilton, $m_{\alpha} \mid \chi_{\alpha}$, immediately giving $c_{\lambda} \leq a_{\lambda}$. Now note $c_{\lambda} \geq 1$, as there exists a non-zero eigenvector $v$ of $\lambda$. Hence,
	\[
		m_{\alpha}(\alpha)(v) = (m_{\alpha}(\lambda))v = 0
	,\]
	so $m_{\alpha}(\lambda) = 0$, and $c_{\lambda} \geq 1$.
\end{proofbox}

\begin{exbox}
	Take the matrix
	\[
	A =
	\begin{pmatrix}
		1 & 0 & -2 \\
		0 & 1 & 1 \\
		0 & 0 & 2
	\end{pmatrix}
	.\]
	Since $A$ is triangular $\chi_{A}(t) = (t - 1)^2(t - 2)$. Hence $m_{A}$ is either $(t - 1)^2(t - 2)$ or $(t - 1)(t - 2)$. We can check that $(A - I)(A - 2I) = 0$, so $m_{A} = (t - 1)(t - 2)$, and $A$ is diagonalizable.
\end{exbox}

\begin{exbox}
	Take the Jordan block
	\[
	J_{\lambda} =
	\begin{pmatrix}
		 \lambda & 1 & \cdots & 0 \\
		 0 & \lambda & \ddots & 0 \\
		 \vdots & \vdots & \ddots & \vdots \\
		 0 & 0 & \cdots & \lambda
	\end{pmatrix}
	\]
	Then $g_{\lambda} = 1$, but $a_{\lambda} = n$.
\end{exbox}

\begin{lemma}
	Take $F = \mathbb{C}$, and $\dim V = n$. For $\alpha \in \mathcal{L}(V)$, the following are equivalent:
	\begin{enumerate}[\normalfont(i)]
		\item $\alpha$ is diagonalizable;
		\item For all $\lambda$ eigenvalues of $\alpha$, $a_{\lambda} = g_{\lambda}$;
		\item For all $\lambda$ eigenvalues of $\alpha$, $c_{\lambda} = 1$.
	\end{enumerate}
\end{lemma}

\begin{proofbox}
	We have shown (i) $\iff$ (iii), so we show (i) $\iff$ (ii).

		Indeed, let $(\lambda_1, \ldots, \lambda_k)$ be the distinct eigenvalues of $\alpha$. We showed that $\alpha$ is diagonalizable if and only if $V = \bigoplus V_{\lambda_i}$. However, we can compute the dimensions of both sides:
		\[
		\dim V = n = \deg \chi_{\alpha} = \sum_{i = 1}^{k} a_{\lambda_i}
		,\]
		\[
		\dim \bigoplus_{i = 1}^{k} V_{\lambda_i} = \sum_{i = 1}^{k} g_{\lambda_i}
		.\]
		Since we know the sum is always direct, we know $\alpha$ is diagonalizable if and only if
		\[
		\sum_{i = 1}^{k}a_{\lambda_i} = \sum_{i = 1}^{k} g_{\lambda_i}
		.\]
		But we know that $g_{\lambda_i} \leq a_{\lambda_i}$, so for this to hold, we must have $a_{\lambda_i} = g_{\lambda_i}$.
\end{proofbox}

\subsection{Jordan Normal Form}%
\label{sub:jordan_normal_form}

For this section, we will take $F = \mathbb{C}$.

\begin{definition}
	Let $A \in \mathcal{M}_n(\mathbb{C})$. We say that $A$ is in \textit{Jordan Normal Form}\index{Jordan normal form} if it is a block diagonal matrix:
	\[
	A =
	\begin{pmatrix}
		J_{n_1}(\lambda_1) & 0 & \cdots & 0 \\
		0 & J_{n_2}(\lambda_2) & \cdots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \cdots & J_{n_k}(\lambda_k)
	\end{pmatrix}
	,\]
	where
	\begin{itemize}
		\item $k \geq 1$, $k$ an integer.
		\item $n_1, \ldots, n_k$ are integers, with
			\[
			\sum_{i = 1}^{k} n_i = n
			.\]
		\item $\lambda_i \in \mathbb{C}$, $1 \leq i \leq k$, \textbf{not necessarily distinct}.
		\item The Jordan block $J_{m}(\lambda)$ is an $m \times m$ matrix with $\lambda$ on the main diagonal and $1$'s on the subdiagonal above: that is,
			\[
				J_{m}(\lambda) =
			\begin{pmatrix}
				 \lambda & 1 & \cdots & 0 \\
				 0 & \lambda & \ddots & 0 \\
				 \vdots & \vdots & \ddots & \vdots \\
				 0 & 0 & \cdots & \lambda
			\end{pmatrix}
			.\] 
	\end{itemize}
\end{definition}

\begin{theorem}
	Every matrix $A \in \mathcal{M}_n(\mathbb{C})$ is similar to a matrix in Jordan Normal Form, and is unique up to ordering the Jordan blocks.
\end{theorem}

\begin{exbox}
	Consider the case $n = 2$. Then the possible Jordan normal forms are
	\[
	\begin{pmatrix}
		\lambda_1 & 0 \\
		0 & \lambda_2
	\end{pmatrix}
	, \quad
	\begin{pmatrix}
		\lambda & 0 \\
		0 & \lambda
	\end{pmatrix}
	, \quad
	\begin{pmatrix}
		\lambda & 1 \\
		0 & \lambda
	\end{pmatrix}
	.\]
	The first has minimal polynomial $m = (t - \lambda_1)(t - \lambda_2)$, the second has minimal polynomial $m = (t - \lambda)$, and the third has minimal polynomial $m = (t - \lambda)^2$.

	In the case $n = 3$, it gets a bit more complicated:
	\begin{align*}
		A &= \begin{pmatrix}
			\lambda_1 & 0 & 0 \\
			0 & \lambda_2 & 0 \\
			0 & 0 & \lambda_3
		\end{pmatrix},& m &= (t - \lambda_1)(t - \lambda_2)(t - \lambda_3), \\
			A &= \begin{pmatrix}
			\lambda_1 & 0 & 0 \\
			0 & \lambda_2 & 0 \\
			0 & 0 & \lambda_2
		\end{pmatrix},& m &= (t - \lambda_1)(t - \lambda_2), \\
		A &= \begin{pmatrix}
			\lambda_1 & 0 & 0 \\
			0 & \lambda_2 & 1 \\
			0 & 0 & \lambda_2
		\end{pmatrix},& m &= (t - \lambda_1)(t - \lambda_2)^2, \\
		A &= \begin{pmatrix}
			\lambda & 0 & 0 \\
			0 & \lambda & 0 \\
			0 & 0 & \lambda
		\end{pmatrix},& m &= (t - \lambda), \\
		A &= \begin{pmatrix}
			\lambda & 0 & 0 \\
			0 & \lambda & 1 \\
			0 & 0 & \lambda
		\end{pmatrix},& m &= (t - \lambda)^2, \\
		A &= \begin{pmatrix}
			\lambda & 1 & 0 \\
			0 & \lambda & 1 \\
			0 & 0 & \lambda
		\end{pmatrix},& m &= (t - \lambda)^3.
	\end{align*}
\end{exbox}

Note that given the Jordan normal form, we can quickly compute $a_{\lambda}$, $g_{\lambda}$ and $c_{\lambda}$. Indeed, let $m \geq 2$ and consider $J_m(\lambda)$. Then,
\begin{align*}
	J_m - \lambda \id &= 
	\begin{pmatrix}
		0 & 1 & \cdots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \cdots & 1 \\
		0 & 0 & \cdots & 0
	\end{pmatrix}, \\
	(J_m - \lambda \id)^2 &=
	\begin{pmatrix}
		0 & 0 & 1 & \cdots & 0 \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		0 & 0 & 0 & \cdots & 1 \\
		0 & 0 & 0 & \cdots & 0 \\
		0 & 0 & 0 & \cdots & 0
	\end{pmatrix}, \\
	(J_m - \lambda \id)^{k} &=
	\begin{pmatrix}
		0 & I_{m-k} \\
		0 & 0
	\end{pmatrix}.
\end{align*}
Hence for $k = m$, we get $(J_m - \lambda \id)^{m} = 0$. So $(J_m - \lambda \id)$ is \textit{nilpotent} of order $m$\index{nilpotent} (this means $u^{m} = 0$ but $u^{m-1} \neq 0$).

In Jordan normal form, we can compute:
\begin{itemize}
	\item $a_{\lambda}$, as this is simply the sum of the sizes of blocks with eigenvalue $\lambda$, which equals the number of $\lambda$'s on the diagonal.
	\item $g_{\lambda} = \dim \Ker(A - \lambda \id)$, which is the number of blocks with eigenvalue $\lambda$, as only the first entry of each Jordan block corresponds to an eigenvalue of $\lambda$.
	\item $c_{\lambda}$, as $(J_m - \lambda \id)$ is nilpotent of order $m$, so $c_{\lambda}$ will be the size of the largest block with eigenvalue $\lambda$.
\end{itemize}

\begin{exbox}
	Take the matrix
	\[
	A =
	\begin{pmatrix}
		0 & -1 \\
		1 & 2
	\end{pmatrix}
	.\]
	Suppose we want to find its Jordan normal form. Then,
	\begin{enumerate}[(i)]
		\item We calculate the characteristic polynomial $\chi_{A}(t) = (t - 1)^2$, so the only eigenvalue is $\lambda = 1$. Since $A - \id \neq 0$, we known $m_{A}(t) = (t - 1)^2$, and so the Jordan normal form will be a Jordan block of size $2$.
		\item We find the eigenvectors:
			\[
			A - \id =
			\begin{pmatrix}
				-1 & -1 \\
				-1 & -1
			\end{pmatrix}
			\]
			So the kernel of $A - \id$ is spanned by one vector, $v_1 = (1, -1)^{T}$. We also look for a (non-unique) $v_2$ such that $(A - \id)v_2 = v_1$, and we can find $v_2 = (-1, 0)^{T}$ works.
	\end{enumerate}
	Hence, in the basis $\mathcal{B} = (v_1, v_2)$,
	\[
	A_{\mathcal{B}} =
	\begin{pmatrix}
		1 & 1 \\
		0 & 1
	\end{pmatrix}
	= \id +
	\begin{pmatrix}
		0 & 1 \\ 
		0 & 0
	\end{pmatrix}
	.\]
	Hence, we can write
	\[
	A =
	\begin{pmatrix}
		1 & -1 \\
		-1 & 0
	\end{pmatrix}
	\begin{pmatrix}
		1 & 1 \\
		0 & 1
	\end{pmatrix}
	\begin{pmatrix}
		1 & -1 \\
		-1 & 0
	\end{pmatrix}^{-1}
	.\]
\end{exbox}

\begin{theorem}
	Let $V$ be a finite dimensional $\mathbb{C}$-vector space, and $\alpha \in \mathcal{L}(V)$. Then write
	\[
		m_{\alpha}(t) = (t - \lambda_1)^{c_1} \cdots (t - \lambda_k)^{c_k}
	.\]
	We can then write
	\[
	V = \bigoplus_{j = 1}^{k}V_j
	,\]
	where $V_j$ is not the eigenspace of $\lambda_j$, but the \textit{generalized eigenspace}\index{generalized eigenspace} $V_j = \Ker[(\alpha - \lambda_j \id)^{c_j}]$.
\end{theorem}

\begin{remark}
	If $\alpha$ is diagonalizable, then $c_j = 1$, so this gives our previous criterion for diagonalizability.
\end{remark}

\begin{proofbox}
	Recall the projectors; we show they are explicit. Indeed, let
	\[
		p_j(t) = \prod_{i \neq j}(t - \lambda_i)^{c_i}
	.\]
	Then the $p_j$ have no common factor, so by Euclid's algorithm, we can find polynomials $q_1, \ldots, q_k$ such that
	\[
	\sum_{i = 1}^{k} p_i q_i = 1
	.\]
	Then define the projectors
	\[
		\pi_j = q_j p_j (\alpha)
	.\]
	Then by our lemma,
	\[
		\id = \sum_{j = 1}^{k} q_j p_j (\alpha) = \sum_{j = 1}^{k} \pi_j(\alpha)
	.\]
	Hence, for all $v \in V$, we get
	\[
		v = \sum_{j = 1}^{k} \pi_j(v)
	.\]
	Recall that $m_{\alpha}(\alpha) = 0$, where
	\[
		m_{\alpha} = \prod_{j = 1}^{k}(t - \lambda_j)^{c_{\lambda_j}}
	.\]
	Hence,
	\[
		0 = m_{\alpha}(\alpha) = (\alpha - \lambda_j \id)^{c_{\lambda_j}} \pi_j
	.\]
	So for all $v \in V$, $\pi_j(v) \in V_j$. So we get the sum
	\[
	V = \sum_{j = 1}^{k} V_j
	.\]
	Finally, we need to show that the sum is direct. Indeed, note $\pi_i \pi_j = 0$ if $i \neq j$, so $\pi_i= \pi_i ( \sum \pi_j) = \pi_i^2$. Hence, on $V_{i}$, $\pi_i$ is the identity.

	Now suppose there exists
	\[
	v = \sum_{i \neq j} v_j
	,\]
	and $v \in V_i$. Then applying $\pi_i$,
	\[
	v = \sum_{i \neq j} 0
	.\]
	Hence $v = 0$, and the sum is direct.
\end{proofbox}

\newpage

\section{Bilinear Forms}%
\label{sec:bilinear_forms}

\begin{definition}
	Let $U, V$ be vector spaces over $F$. Then
	\[
	\phi : U \times V \to F
	\]
	is a \textit{bilinear form}\index{bilinear form} if it is linear in both components.
\end{definition}

\begin{exbox}
	\begin{enumerate}[(i)]
		\item[]
		\item Take $V \times V^{\ast} \to F$ by $(v, \theta) \mapsto \theta(v)$.
		\item The scalar product\index{scalar product} on $U = V = \mathbb{R}^{n}$ is $\psi : \mathbb{R}^{n} \times \mathbb{R}^{n} \to \mathbb{R}$ by
			\[
				\left(
					\begin{pmatrix}
						x_1 \\
						\vdots \\
						x_n
					\end{pmatrix},
					\begin{pmatrix}
						y_1 \\
						\vdots \\
						y_n
					\end{pmatrix}
				\right)
				\mapsto  \sum_{i = 1}^{n} x_i y_i
			.\]
		\item If $U = V = \mathcal{C}([0, 1], \mathbb{R})$, then we can define
			\[
				\phi(f, g) = \int_{0}^{1}f(t)g(t) \diff t
			.\]
			This can be thought of as an infinite dimensional scalar product.
	\end{enumerate}
\end{exbox}

\begin{definition}
	Let $\mathcal{B} = (e_1, \ldots, e_m)$ be a basis of $U$, and $\mathcal{C} = (f_1, \ldots, f_n)$ be a basis of $V$. If $\phi : U \times V \to F$ is a bilinear form, then the matrix of $\phi$ with respect to $\mathcal{B}$ and $\mathcal{C}$\index{matrix of bilinear form} is
	\[
		[\phi]_{\mathcal{B}, \mathcal{C}} = (\phi(e_i, f_j))
	.\]
\end{definition}

\begin{lemma}
	\[
		\phi(u, v) = [u]_{\mathcal{B}}^{T} [\phi]_{\mathcal{B}, \mathcal{C}} [v]_{\mathcal{C}}
	.\]
\end{lemma}

\begin{proofbox}
	Let
\[
u = \sum_{i = 1}^{m} \lambda_i e_i, \quad v = \sum_{j = 1}^{n} \mu_ij f_j
.\]
Since $\phi$ is a bilinear form,
\begin{align*}
	\phi(u, v) &= \phi \Biggl( \sum_{i = 1}^{m} \lambda_i e_i, \sum_{j = 1}^{n} \mu_j e_j \Biggr) = \sum_{i = 1}^{m}\sum_{j = 1}^{n} \lambda_i \mu_j \phi(e_i, f_j) \\
		   &= [u]_{\mathcal{B}}^{T}[\phi]_{\mathcal{B}, \mathcal{C}}[v]_{\mathcal{C}}.
\end{align*}
\end{proofbox}

\begin{remark}
	$[\phi]_{\mathcal{B},\mathcal{C}}$ is the only matrix satisfying this property.
\end{remark}

\begin{definition}
	$\phi: U \times V \to F$ a bilinear form determines two linear maps:
	\begin{align*}
		\phi_L : U \to V^{\ast}& \\
		\phi_L(u) : V &\to F \\
			      v &\mapsto \phi(u, v)\\
			      \phi_R : V \to U^{\ast}& \\
			      \phi_R(v) : U &\to F \\
			      u &\mapsto \phi(u, v)
	\end{align*}
\end{definition}

\begin{lemma}
	Let $\mathcal{B} = (e_1, \ldots, e_m)$ a basis of $U$, and $\mathcal{B}^{\ast} = (\varepsilon_1, \ldots, \varepsilon_m)$ a dual basis of $U^{\ast}$, Similarly, let $\mathcal{C} = (f_1, \ldots, f_n)$ be a basis of $V$, and $\mathcal{C}^{\ast} (\eta_1, \ldots, \eta_n)$ a dual basis of $V^{\ast}$.

	Let $A = [\phi]_{\mathcal{B}, \mathcal{C}}$. Then,
	\begin{align*}
		[\phi_R]_{\mathcal{C}, \mathcal{B}^{\ast}} &= A, \\
		[\phi_L]_{\mathcal{B}, \mathcal{C}^{\ast}} &= A^{T}.
	\end{align*}
\end{lemma}

\begin{proofbox}
	We have $\phi_L(e_i, f_j) = \phi(e_i, f_j) = A_{ij}$, and so
	\[
		\phi_L(e_i) = \sum A_{ij} \eta_j
	.\]
	Similarly, $\phi_R(f_j)(e_i) = \phi(e_i, f_j) = A_{ij}$, so
	\[
		\phi_R(f_{j}) = \sum A_{ij} \varepsilon_i
	.\]
	This naturally gives our result.
\end{proofbox}

\begin{definition}
	Let $\Ker \phi_L$ be the \textit{left kernel}\index{left kernel} of $\phi$, and $\Ker \phi_R$ be the \textit{right kernel}\index{right kernel} of $\phi$.

	We say that $\phi$ is non-degenerate\index{non-degenerate bilinear form} if $\Ker \phi_L = \{0\}$ and $\Ker \phi_R = \{0\}$. Otherwise, we say that $\phi$ is degenerate.
\end{definition}

\begin{lemma}
	Let $U, V$ be finite dimensional, $\mathcal{B}, \mathcal{C}$ bases of $U$ and $V$, and $\phi: U \times V \to F$ a bilinear form. Let $A = [\phi]_{\mathcal{B}, \mathcal{C}}$.

	Then $\phi$ is non-degenerate if and only if $A$ is invertible.
\end{lemma}

\begin{corollary}
	If $\phi$ is non-degenerate, then $\dim U = \dim V$.
\end{corollary}

\begin{proofbox}
	$\phi$ is non-degenerate if and only if $\Ker \phi_L = \{0\}$ and $\Ker \phi_R = \{0\}$. But this implies $\nullity (A^{T}) = 0$ and $\nullity (A) = 0$, hence by rank-nullity theorem, we must have $\rank (A^{T}) = \dim U$, and $\rank (A) = \dim V$. But this gives $A$ invertible and $\dim U = \dim V$.
\end{proofbox}

\begin{remark}
	Taking $\phi : \mathbb{R}^{n} \times \mathbb{R}^{n} \to \mathbb{R}$ by the scalar product, then $\phi$ is non-degenerate, as in the standard basis $\mathcal{B}$,
	\[
		[\phi]_{\mathcal{B}, \mathcal{B}} = I_n
	.\]
\end{remark}

\begin{corollary}
	When $U$ and $V$ are finite dimensional, then choosing a non-degenerate bilinear form $\phi: U \times V \to F$ is equivalent to choosing an isomorphism $\phi_L : U \to V^{\ast}$.
\end{corollary}

\begin{definition}
	If $T \subset U$, we define
	\[
		T^{\perp} = \{v \in V \mid \phi(t, v) = 0 \, \forall t \in T\}
	.\]
	Similarly, if $S \subset V$, then
	\[
		^{\perp}S = \{u \in U \mid \phi(u, s) = 0 \, \forall s \in S\}
	.\]
\end{definition}

\begin{proposition}
	Let $\mathcal{B}, \mathcal{B}'$ be two bases of $U$, and $P = [\id]_{\mathcal{B}', \mathcal{B}}$, and $\mathcal{C}, \mathcal{C}'$ two bases of $V$, and $Q = [\id]_{\mathcal{C}', \mathcal{C}}$, then if $\phi: U \times V \to F$ is a bilinear form, then
	\[
		[\phi]_{\mathcal{B}', \mathcal{C}'} = P^{T} [\phi]_{\mathcal{B}, \mathcal{C}} Q
	.\]
\end{proposition}

\begin{proofbox}
	We have
\[
	\phi(u, v) = [u]_{\mathcal{B}}^{T} [\phi]_{\mathcal{B}, \mathcal{C}}[v]_{\mathcal{C}} = (P[u]_{\mathcal{B}'})^{T}[\phi]_{\mathcal{B},\mathcal{C}}(Q[v]_{\mathcal{C}'}) = [u]_{\mathcal{B}'}^{T} (P^{T} [\phi]_{\mathcal{B}, \mathcal{C}} Q)[v]_{\mathcal{C}'}
,\]
which implies $P^{T}[\phi]_{\mathcal{B}, \mathcal{C}}Q = [\phi]_{\mathcal{B}', \mathcal{C}'}$.
\end{proofbox}

\begin{definition}
	The rank of $\phi$ ($\rank \phi$) is the rank of any matrix representing $\phi$.\index{rank of bilinear form}
\end{definition}

This is true as $\rank (P^{T}AQ) = \rank A$, if $P$ and $Q$ are invertible.

Note we could have equivalently defined $\rank \phi = \rank \phi_L = \rank \phi_R$.

Recall a bilinear form is a linear mapping $\phi : V \times V \to F$, where $V$ is a finite-dimensional $F$-vector space. If $\mathcal{B}$ is a basis of $V$, then
\[
	[\phi]_{\mathcal{B}} = [\phi]_{\mathcal{B}\mathcal{B}} = (\phi(e_i, e_j))_{i,j}
.\]

\begin{lemma}
	If $\phi : V \times V \to F$ is bilinear, and $\mathcal{B}, \mathcal{B}'$ are two bases of $V$, then letting $P = [\id]_{\mathcal{B}',\mathcal{B}}$, we have
	\[
		[\phi]_{\mathcal{B}} = P^{T}[\phi]_{\mathcal{B}}P
	.\] 
\end{lemma}

Indeed, this is a special case of lemma 6.1.

\begin{definition}
	For $A, B \in \mathcal{M}_n(F)$, we say $A$ and $B$ are \textit{congruent}\index{congruent matrices} if and only if there exists an invertible matrix $P$ such that $A = P^{T}BP$.
\end{definition}
\begin{remark}
	This defines an equivalence relation.
\end{remark}

\begin{definition}
	A bilinear form $\phi$ on $V$ is \textit{symmetric}\index{symmetric form} if $\phi(u, v) = \phi(v, u)$, for all $u, v \in V$.
\end{definition}
\begin{remark}
	If $A \in \mathcal{M}_n(F)$, we say that $A$ is symmetric if and only if $A = A^{T}$, so $a_{ij} = a_{ji}$.

	Then $\phi$ is symmetric if and only if $[\phi]_{\mathcal{B}}$ is symmetric in any basis $\mathcal{B}$ of $V$.

	Note if we want to represent $\phi$ by a diagonal matrix, then $\phi$ must be symmetric.
\end{remark}

\begin{definition}
	A map $Q : V \to F$ is a \textit{quadratic form}\index{quadratic form} if there exists a bilinear form $\phi : V \times V \to F$ such that for all $u \in V$,
	\[
		Q(u) = \phi(u, u)
	.\]
\end{definition}
\begin{remark}
	Let $\mathcal{B} = (e_i)$, and $A = [\phi]_{\mathcal{B}}$.

	Then writing $u = \sum_{i = 1}^{n} x_i e_i$,
	\begin{align*}
		Q(u) &= \phi(u, u) = \phi \Biggl( \sum_{i = 1}^{n} x_i e_i, \sum_{i = 1}^{n} x_i e_i \Biggr) \\
		     &= \sum_{i = 1}^{n} \sum_{j = 1}^{n} x_i x_j \underbrace{\phi(e_i, e_j)}_{a_{ij}} \\
		     &= \sum_{i = 1}^{n} \sum_{j = 1}^{n} x_i x_j a_{ij} = X^{T}AX.
	\end{align*}

	So we have $Q(u) = X^{T}AX$. Moreover, note we can replace $A$ with $\frac{1}{2}(A + A^{T})$ and preserve $Q$:
	\begin{align*}
		X^{T}AX &= \sum_{i,j = 1}^{n} a_{ij} x_i x_j = \sum_{i,j = 1}^{n} a_{ji}x_i x_j = \frac{1}{2} \sum_{i,j = 1}^{n} (a_{ij} + a_{ji})x_i x_j \\
			&= \frac{1}{2}X^{T}(A + A^{T})X.
	\end{align*}
\end{remark}

\begin{proposition}
	If $Q : V \times V \to F$ is a quadratic form, then there exists a unique symmetric bilinear form $\phi : V \times V \to F$ such that for all $u \in V$, $Q(u) = \phi(u, u)$.
\end{proposition}

\begin{proofbox}
	Let $\psi$ be a bilinear form on $V$ such that $Q(u) = \psi(u, u)$, for all $u \in V$. Define
	 \[
		 \phi(u, v) = \frac{1}{2}(\psi(u, v) + \psi(v,u))
	.\]
	Then $\phi$ is symmetric, and $\phi(u, u) = \psi(u, u) = Q(u)$. This proves the existence of a symmetric $\phi$.

	Now let $\phi$ be a symmetric bilinear form such that $\phi(u, u) = Q(u)$. Then,
	\begin{align*}
		Q(u + v) &= \phi(u + v, u + v) = \phi(u, u) + \phi(u, v) + \phi(v, u) + \phi(v, v) \\
			 &= Q(u) + 2 \phi(u, v) + Q(v),
	\end{align*}
	\[
		\implies \phi(u, v) = \frac{1}{2} \bigl[Q(u+v) - Q(u) - Q(v)\bigr]
	.\]
	This is known as the \textit{polarization identity}\index{polarization identity}.
\end{proofbox}

\begin{theorem}
	Let $\phi : V \times V \to F$ be a symmetric bilinear form. Then there exists a basis $\mathcal{B}$ of $V$ such that $[\phi]_{\mathcal{B}}$ is diagonal.
\end{theorem}

\begin{proofbox}
	We will only look at the case when $\dim V$ is finite. Here, we will proceed by induction on the dimension $n$. Note $n = 1$ is trivial.

	Suppose that the theorem holds for all dimensions less than $n$. Let $\phi : V \times V \to F$ be a symmetric bilinear form. Then if $\phi(u, u) = 0$ for all $u \in V$, then $\phi$ is identically zero by the polarization identity.

	Hence we may pick $u \in V \setminus \{0\}$ such that $\phi(u, u) \neq 0$. Let $e_1 = u$. We define
	\[
		U = (\langle e_1 \rangle)^{\perp} = \{v \in V \mid \phi(e_1, v) = 0\} = \Ker \{ \phi(e_1: \cdot) : V \to F\}
	.\]
	By the rank-nullity theorem,
	\[
	\dim V = n = 1 + \dim U
	,\]
	as the rank of $\phi(e_1, \cdot)$ is exactly 1, since it is non-zero at $e_1$.

	We now prove that $U + \langle e_1 \rangle = U \oplus \langle e_1 \rangle$. Indeed, if $v \in \langle e_1 \rangle \cap U$, then $v = \lambda e_1$, and $\phi(e_1, v) = 0$. But then,
	\[
		0 = \phi(e_1, \lambda e_1) = \lambda \underbrace{\phi(e_1, e_1)}_{\neq 0}
	,\]
	so $\lambda = 0$, and $v = 0$. This gives $V = U \oplus \langle e_1 \rangle$.

	Now complete to a basis $\mathcal{B} = (e_1, \ldots, e_n)$ of $V$. Then,
	\[
		[\phi]_{\mathcal{B}} = 
		\begin{pmatrix}
			\phi(e_1, e_1) & 0 & \cdots & 0 \\
			0 & & & \\
			\vdots & & A' & \\
			0 & & &
		\end{pmatrix}
	,\]
	since $e_2, \ldots, e_n \in U$, so $\phi(e_1, e_2) = \cdots = \phi(e_1, e_n) = 0$. Now $A' = (\phi(e_i, e_j))_{2 \leq i, j \leq n}$, so $(A')^{T} = A'$.

	Consider $\phi|_{U} : U \times U \to F$, which is bilinear and symmetric and matrix $A'$ in $\mathcal{B}' = (e_2, \ldots, e_n)$. By the induction hypothesis, we can find $(e_2', \ldots, e_n') = \mathcal{B}''$, a basis of $U$ in which $[\phi|_{U}]_{\mathcal{B}''}$ is diagonal.

	Hence, we get $[\phi]_{(e_1, e_2', \ldots, e_n')}$ is diagonal.
\end{proofbox}

\begin{exbox}
	Take $V = \mathbb{R}^3$, and the quadratic form
	 \begin{align*}
		 Q(x_1, x_2, x_3) &= x_1^2 + x_2^2 + 2x_3^2 + 2x_1 x_2 + 2x_1 x_3 - 2x_2 x_3 \\
				  &= X^{T}AX, \\
		 A &=
		 \begin{pmatrix}
			 1 & 1 & 1 \\
			 1 & 1 & -1 \\
			 1 & -1 & 2
		 \end{pmatrix}.
	\end{align*}
	We have two ways that we can diagonalize this:
	\begin{itemize}
		\item Follow the above proof, use induction.
		\item Complete the square:
			\begin{align*}
				Q(x_1, x_2, x_3) &= x_1^2 + x_2^2 + 2x_3^2 + 2x_1x_2 + 2x_1x_3 - 2x_2 x_3 \\
						 &= (x_1 + x_2 + x_3)^2 + x_3^2 - 4x_2 x_3 \\
						 &= (\underbrace{x_1 + x_2 + x_3}_{x_1'})^2 + (\underbrace{x_3 - 2x_2}_{x_2'})^2 - (\underbrace{2x_2}_{x_3'})^2.
			\end{align*}
			Then if $P$ is the change of basis matrix from $(x_1', x_2', x_3')$ to $(x_1, x_2, x_3)$,
			\[
			P^{T}AP =
			\begin{pmatrix}
				1 & 0 & 0 \\
				0 & 1 & 0 \\
				0 & 0 & -1
			\end{pmatrix}
			.\]
			To find $P$, we have
			\[
				\begin{pmatrix}
					x_1' \\
					x_2' \\
					x_3'
				\end{pmatrix}
				=
				\begin{pmatrix}
					1 & 1 & 1 \\
					0 & -2 & 1 \\
					0 & -2 & 0
				\end{pmatrix}
				= P^{-1}
			.\]
	\end{itemize}
\end{exbox}

\subsection{Sylvester's Law}
\label{sub:sylvesters_law}

Recall the previous theorem:

\begin{theorem}
	If $\dim V < \infty$, and $\phi : V \times V \to F$ is a symmetric bilinear form, then there exists a basis $\mathcal{B}$ of $V$ in which $[\phi]_{\mathcal{B}}$ is diagonal.
\end{theorem}

We get the following corollary:

\begin{corollary}
	By taking $F = \mathbb{C}$, we get that if $\phi$ is a symmetric bilinear form on $V$, then there exists a basis $\mathcal{B}$ of $V$ such that
	\[
		[\phi]_{\mathcal{B}} =
		\begin{pmatrix}
			I_r & 0 \\
			0 & 0
		\end{pmatrix}
	,\]
	where $r = \rank(\phi)$.
\end{corollary}

\begin{proofbox}
	Pick a basis $\mathcal{E} = (e_1, \ldots, e_n)$ such that
	\[
		[\phi]_{\mathcal{E}} =
		\begin{pmatrix}
			a_1 & & 0 \\
			    & \ddots & \\
			0 & & a_n
		\end{pmatrix}
	.\]
	Then we can reorder the $a_i$ such that $a_i \neq 0$ for $1 \leq i \leq r$, and $a_i = 0$ for $i > r$.

	To finish, for $i \leq r$, let $\sqrt{a_i}$ be a choice of complex square root of $a_i$. Then, we can define
	\[
	v_i =
	\begin{cases}
		\frac{e_i}{\sqrt a_i} & 1 \leq i \leq r,\\
		e_i & i > r.
	\end{cases}
	\]
	Hence, letting $\mathcal{B} = (v_1, \ldots, v_r, e_{r+1}, \ldots, e_n)$, we get
	\[
		[\phi]_{\mathcal{B}} =
		\begin{pmatrix}
			I_r & 0 \\
			0 & 0
		\end{pmatrix}
	.\]
\end{proofbox}

\begin{corollary}
	Every symmetric matrix of $\mathcal{M}_n(\mathbb{C})$ is congruent to a unique matrix of the form
	\[
	\begin{pmatrix}
		I_r & 0 \\
		0 & 0
	\end{pmatrix}
	.\]
\end{corollary}

For $F = \mathbb{R}$, we would like to say the same thing, however over $\mathbb{R}$ we cannot take complex square roots. However, we get the following, weaker corollary.

\begin{corollary}
	Let $F = \mathbb{R}$. Then if $\dim V < \infty$, and $\phi$ is a symmetric bilinear form on $V$, then there exists a basis $\mathcal{B}$ of $V$ such that
	\[
		[\phi]_{\mathcal{B}} =
		\begin{pmatrix}
			I_p & 0 & 0 \\
			   0 & -I_q & 0 \\
			0 & 0 & 0
		\end{pmatrix}
	,\]
	where $p, q \geq 0$ and $p + q = \rank(\phi)$.
\end{corollary}

\begin{proofbox}
	Let $\mathcal{E} = (e_1, \ldots, e_n)$ be a basis of $V$ such that
	\[
		[\phi]_{\mathcal{E}} =
		\begin{pmatrix}
			a_1 & & 0 \\
			    & \ddots & \\
			0 & & a_n
		\end{pmatrix}
	,\]
	for $a_i \in \mathbb{R}$. Then we can reorder $a_i$ such that $a_i > 0$ for $1 \leq i \leq p$, $a_i < 0$ for $p+1 \leq i \leq p+q$, and $a_i = 0$ for $i \geq p+q$.

	We can define
	\[
	v_i =
	\begin{cases}
		\frac{e_i}{\sqrt a_i} & 1 \leq i \leq p, \\
		\frac{e_i}{\sqrt{|a_i|}} & p+1 \leq i \leq p+q, \\
		e_i & i > p+q.
	\end{cases}
	\]
	Then over $\mathcal{B} = (v_1, \ldots, v_n)$, $[\phi]_{\mathcal{B}}$ has the required form.
\end{proofbox}

\begin{definition}[Signature]
	For a symmetric bilinear form $\phi$ over a finite-dimensional vector space $V$, we define the \textit{signature}\index{signature} as
	\[
	s(\phi) = p - q
	.\]
	Similarly, we can define the signature of a quadratic form.
\end{definition}

For this definition to make sense, $p - q$ must not depend on the basis.

\begin{theorem}[Sylvester's Law of Inertia]\index{Sylvester's law}
	Let $F = \mathbb{R}$, and $\dim V < \infty$. If $\phi$ is a symmetric bilinear form on $V$, then if $\phi$ is represented by
	\begin{align*}
		[\phi]_{\mathcal{B}} &= 
		\begin{pmatrix}
			I_p & 0 & 0 \\
			   0 & -I_q & 0 \\
			0 & 0 & 0
		\end{pmatrix}
		, \\
		[\phi]_{\mathcal{B}'} &=
		\begin{pmatrix}
			I_{p'} & 0 & 0 \\
			      0 & -I_{q'} & 0 \\
			0 & 0 & 0
		\end{pmatrix},
	\end{align*}
	then $p = p'$ and $q = q'$.
\end{theorem}

\begin{definition}
	Let $\phi$ be a symmetric bilinear form on a real valued vector space $V$. We say that
	\begin{enumerate}[(i)]
		\item $\phi$ is \textit{positive definite}\index{positive definite} if for all $u \in V \setminus \{0\}$ $\phi(u, u) > 0$,
		\item $\phi$ is \textit{positive semi-definite}\index{positive semi-definite} if for all $u \in V$, $\phi(u, u) \geq 0$,
		\item $\phi$ is \textit{negative definite}\index{negative definite} if for all $u \in V \setminus \{0\}$, $\phi(u, u) < 0$,
		\item $\phi$ is \textit{negative semi-definite}\index{negative semi-definite} if for all $u \in V$, $\phi(u, u) \leq 0$.
	\end{enumerate}
\end{definition}

\begin{exbox}
	Consider the matrix
	\[
	\begin{pmatrix}
		I_p & 0 \\
		0 & 0
	\end{pmatrix}
	,\]
	over an $n$-dimensional vector space. Then this is positive definite for $p = n$, and positive semi-definite for all $p \leq n$.
\end{exbox}

Using this definition, we can prove Sylvester's law.

\begin{proofbox}
	In order to prove that $p$ is independent of the choice of the basis, we show that $p$ is the largest dimension of a subspace on which $\phi$ is positive definite. Then this implies the theorem.

	Indeed, say $\mathcal{B} = (v_1, \ldots, v_n)$, in which
	\[
		[\phi]_{\mathcal{B}} =
		\begin{pmatrix}
			I_p & 0 & 0\\
			   0 & I_q & 0 \\
			0 & 0 & 0
		\end{pmatrix}.
	\]
	If we let $X = \langle v_1, \ldots, v_p \rangle$, then $\phi$ is positive definite on $X$. Indeed, if $u = \sum_{i = 1}^{p} \lambda_i v_i$, then
	\begin{align*}
		Q(u) &= \phi(u, u) = \phi \Biggl( \sum_{i = 1}^{p} \lambda_i v_i, \sum_{i = 1}^{p} \lambda_i v_i \Biggr) \\
		     &= \sum_{i,j = 1}^{p} \lambda_i \lambda_j \phi(v_i, v_j) \\
		     &= \sum_{i = 1}^{p} \lambda_i^2 > 0
	\end{align*}
	for $u \neq 0$.

	Now suppose that $\phi$ is positive definite when restricted to another subspace $X'$. Let $X = \langle v_1, \ldots, v_p \rangle$, and $Y = \langle v_{p+1}, \ldots, v_n \rangle$. Then $\phi$ is negative semi-definite on $Y$. But this implies $X' \cap Y = \{0\}$, so $Y + X' = Y \oplus X'$, and so $\dim Y + \dim X' \leq n$, giving $\dim X' \leq p$.
\end{proofbox}

\begin{remark}
	We did not need to show this for our proof, but we can show $q$ is the largest dimension of a subspace in which $\phi$ is negative definite.
\end{remark}

We have seen earlier that the (right) kernel of a bilinear form $\phi$ is
\[
	U = \Ker \phi_R = \{v \in V \mid \forall u \in V, \phi(u, v) = 0\}
.\]

Then we know $\dim U + \rank(\phi) = n$. For $F = \mathbb{R}$, then from Sylvester's law, there is a subspace $T$ of dimension $n - (p+q) + \min \{p, q\}$ such that $\phi|_T = 0$, by pairing up respective elements of $P = \langle v_1, \ldots, v_p \rangle$ and $Q = \langle v_{p+1}, \ldots, v_{p+q} \rangle$.

Moreover, we can show that this is the largest dimension of a subspace $T'$ on which $\phi|_{T' \times T'} = 0$.

\subsection{Sesquilinear Forms}
\label{sub:sesquilinear_forms}

\begin{definition}
	For $F = \mathbb{C}$, the \textit{standard inner product}\index{standard inner product} on $\mathbb{C}^{n}$ for
\[
x =
\begin{pmatrix}
	x_1 \\
	\vdots \\
	x_n
\end{pmatrix}
, y =
\begin{pmatrix}
	y_1 \\
	\vdots \\
	y_n
\end{pmatrix}
,\]
is
\[
\langle x, y \rangle = \sum_{i = 1}^{n} x_i \overline{y_i}
.\]
In particular,
\[
\|x\|^2 = \langle x, x \rangle = \sum_{i = 1}^{n} |x_i|^2
.\]
\end{definition}

Note that $(x, y) \mapsto \langle x, y \rangle$ is \textbf{not a bilinear form}: for $\lambda \in \mathbb{C}$,
\begin{align*}
	\langle \lambda x, y \rangle &= \sum_{i = 1}^{n} \lambda x_i \overline{y_i} = \lambda \langle x, y \rangle, \\
	\langle x, \lambda y \rangle &= \sum_{i = 1}^{n} x_i \overline{\lambda y_i} = \overline{\lambda} \langle x, y \rangle.
\end{align*}
Instead, it is \textit{antilinear}\index{antilinear} with respect to the second coordinate.

Similar to the standard inner product over $\mathbb{R}$, we can extend this concept to arbitrary antilinear forms.

\begin{definition}
	Let $V, W$ be $\mathbb{C}$-vector spaces. A \textit{sesquilinear form}\index{sesquilinear form} $\phi$ is a function $\phi : V \times W \to \mathbb{C}$ such that
	\begin{enumerate}[(i)]
		\item $\phi(\lambda_1v_1 + \lambda_2v_2, w) = \lambda_1 \phi(v_1, w) + \lambda_2 \phi(v_2, w)$ (i.e. it is linear in the first variable),
		\item $\phi(v, \lambda_1w_1 + \lambda_2w_2) = \overline{\lambda_1}\phi(v, w_1) + \overline{\lambda_2}\phi(v, w_2)$ (i.e. it is antilinear in the second variable).
	\end{enumerate}
\end{definition}

\begin{definition}
	For $\mathcal{B} = (v_1, \ldots, v_m)$ and $\mathcal{C} = (w_1, \ldots, w_n)$, bases of $V$ and $W$, then
	\[
		[\phi]_{\mathcal{B},\mathcal{C}} = (\phi(v_i, w_j))
	\]
	is an $m \times n$ matrix.
\end{definition}

\begin{lemma}
	$\phi(v, w) = [v]_{\mathcal{B}}^{T}[\phi]_{\mathcal{B},\mathcal{C}}\overline{[w]_{\mathcal{C}}}$.
\end{lemma}

\begin{proofbox}
	Exercise.
\end{proofbox}

\begin{lemma}
	Let $\mathcal{B}, \mathcal{B}'$ be bases for $V$, and $\mathcal{C},\mathcal{C}'$ be basis for $W$ Let $P = [\id]_{\mathcal{B}',\mathcal{B}}$ and $Q = [\id]_{\mathcal{C}',\mathcal{C}}$. Then,
	\[
		[\phi]_{\mathcal{B}',\mathcal{C}} = P^{T}[\phi]_{\mathcal{B},\mathcal{C}} \overline{Q}
	.\]
\end{lemma}

\begin{proofbox}
	Exercise.
\end{proofbox}

\subsection{Hermitian Forms}
\label{sub:hermitian_forms}

\begin{definition}
	Let $V$ be a finite dimensional $\mathbb{C}$-vector space. A sesquilinear form $\phi : V \times V \to \mathbb{C}$ is called \textit{Hermitian}\index{Hermitian} if
	\[
		\forall (u, v) \in U \times V, \qquad \phi(u, v) = \overline{\phi(v, u)}
	.\]
\end{definition}

\begin{remark}
	If $\phi$ is Hermitian, then $\phi(u, u) = \overline{\phi(u, u)}$, so $\phi(u, u) \in \mathbb{R}$. In particular, this allow us to define positive or negative definite Hermitian forms.
\end{remark}

\begin{lemma}
	A sesquilinear form $\phi : V \times V \to \mathbb{C}$ is Hermitian if and only if for all bases $\mathcal{B}$ of $V$,
	\[
		[\phi]_{\mathcal{B}} = \overline{[\phi]_{\mathcal{B}}^{T}}
	.\]
\end{lemma}

\begin{proofbox}
	If $A = [\phi]_{\mathcal{B}} = (a_{ij})$ where $\mathcal{B} = (e_1, \ldots, e_n)$, then $a_{ij} = \phi(e_i, e_j)$. Then,
	\[
	a_{ji} = \phi(e_j, e_i) = \overline{\phi(e_i, e_j)} = \overline{a_{ij}}
	.\]
	Thus, $[\phi]_{\mathcal{B}}^{T} = \overline{[\phi]_{\mathcal{B}}}$.

	Conversely, if $[\phi]_{\mathcal{B}} = A$, where $A = \overline{A^{T}}$, then if
	\[
	u = \sum_{i = 1}^{n} \lambda_i e_i, \qquad v = \sum_{i = 1}^{n} \mu_i e_i
	,\]
	then we have
	\begin{align*}
		\phi(u, v) &= \phi \Biggl( \sum_{i = 1}^{n} \lambda_i e_i , \sum_{i = 1}^{n} \mu_i e_i \Biggr) \\
			   &= \sum_{i,j = 1}^{n} \lambda_i \overline{\mu_j} \phi(e_i, e_j) = \sum_{i, j = 1}^{n} \lambda_i \overline{\mu_j} a_{ij},
	\end{align*}
	and we can similarly compute
	\begin{align*}
		\overline{\phi(v, u)} &= \overline{\phi \Biggl( \sum_{i = 1}^{n} \mu_i e_i, \sum_{i = 1}^{n} \lambda_i e_i \Biggr)} \\
		&= \overline{\sum_{i,j = 1}^{n} \mu_i \overline{\lambda_j}\phi(e_i, e_j)} = \sum_{i,j = 1}^{n} \overline{\mu_i}\lambda_i \overline{a_{ij}} \\
		&= \sum_{i, j = 1}^{n} \lambda_i \overline{\mu_j} \overline{a_{ji}} = \sum_{i, j = 1}^{n}\lambda_i \overline{\mu_j} a_{ij}.
	\end{align*}
	Hence $\phi(u, v) = \overline{\phi(v, u)}$.
\end{proofbox}

Similar to real symmetric matrices, we have the polarization identity for Hermitian matrices.

\begin{proposition}
	A Hermitian form $\phi$ on a complex vector space $V$ is entirely determined by its quadratic form $Q : V \to \mathbb{R}$, $u \mapsto \phi(u, u)$, by the formula
	\[
		\phi(u, v) = \frac{1}{4} [Q(u+v) - Q(u-v) + iQ(u+iv) - iQ(u-iv)]
	.\]
\end{proposition}

\begin{proofbox}
	Exercise. (Just check).
\end{proofbox}

\begin{theorem}[Sylvester's Law for Hermitian Forms]\index{Sylvester's law}
	Let $V$ be a finite-dimensional $\mathbb{C}$-vector space, and $\phi : V \times V \to \mathbb{C}$ a Hermitian form on $V$. Then, there exists a basis $\mathcal{B} = (v_1, \ldots, v_n)$ of $V$ such that
	\[
		[\phi]_{\mathcal{B}} =
		\begin{pmatrix}
			I_p & 0 & 0 \\
			    0 & -I_q & 0 \\
			0 & 0 & 0
		\end{pmatrix}
	,\]
	where $p$ and $q$ depend only on $\phi$.
\end{theorem}

\begin{proofbox}
	(Sketch: this is nearly identical to the real case).

	We prove existence first. If $\phi \equiv 0$, we are done. Otherwise, assume $\phi \neq 0$, then the polarization identity ensures that there exists $e_1$ such that $\phi(e_1, e_1) \neq 0$. Then rescaling $v_1 = \frac{e_1}{\sqrt{\phi(e_1, e_1)|}}$, we have $\phi(v_1, v_1) = \pm 1$.

	Then considering the orthogonal complement $W = \{ w \in V \mid \phi(v_1, w) = 0\}$, we get $V = \langle v_1 \rangle \oplus W$. By induction on the dimension, as $\phi|_{W}$ is Hermitian on $W \times W$, we get the required form.

	To show uniqueness, we show $p$ is the maximal dimension of a subspace on which $\phi$ is positive definite. This can be done exactly in the same way as for real symmetric matrices.
\end{proofbox}

\subsection{Skew Symmetric Real Valued Forms}
\label{sub:skew_symmetric_real_valued_forms}

\begin{definition}
	A bilinear form $\phi : V \times V \to \mathbb{R}$ is \textit{skew symmetric}\index{skew symmetric} or \textit{antisymmetric}\index{antisymmetric} if
	\[
	\forall (u, v) \in U \times V, \qquad \phi(u, v) = - \phi(v, u)
	.\]
\end{definition}

\begin{remark}
	\begin{enumerate}[(i)]
		\item[]
		\item Letting $v = u$, $\phi(u, u) = - \phi(u, u)$, so $\phi(u, u) = 0$.
		\item For all bases $\mathcal{B}$ of $V$, $[\phi]_{\mathcal{B}} = - [\phi]_{\mathcal{B}}^{T}$.
		\item If $A \in \mathcal{M}_n(\mathbb{R})$, then
			\[
			A = \frac{1}{2}(A + A^{T}) + \frac{1}{2}(A - A^{T})
			.\]
	\end{enumerate}
\end{remark}

\begin{theorem}[Sylvester's law for Skew Symmetric Forms]
	Let $V$ be a finite dimensional $\mathbb{R}$-vector space, and $\phi : V \times V \to \mathbb{R}$ be a skew symmetric bilinear form. Then there exists a basis $\mathcal{B}$ of $V$, say
	\[
	\mathcal{B} = (v_1, w_1, v_2, w_2, \ldots, v_m, w_m, v_{2m+1}, v_{2m+2}, \ldots, v_n)
	,\]
	such that
	\[
		[\phi]_{\mathcal{B}} =
		\begin{pmatrix}
			0 & 1 & 0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
			-1 & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
			0 & 0 & 0 & 1 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
			0 & 0 & -1 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
			\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
			0 & 0 & 0 & 0 & \cdots & 0 & 1 & 0 & \cdots & 0 \\
			0 & 0 & 0 & 0 & \cdots & -1 & 0 & 0 & \cdots & 0 \\
			0 & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
			\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
			0 & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0
		\end{pmatrix}
	.\]
\end{theorem}

\begin{corollary}
	Skew symmetric matrices have an even rank.
\end{corollary}

\begin{proofbox}
	(Sketch).

	We proceed by induction on the basis of $V$. If $\phi = 0$, we are done, so assume $\phi \neq 0$. Then, there exist $v_1, w_1$ such that $\phi(v_1, w_1) \neq 0$. After scaling, we can let $\phi(v_1, w_1) = 1$, and $\phi(w_1, v_1) = -1$.

	Then $v_1, w_1$ are linearly independent, as $\phi(v_1, \lambda v_1) = \lambda \phi(v_1, v_1) = 0$. Define $U = \langle v_1, w_1 \rangle$, and define the orthogonal complement $W = \{ v \in V \mid \phi(v_1, v) = \phi(w_1, v) = 0\}$. Then $V = U \oplus W$, so applying the induction hypothesis gives us the required conclusion.
\end{proofbox}

\subsection{Inner Product Spaces}
\label{sub:inner_product_spaces}

We have seen that for positive definite bilinear forms, we can define a scalar product, and a norm.

These have a generalization to infinite dimensional space, named \textit{Hilbert Spaces} after their progenitor.

\begin{definition}
	Let $V$ be a vector space over $\mathbb{R}$ (resp. $\mathbb{C}$). An \textit{inner product}\index{inner product} is a positive definite, symmetric (resp. Hermitian) form $\phi$ on $V$. Then $V$ is called a real (resp. complex) \textit{inner product space}\index{inner product space}.
\end{definition}

Over inner product spaces, we simplify $\phi(u, v)$ as $\langle u, v \rangle$.

\begin{exbox}
	\begin{enumerate}[(i)]
		\item Take $V = \mathbb{R}^{n}$. Then for
			\[
			x =
			\begin{pmatrix}
				x_1 \\
				\vdots \\
				x_n
			\end{pmatrix}
			, \qquad y =
			\begin{pmatrix}
				y_1 \\
				\vdots \\
				y_n
			\end{pmatrix}
			,\]
			we have inner product
			\[
				\langle x, y \rangle = \sum_{i = 1}^{n} x_i y_i
			.\]
		\item Take $V = \mathbb{C}^{n}$. Then we have inner product
			\[
			\langle x, y\rangle = \sum_{i = 1}^{n} x_i \overline{y_i}
			.\]
		\item If $V = \mathcal{C}([0, 1], \mathbb{C})$, i.e. the space of continuous functions $f : [0, 1] \to \mathbb{C}$, then we have inner product
			\[
			\langle f, g \rangle = \int_{0}^{1}f(t) \overline{g(t)} \diff t
			.\]
			This is known as the $L^{2}$ scalar product.
	\end{enumerate}
	We can check these all form inner products. In particular, $\langle u, u \rangle = 0 \implies u = 0$, due to the positive definite assumption.
\end{exbox}

\subsection{Gram-Schmidt}
\label{sub:gram_schmidt}

We work with a vector space $V$ over $\mathbb{R}$ or $\mathbb{C}$ with inner product $\langle \cdot, \cdot \rangle$.

Define the \textit{norm}\index{norm} of $v \in V$ as
\[
	\|v\| = \sqrt{\langle v, v \rangle}
,\]
and as the inner product is positive definite, $\|v\| = 0 \iff v = 0$. This gives a notion of length in $V$.

\begin{lemma}[Cauchy-Schwarz]\index{Cauchy-Schwarz}
	For $u, v \in V$,
	\[|\langle u, v \rangle| \leq \|u\|\|v\|.\]
	Moreover, equality holds if and only if $u$ and $v$ are collinear.
\end{lemma}

\begin{proofbox}
	For $F = \mathbb{R}$ or $\mathbb{C}$, let $t \in F$. Then,
	\begin{align*}
		0 &\leq \|tu-v\|^2 = \langle tu-v, tu-v\rangle \\
		  &= t \overline{t}\langle u, u \rangle - t \langle u, v \rangle - \overline{t} \langle v, u \rangle + \langle v, v\rangle \\
		  &= |t|^2\|u\|v^2 - 2 \Re (t \langle u, v \rangle) + \|v\|^2.
	\end{align*}
	Set $t = \frac{\langle u, v \rangle}{\|u\|^2}$, assuming $u \neq 0$. Then,
	\begin{align*}
		0 &\leq \frac{|\langle u, v \rangle|^2}{\|u\|^{4}}\|u\|^2 - 2 \Re \biggl( \frac{|\langle u, v \rangle|^2}{\|u\|^2} \biggr) + \|v^2\|, \\
		0 &\leq \|v\|^2 - \frac{|\langle u, v \rangle|^2}{\|u\|^2}, \\
		|\langle u, v \rangle|^2 \leq \|u\|^2\|v\|^2,
	\end{align*}
	as required. Then equality holds when $tu-v = 0$, so when $u, v$ are collinear.
\end{proofbox}

\begin{corollary}[Triangle inequality]\index{triangle inequality}
	$\|u + v\| \leq \|u\| + \|v\|$.
\end{corollary}

\begin{proofbox}
	We square both sides:
	\begin{align*}
		\|u+v\|^2 &= \langle u + v, u + v \rangle = \|u\|^2 + 2 \Re (\langle u, v \rangle) + \|v\|^2 \\
			  &\leq \|u\|^2 + 2 \|u\|\|v\| + \|v\|^2 \\
			  &= (\|u\| + \|v\|)^2.
	\end{align*}
\end{proofbox}

\begin{definition}
	A set $(e_1, \ldots, e_k)$ of non-zero vectors of $V$ is
	\begin{enumerate}[(i)]
		\item \textit{orthogonal}\index{orthogonal} if $\langle e_i, e_j \rangle = 0$ for $i \neq j$,
		\item \textit{orthonormal}\index{orthonormal} if $\langle e_i, e_j \rangle = \delta_{ij}$, where
			\[
			\delta_{ij} =
			\begin{cases}
				1 & i = j, \\
				0 & i \neq j.
			\end{cases}
			\]
	\end{enumerate}
\end{definition}

\begin{lemma}
	If $(e_1, \ldots, e_k)$ is orthogonal, then
	\begin{enumerate}[\normalfont(i)]
		\item the family is free;
		\item if $v = \sum_{j = 1}^{k} \lambda_j e_j$, then
			\[
			\lambda_j = \frac{\langle v, e_j \rangle}{\|e_j\|^2}
			.\]
	\end{enumerate}
\end{lemma}

\begin{proofbox}
	To show (i), first say $\sum_{j = 1}^{k} \lambda_j e_j = 0$. Then,
	\[
	0 = \biggl\langle \sum_{j = 1}^{k} \lambda_j e_j, e_i \biggr\rangle = \sum_{j = 1}^{k} \lambda_j \langle e_j, e_i \rangle = \lambda_i \|e_i\|^2
	,\]
	for all $1 \leq i \leq k$, so the family is free.

	Similarly if $v = \sum_{i = 1}^{k} \lambda_i e_i$, then
	\begin{align*}
		\langle v, e_j \rangle &= \sum_{i = 1}^{k} \lambda_i \langle e_i, e_j \rangle = \lambda_j \|e_j\|^2, \\
		\implies \lambda_j &= \frac{1}{\|e_j\|^2} \langle v, e_j \rangle.
	\end{align*}
\end{proofbox}

\begin{lemma}[Parseval's identity]\index{Parseval's identity}
	If $V$ is a finite dimensional inner product space, and $(e_1, \ldots, e_n)$ is an orthonormal basis, then
	\[
	\langle u, v \rangle = \sum_{i = 1}^{n} \langle u, e_i \rangle \overline{\langle v, e_i \rangle}
	.\]
	In particular, in an orthonormal basis,
	\[
	\|v\|^2 = \langle v, v \rangle = \sum_{i = 1}^{n} |\langle v, e_i \rangle|^2
	,\]
	where $\langle v, e_i \rangle$ are the coordinates of $v$:
	\[
	v = \sum_{i = 1}^{n} \langle v, e_i \rangle e_i
	.\]
\end{lemma}

\begin{proofbox}
	Suppose that
	\[
	u = \sum_{i = 1}^{n} \langle u, e_i \rangle e_i, \qquad v = \sum_{i = 1}^{n} \langle v, e_i \rangle e_i
	.\]
	Then,
	\begin{align*}
		\langle u, v \rangle = \biggl\langle \sum_{i = 1}^{n} \langle u, e_i \rangle e_i, \sum_{i = 1}^{n} \langle v, e_i \rangle e_i \biggr\rangle = \sum_{i = 1}^{n} \langle u, e_i \rangle \overline{\langle v, e_i \rangle}.
	\end{align*}
\end{proofbox}

\begin{theorem}[Gram-Schmidt Orthogonalization]\index{Gram-Schmidt}
	Let $V$ be an inner product space, $I$ a countable set and $(v_i)_{i \in I}$ linearly independent.

	Then there exists a sequence $(e_i)_{i \in I}$ of orthonormal vectors such that
	\[
	\spn \langle v_1, \ldots, v_k \rangle = \spn \langle e_1, \ldots, e_k \rangle
	,\]
	for all $k \geq 1$. In particular, for finite dimensional vector spaces, this proves the existence of an orthonormal basis.
\end{theorem}

\begin{proofbox}
	We proceed by induction. on $k$.

	For $k = 1$, since $v_1 \neq 0$, we can take $e_1 = \frac{v_1}{\|v_1\|}$.

	Now, suppose we have constructed $(e_1, \ldots, e_k)$, and we want to find $e_{k+1}$. Then, define
	\[
	e_{k+1}' = v_{k+1} - \sum_{i = 1}^{k}\langle v_{k+1}e_i \rangle e_i
	.\]
	We show that $e_{k+1}'$ non-zero, otherwise
	\[v_{k+1} \in \langle e_1 ,\ldots, e_k \rangle = \langle v_1, \ldots, v_k\rangle,\]
	contradicting our assumption $(v_i)$ is free. Now we prove $e_{k+1}'$ is orthogonal to $e_j$. Indeed,
	\begin{align*}
		\langle e_{k+1}', e_j \rangle &= \biggl\langle v_{k+1} - \sum_{i = 1}^{k} \langle v_{k+1}, e_i \rangle e_i, e_j \biggr\rangle \\
					      &= \langle v_{k+1}, e_j \rangle - \langle v_{k+1}, e_j \rangle = 0.
	\end{align*}
	Moreover, it is clear that
	\[
	\langle v_1, \ldots, v_{k+1} \rangle = \langle e_1, \ldots, e_k, e_{k+1}'\rangle
	.\]
	All that is left to do is to normalise $e_{k+1}'$, which can be done by taking
	\[
	e_{k+1} = \frac{e_{k+1}'}{\|e_{k+1}'\|}
	.\]
\end{proofbox}

\begin{corollary}
	If $V$ is a finite dimensional inner product space, then any orthonormal set of vectors can be extended to an orthonormal basis of $V$.
\end{corollary}

\begin{proofbox}
	Pick $(e_1, \ldots, e_k)$ orthonormal. Then they are linearly independent, so we can extend to a basis $(e_, \ldots, e_k, v_{k+1}, \ldots, v_n)$ to the basis of $V$. Applying Gram-Schmidt to this set, and noticing there is no need to modify $e_i$ for $1 \leq i \leq k$, we get a basis $(e_1, \ldots, e_k, e_{k+1}, \ldots, e_n)$ an orthonormal basis of $V$.
\end{proofbox}

\begin{remark}
	If $A \in \mathcal{M}_n(\mathbb{R})$, then $A$ has orthonormal columns vectors if and only if
	\[
	A^{T}A = I
	.\]
	Similarly, if $A \in \mathcal{M}_n(\mathbb{C})$, then $A$ has orthonormal column vectors if and only if
	\[
	A^{T}\overline{A} = I
	.\]
\end{remark}

\begin{definition}
	\begin{enumerate}[(i)]
		\item[]
		\item A matrix $A \in \mathcal{M}_n(\mathbb{R})$ is \textit{orthogonal}\index{orthogonal} if
		\[
		A^{T}A = I
		.\]
	\item A matrix $A \in \mathcal{M}_n(\mathbb{C})$ is \textit{unitary}\index{unitary} if
		\[
		A^{T}\overline{A} = I
		.\]
	\end{enumerate}
\end{definition}

\begin{proposition}
	For $A \in \mathcal{M}_n(\mathbb{R})$ (resp. $\mathcal{M}_n(\mathbb{C})$, then $A$ can be written as $RT$, where
	\begin{itemize}
		\item $T$ is upper triangular,
		\item $R$ is orthogonal (resp. unitary).
	\end{itemize}
\end{proposition}

\begin{proofbox}
	Exercise (apply Gram-Schmidt to the column vectors of $A$).
\end{proofbox}

\subsection{Orthogonal Complement and Projection}
\label{sub:orthogonal_complement_and_projection}

\begin{definition}
	Let $V$ be an inner product space, and $V_1, V_2 \leq V$. We say that $V$ is the \textit{orthogonal direct sum}\index{orthogonal direct sum} of $V_1$ and $V_2$ if
	\begin{enumerate}[(i)]
		\item $V = V_1 \oplus V_2$,
		\item For all $(v_1, v_2) \in V_1 \times V_2$, $\langle v_1, v_2 \rangle = 0$.
	\end{enumerate}
\end{definition}

\begin{remark}
	For $v \in V_1 \cap V_2$,
	\[
	\|v\|^2 = \langle v , v \rangle = 0 \implies v = 0
	.\]
\end{remark}

\begin{definition}
	Let $V$ be an inner product space, and $W \leq V$. Then we define the \textit{orthogonal complement}\index{orthogonal complement} of $W$ as
	\[
		W^{\perp} = \{ v \in V \mid \langle v, w \rangle = 0 \, \forall w \in W\}
	.\]
\end{definition}

\begin{lemma}
	Let $V$ be a finite dimensional inner product space, and $W \leq V$. Then $V$ is the orthogonal direct sum of $W$ and $W^{\perp}$.
\end{lemma}

To prove this, we introduce the following definition.

\begin{definition}
	Suppose $V = U \oplus W$, so $U$ is a complement of $W$ in $V$. We define
	\begin{align*}
		\pi : V &\to W \\
		v = u + w &\mapsto w
	\end{align*}
	We say $\pi$ is the \textit{projection operator}\index{projection operator} onto $W$.
\end{definition}

\begin{remark} 
	$\id - \pi$ is a projection onto $U$.
\end{remark}

If $V$ is an inner product space and $W$ is finite dimensional, then we can choose $U = W^{\perp}$, for which $\pi$ is explicit.

\begin{lemma}
	Let $V$ be an inner product space. Let $W \leq V$, with $W$ finite dimensional. Let $(e_1, \ldots, e_k)$ be an orthonormal basis of $W$. Then,
	\begin{enumerate}[\normalfont(i)]
		\item $\pi(v) = \sum_{i = 1}^{k} \langle v, e_i \rangle e_i$, for all $v \in V$, and $V$ is the orthogonal direct sum of $W$ and $W^{\perp}$,
		\item  For all $v \in V$ and $w \in W$ 
			\[
			\|v - \pi(v)\| \leq \|v - w\|
			,\]
			with equality if and only if $w = \pi(v)$.
	\end{enumerate}
\end{lemma}

\begin{remark}
	This has an infinite dimensional generalization: instead of taking $V$ an inner product space, we can let $V$ be a Hilbert space, and instead of letting $W$ be finite dimensional, we can say that $W$ is closed.

	Geometrically, this says that $\pi(v)$ is the closest point on $W$ to $v$.
\end{remark}

\begin{proofbox}
	For (i), we have $W = \spn \langle e_1, \ldots, e_k \rangle$, where the $(e_i)$ are orthonormal. We can define
	\[
	\pi(v) = \sum_{i = 1}^{k}\langle v, e_i \rangle e_i
	.\]
	Then, notice that
	\[
		v = \underbrace{\pi(v)}_{\in W} + (v -  \pi(v))
	,\]
	so we wish to show that $v - \pi(v) \in W^{\perp}$, but notice
	\begin{align*}
		& v - \pi(v) \in W^{\perp} \\
		\iff & \forall w \in W, \quad \langle v - \pi(v), w \rangle = 0 \\
		\iff & \forall 1 \leq j \leq k, \quad \langle v - \pi(v), e_j \rangle = 0.
	\end{align*}
	Computing for all $e_j$,
	\begin{align*}
		\langle v - \pi(v), e_j \rangle &= \biggl \langle v - \sum_{i = 1}^{k} \langle v, e_i \rangle e_i, e_j \biggr\rangle \\
						&= \langle v, e_j \rangle - \langle v, e_j \rangle = 0,
	\end{align*}
	which shows that $v - \pi(v) \in W^{\perp}$. Hence, since $v = \pi(v) + (v - \pi(v))$, $V = W + W^{\perp}$, and we known $W \cap W^{\perp} = \{0\}$, so $V$ is the orthogonal direct sum of $W$ and $W^{\perp}$.

	For (ii), let $w \in W$. Then,
	\begin{align*}
		\|v - w\|^2 &= \|v - \pi(v) + \pi(v) - w\|^2 \\
			    &= \langle v - \pi(v) + \pi(v) - w, v - \pi(v) + \pi(v) - w \rangle \\
			    &= \|v - \pi(v)\|^2 + \|\pi(v) - w\|^2 \geq \|v - \pi(v)\|^2,
	\end{align*}
	with equality if and only if $w = \pi(v)$.
\end{proofbox}

\newpage

\section{Adjoint Map}
\label{sec:adjoint_map}

\begin{definition}
	Let $V, W$ be finite dimensional inner product spaces, and let $\alpha \in \mathcal{L}(V, W)$. Then, there exists a \emph{unique} linear map $\alpha^{\ast} : W \to V$ such that for all $(v, w) \in V \times W$,
	\[
	\langle \alpha(v), w \rangle = \langle v, \alpha^{\ast}(w) \rangle
	.\]
	Moreover, if $\mathcal{B}$ and $\mathcal{C}$ are orthonormal bases of $V$ and $W$, then
	\[
		[\alpha^{\ast}]_{\mathcal{C},\mathcal{B}} = \overline{[\alpha]_{\mathcal{B},\mathcal{C}}^{T}}
	.\]
\end{definition}

\begin{proofbox}
	This is a computation. Let $\mathcal{B} = (v_1, \ldots, v_n)$ and $\mathcal{C} = (w_1, \ldots, w_m)$. Then, letting $A = [\alpha]_{\mathcal{B},\mathcal{C}} = (a_{ij})$, if we let $[\alpha^{\ast}]_{\mathcal{C},\mathcal{B}} = \overline{A^{T}} = C = (c_{ij})$, then $c_{ij} = \overline{a_{ij}}$. We compute
	\begin{align*}
		\biggl\langle \alpha \biggl( \sum_{i=1}^{n} \lambda_i v_i \biggr), \sum_{j=1}^{m} \mu_j w_j \biggr\rangle &= \biggl \langle \sum_{i,k} \lambda_i a_{ki} w_k, \sum_{j = 1}^{m} \mu_j w_j \biggr \rangle \\
															  &= \sum_{i,j} \lambda_i a_{ji} \overline{\mu_j}.
	\end{align*}
	Similarly, we can compute
	\begin{align*}
	\biggl\langle \sum_{i = 1}^{n} \lambda_i v_i, \alpha^{\ast} \biggl( \sum_{j = 1}^{m} \mu_j w_j \biggr) \biggr \rangle &= \biggl \langle \sum_{i=1}^{n} \lambda_i v_i, \sum_{j, k} \mu_j c_{kj} v_k \biggr \rangle \\
															      &= \sum_{ij} \lambda_i \overline{c_{ij}} \overline{\mu_j}.
	\end{align*}
	Then as $\overline{c_{ij}} = a_{ji}$, these give the same results, proving the existence of $\alpha^{\ast}$.

	Now, the proof of uniqueness follows by computing $\alpha^{\ast}(w_j)$: for all $1 \leq i \leq n$,
	\begin{align*}
		\langle v_i, \alpha^{\ast}(w_j) \rangle &= \langle \alpha(v_i), w_j \rangle \\
							&= \biggl \langle \sum_{k = 1}^{m} a_{ki}w_k, w_j \biggr \rangle = a_{ji},
	\end{align*}
	so  $\langle \alpha^{\ast}(w_j), v_i \rangle = \overline{a_{ji}}$, giving
	\[
	\alpha^{\ast}(w_j) = \sum_{i = 1}^{n} \overline{a_{ji}} v_i
	.\]
	This uniquely determines $\alpha^{\ast}$ by linearity.
\end{proofbox}

\begin{remark}
	Notice that we use the same notation $\alpha^{\ast}$ for the adjoint of $\alpha$, and for the dual of $\alpha$.

	Indeed, if $V, W$ are real product spaces and $\alpha \in \mathcal{L}(V, W)$, then defining
	\begin{align*}
		\psi_{R,V} : V &\to V^{\ast} \\
		v &\mapsto \langle \cdot, v \rangle, \\
		\psi_{R,W} : W &\to W^{\ast} \\
		w &\mapsto \langle \cdot, w \rangle,
	\end{align*}
	then $\phi_{R,V}$ and $\phi_{R,W}$ are isomorphisms, and the adjoint map of $\alpha$ is given by
	\[
		\begin{tikzcd}
			W \arrow[r, "{\psi_{R, W}}", swap] & W^{\ast} \arrow[r, "{\text{dual of } \alpha}", swap] & V^{\ast} \arrow[r, "{\psi_{R,V}^{-1}}", swap] & V
		\end{tikzcd}
	\] 
\end{remark}

\subsection{Self-adjoint Maps and Isometries}
\label{sub:self_adjoint_maps_and_isometries}

\begin{definition}
	Let $V$ be a finite dimensional inner product space, and $\alpha \in \mathcal{L}(V)$. Let $\alpha^{\ast} \in \mathcal{L}(V)$ be the adjoint map. Then,
	\begin{enumerate}[(i)]
		\item $\alpha$ is \textit{self-adjoint}\index{self-adjoint} if
			\[
			\forall (v, w) \in V \times V, \langle \alpha v, w \rangle = \langle v, \alpha w \rangle \iff \alpha = \alpha^{\ast}
		.\]
		Over $\mathbb{R}$, we say $\alpha$ is symmetric, and over $\mathbb{C}$, we say $\alpha$ is Hermitian.
	\item $\alpha$ is an \textit{isometry}\index{isometry} if
		\[
		\forall (v, w) \in V \times V, \langle \alpha v, \alpha w \rangle = \langle v, w \rangle \iff \alpha^{\ast} = \alpha^{-1}
		.\]
		Over $\mathbb{R}$, we say $\alpha$ is orthogonal,\index{orthogonal map} and over $\mathbb{C}$ we say $\alpha$ is unitary\index{unitary map}.
	\end{enumerate}
\end{definition}

\begin{proofbox}
	We show that these conditions are indeed equivalent. The equivalence for self-adjoint maps is clear for definition, so we look at isometries.

	First, we show that $\langle \alpha v, \alpha w \rangle = \langle v, w \rangle$ for all $v, w$ is equivalent to $\alpha$ invertible and $\alpha^{\ast} = \alpha^{-1}$.

	First, assume $\langle \alpha v, \alpha w \rangle = \langle v, w \rangle$ for all $v, w \in V$. Then setting $v = w$,
	\[
	\|\alpha v\|^2 = \langle \alpha v, \alpha v \rangle = \langle v, v \rangle = \|v\|^2
	.\]
	Hence $\alpha$ preserves the norm, so $\Ker \alpha = \{0\}$. As $V$ is finite dimensional, $\alpha$ is bijective, hence $\alpha^{-1}$ is well defined. Now, for all $v, w \in V$,
	\[
	\langle v, \alpha^{\ast}w \rangle = \langle \alpha v, w \rangle = \langle \alpha v, \alpha(\alpha^{-1} w) \rangle = \langle v, \alpha^{-1}(w)\rangle
	.\]
	Since this holds for all $v$, $\alpha^{\ast}(w) = \alpha^{-1}(w)$, and as this holds for all $w$, $\alpha^{\ast} = \alpha^{-1}$.

	The converse is similar: if $\alpha^{\ast} = \alpha^{-1}$, then
	\[
	\langle \alpha v, \alpha w \rangle = \langle v, \alpha^{\ast} \alpha w \rangle = \langle v, w \rangle
	.\]
\end{proofbox}

\begin{remark}
	Using the polarization identity, one can show that
	\[
		\alpha \text{ is an isometry} \iff \forall v \in V, \|\alpha(v)\| = \|v\|
	.\]
	Indeed, one direction is easy to see. For the other direction, if $\|\alpha(v)\| = \|v\|$, define a new inner product by
	\[
		(v, w) = \langle \alpha v, \alpha w \rangle
	.\]
	Then $(\cdot, \cdot)$ is linear (resp. antilinear) as $\alpha$ is linear, positive definite as for $v \neq 0$, $(v, v) = \langle \alpha v, \alpha v \rangle = \langle v, v \rangle > 0$, and symmetric (resp. Hermitian) as $\langle \cdot, \cdot \rangle$ is Hermitian.

	Now $(v, v) = \langle v, v \rangle$ for all $v$, so by the polarization identity, $(v, w) = \langle v, w \rangle$ for all $v, w$. Hence
	\[
		\langle \alpha v, \alpha w \rangle = (v, w) = \langle v, w \rangle
	,\]
	so $\alpha$ is an isometry.
\end{remark}

\begin{lemma}
	Let $V$ be a finite dimensional real (resp. complex) inner product space. Then $\alpha \in \mathcal{L}(V)$ is:
	\begin{enumerate}[\normalfont(i)]
		\item self adjoint if and only if, in any orthonormal basis $\mathcal{B}$ of $V$, $[\alpha]_{\mathcal{B}}$ is symmetric (resp. Hermitian),
		\item an isometry if and only if in any orthonormal basis $\mathcal{B}$ of $V$, $[\alpha]_{\mathcal{B}}$ is orthogonal (resp. unitary).
	\end{enumerate}
\end{lemma}

\begin{proofbox}
	If $\mathcal{B}$ is an orthonormal basis, then $[\alpha^{\ast}]_{\mathcal{B}} = \overline{[\alpha]_{\mathcal{B}}^{T}}$.
	\begin{itemize}
		\item If $\alpha$ is self-adjoint, then $\overline{[\alpha]_{\mathcal{B}}^{T}} = [\alpha]_{\mathcal{B}}$.
		\item If $\alpha$ is an isometry, then $\overline{[\alpha]_{\mathcal{B}}^{T}} = [\alpha]_{\mathcal{B}}^{-1}$.
	\end{itemize}
\end{proofbox}

\begin{definition}
	Let $V$ be a finite dimensional inner product space over $F$.
	\begin{itemize}
		\item If $F = \mathbb{R}$, then we denote
			\[
				\mathrm{O}(V) = \{\alpha \in \mathcal{L}(V) \mid \alpha \text{ is an isometry}\}
			\]
			as the \textit{orthogonal group}\index{orthogonal group} of $V$.
		\item If $F = \mathbb{C}$, then we denote
			\[
				\mathrm{U}(V) = \{ \alpha \in \mathcal{L}(V) \mid \alpha \text{ is an isometry}\}
			\]
			as the \textit{unitary group}\index{unitary group} of $V$.
	\end{itemize}
\end{definition}

\begin{remark}
	For $V$ finite dimensional, and $(e_1, \ldots, e_n)$ an orthonormal basis, then
	\begin{itemize}
		\item For $F = \mathbb{R}$, there is a bijection
			\begin{align*}
				\mathrm{O}(V) &\to \{\text{orthonormal bases of } V\} \\
				\alpha &\mapsto (\alpha(e_1), \ldots, \alpha(e_n)).
			\end{align*}
		\item This also holds for $F = \mathbb{C}$: there is a bijection
			\begin{align*}
				\mathrm{U}(V) &\to \{\text{orthonormal bases of } V\} \\
				\alpha &\mapsto (\alpha(e_1), \ldots, \alpha(e_n)).
			\end{align*}
	\end{itemize}
\end{remark}

\subsection{Spectral Theory for Self Adjoint Maps}
\label{sub:spectral_theory_for_self_adjoint_maps}

Spectral theory is the study of the \textit{spectrum} of operators. This is very useful notion that comes up in further mathematics, physics (especially quantum mechanics), and is true for infinite dimensional Hilbert spaces.

\begin{lemma}
	Let $V$ be a finite dimensional inner product space. Let $\alpha \in \mathcal{L}(V)$ be self adjoint. Then,
	\begin{enumerate}[\normalfont(i)]
		\item $\alpha$ has real eigenvalues,
		\item The eigenvalues of $\alpha$ with respect to different eigenvalues are orthogonal.
	\end{enumerate}
\end{lemma}

\begin{proofbox}
	For (i), take $v \in V \setminus \{0\}$, and $\lambda \in \mathbb{C}$ such that $\alpha v = \lambda v$. Then,
	\begin{align*}
		\lambda \|v\|^2 &= \langle \lambda v, v \rangle = \langle \alpha v, v \rangle \\
				&= \langle v, \alpha^{\ast} v \rangle = \langle v, \alpha v \rangle \\
				&= \langle v, \lambda v \rangle = \overline{\lambda} \|v\|^2.
	\end{align*}
	Since $v \neq 0$, $\|v\| \neq 0$, so $\lambda = \overline{\lambda}$, and so $\lambda \in \mathbb{R}$.

	Now, for (ii) take two distinct eigenvalues $\lambda, \mu$ with eigenvectors $v, w$ respectively, so $\alpha v = \lambda v$, $\alpha w = \mu w$, with $\lambda, \mu \in \mathbb{R}$, $v, w \neq 0$ and $\lambda \neq mu$. Then,
	\begin{align*}
		\lambda \langle v, w\rangle &= \langle \lambda v, w \rangle = \langle \alpha v , w \rangle \\
					    &= \langle v, \alpha^{\ast} w \rangle = \langle v, \alpha w \rangle \\
					    &= \langle v, \mu w \rangle = \overline{\mu}\langle v, w \rangle \\
					    &= \mu \langle v, w \rangle.
	\end{align*}
	But as $\lambda \neq \mu$, we have $\langle v, w \rangle = 0$.
\end{proofbox}

This lemma leads us to the main proof of this part.

\begin{theorem}[Spectral theorem for self adjoint maps]\index{spectral theorem}
	Let $V$ be a finite dimensional inner product space. Let $\alpha \in \mathcal{L}(V)$ be self adjoint. Then $V$ has an orthonormal basis of eigenvectors of $\alpha$.
\end{theorem}

\begin{proofbox}
	We argue by induction on the dimension on $V$. Now $n = 1$ is trivial, so assume $n > 1$.

	Let $\mathcal{B}$ be any orthonormal basis of $V$, and say $A = [\alpha]_{\mathcal{B}}$. By the fundamental theorem of algebra, $\chi_A(t)$ has a complex root, which is an eigenvalue of $\alpha$. Since $\alpha = \alpha^{\ast}$, this eigenvalue is real. Let $\lambda \in \mathbb{R}$ be this eigenvalue.

	Pick an eigenvector $v_1 \in V \setminus \{0\}$, such that $\|v_1\| = 1$, and let $U = \langle v_1 \rangle^{\perp} \leq V$.

	Then, $U$ is stable by $\alpha$. Indeed, let $u \in U$, then
	\begin{align*}
		\langle \alpha u, v_1 \rangle &= \langle u, \alpha^{\ast} v_1 \rangle = \langle u, \alpha v_1 \rangle \\
					      &= \langle u, \lambda v_1 \rangle = \lambda \langle u, v_1 \rangle = 0.
	\end{align*}
	This implies we may consider $\alpha|_U \in \mathcal{L}(U)$, which is also self adjoint. Then, as $\dim U = \dim V - 1 = n - 1$, by the induction hypothesis, there exists $(v_2, \ldots, v_n)$, which is an orthonormal basis of eigenvectors for $\alpha|_U$.

	Thus, $(v_1, v_2, \ldots, v_n)$ is an orthonormal basis of $V$, consisting of eigenvectors of $\alpha$.
\end{proofbox}

\begin{corollary}
	Let $V$ be a finite dimensional inner product space. If $\alpha \in \mathcal{L}(V)$ is self adjoint, then $V$ is the direct sum of all the eigenspaces of $\alpha$.
\end{corollary}

\subsection{Spectral Theory for Unitary Maps}
\label{sub:spectral_theoery_for_unitary_maps}

\begin{lemma}
	Let $V$ be a complex inner product space, and let $\alpha \in \mathcal{L}(V)$ be unitary. Then,
	\begin{enumerate}[\normalfont(i)]
		\item All the eigenvalues of $\alpha$ lie on the unit circle,
		\item Eigenvalues corresponding to distinct eigenvalues are orthogonal.
	\end{enumerate}
\end{lemma}

\begin{proofbox}
	For (i), let $\lambda \in \mathbb{C}$ be an eigenvalue, and $v$ be a non-zero eigenvector of $\lambda$. Then $\lambda \neq 0$, otherwise $\alpha$ is not invertible, hence not unitary. Then,
	\begin{align*}
		\lambda\|v\|^2 &= \lambda \langle v, v \rangle = \langle \lambda v, v \rangle \\
			       &= \langle \alpha v, v \rangle = \langle v, \alpha^{\ast} v \rangle \\
			       &= \langle v, \alpha^{-1} v \rangle = \langle v, \lambda^{-1} v \rangle \\
			       &= \overline{\lambda^{-1}} \|v\|^2.
	\end{align*}
	Hence as $\|v\|^2 \neq 0$, $\lambda \overline{\lambda} = 1$, so $|\lambda| = 1$.

	For (ii), let $\lambda, \mu$ be eigenvalues with non-zero eigenvectors $v, w$ respectively. Then,
	\begin{align*}
		\lambda \langle v, w \rangle &= \langle \lambda v, w \rangle = \langle \alpha v, w \rangle \\
					     &= \langle v, \alpha^{\ast} w \rangle = \langle v, \alpha^{-1} w \rangle \\
					     &= \langle v, \mu^{-1} w \rangle = \overline{\mu^{-1}} \langle v, w \rangle \\
					     &= \mu \langle v, w \rangle.
	\end{align*}
	As $\lambda \neq \mu$, we get $\langle v, w \rangle = 0$.
\end{proofbox}

\begin{theorem}[Spectral theorem for unitary maps]\index{spectral theorem}
	Let $V$ be a finite dimensional complex inner product space, and let $\alpha \in \mathcal{L}(V)$ be unitary. Then $V$ has an orthonormal basis made of eigenvectors of $\alpha$.
\end{theorem}

Equivalently, a unitary map on a Hermitian inner product space can be diagonalized in an orthonormal basis.

\begin{proofbox}
	We proceed as in the other proof of the spectral theorem, by induction on the dimension $n$. $n = 1$ is easy to check, so assume $n > 1$.

	Pick any orthonormal basis $\mathcal{B}$ of $V$, and let $A = [\alpha]_{\mathcal{B}}$. By the fundamental theorem of algebra, $\chi_A(t)$ has a complex root, so $\alpha$ has a complex eigenvalue $\lambda$, with $|\lambda| = 1$ as $\alpha$ is unitary.

	Then, fix an eigenvector of $\lambda$, $v_1 \in V \setminus \{0\}$ with $\|v_1\| = 1$. Let $U = \langle v_1 \rangle^{\perp}$, then we claim $U$ is stable by $\alpha$. Indeed, for $u \in U$,
	\begin{align*}
		\langle \alpha u, v_1 \rangle &= \langle u, \alpha^{-1} v_1 \rangle = \langle u, \alpha^{-1}v_1 \rangle \\
					      &= \langle u, \lambda^{-1} v_1 \rangle = \overline{\lambda^{-1}} \langle u, v_1 \rangle = 0.
	\end{align*} 
	Hence, we can consider $\alpha|_U \in \mathcal{L}(U)$ which is unitary, so by the induction hypothesis, $\alpha|_U$ is diagonalizable in an orthonormal basis $(v_2, \ldots, v_n)$. Hence, $(v_1, \ldots, v_n)$ is an orthonormal basis of $V$ made of eigenvectors of $\alpha$.
\end{proofbox}

\begin{remark}
	This proof only worked as we were working over the complex numbers, which allowed us to use the fundamental theorem of algebra to deduce that $\alpha$ had an eigenvalue.

	In general, a real valued orthonormal matrix \textbf{cannot be diagonalized} over $\mathbb{R}$. Take, for example, the rotation matrices
	\[
	A = \begin{pmatrix}
		\cos \theta & - \sin \theta \\
		\sin \theta & \cos \theta
	\end{pmatrix}
	.\]
	Then $A$ has eigenvalues $\lambda = e^{\pm i \theta}$, which in general are not real.
\end{remark}

\subsection{Application to Bilinear Forms}
\label{sub:application_to_bilinear_forms}

We have seen in the previous section that, for self adjoint or unitary maps in a finite dimensional inner product space, we can diagonalize them over an orthonormal basis.

We can reformulate these statements in terms of bilinear forms.

\begin{corollary}
	Let $A \in \mathcal{M}_n(\mathbb{R})$ (resp. $\mathbb{C}$) be a symmetric (resp. Hermitian) matrix. Then there is an orthogonal (resp. unitary) matrix such that $P^{T}AP$ (resp. $P^{\dagger}AP$) is diagonal with real valued entries.
\end{corollary}

\begin{proofbox}
	Let $\langle \cdot, \cdot \rangle$ be the standard inner product over $\mathbb{R}^{n}$ (resp. $\mathbb{C}^{n}$). Then $A \in \mathcal{L}(\mathbb{R}^{n})$ (resp. $\mathcal{L}(\mathbb{C}^{n})$) is self adjoint, hence we can find an orthonormal basis such that $A$ is diagonal in this basis, say $(v_1, \ldots, v_n)$.

	Let $P =
	\begin{pmatrix}
		v_1 & \cdots & v_n
	\end{pmatrix}
	$. Then $P$ is orthogonal (resp. unitary), so $P^{T}P = I$ (resp. $P^{\dagger}P = I$), and we get
	\[
		PP^{T}AP (\text{or } P^{\dagger}AP) = P^{-1}AP = D =
		\begin{pmatrix}
			\lambda_1 & & 0 \\
				  & \ddots & \\
				 0 & & \lambda_n
		\end{pmatrix}
	,\]
	and we known $\lambda_i$ are real, as they are the eigenvalues of a self-adjoint operator.
\end{proofbox}

\begin{corollary}
	Let $V$ be a finite dimensional real (resp. complex) inner product space. Let $\phi : V \times V \to F$ be a symmetric (resp. Hermitian) bilinear form. Then there is an orthonormal basis of $V$ such that $\phi$ in this basis is represented by a diagonal matrix.
\end{corollary}

\begin{proofbox}
	Let $\mathcal{B}$ be any orthonormal basis of $V$, and let $A = [\phi]_{\mathcal{B}}$. Then, since $A^{T} = A$, there exists an orthogonal (resp. unitary) matrix $P$ such that $P^{T}AP$ (resp. $P^{\dagger}AP$) is a diagonal matrix $D$.

	Let $v_i$ be the $i$'th row of $P^{T}$ (resp. $P^{\dagger}$), then $(v_1, \ldots, v_n)$ form an orthonormal basis $\mathcal{B}'$ of $V$, and $[\phi]_{\mathcal{B}'} = D$.
\end{proofbox}

\begin{remark}
	The entries of $D$ are the eigenvalues of $A$. Moreover $s(A)$ is the number of positive eigenvalues of $A$, minus the number of negative eigenvalues of $A$.
\end{remark}

\begin{corollary}[Simultaneous Diagonalization]
	Let $V$ be a finite dimensional real (resp. complex) vector space. Let $\phi, \psi : V \times V \to F$, where $\phi, \psi$ are both bilinear, symmetric (resp. Hermitian) forms.

	Assuming $\phi$ is positive definite, then there exists a basis $(v_1, \ldots, v_n)$ of $V$ with respect to which both $\phi$ and $\psi$ are represented by a diagonal matrix.
\end{corollary}

\begin{proofbox}
	As $\phi$ is positive definite, it induces a scalar product on $V$, so $V$ equipped with $\phi$ is a finite dimensional inner product space:
	\[
	\langle u, v \rangle = \phi(u, v)
	.\]
	Hence there exists an orthonormal (with respect to $\phi$) basis of $V$ in which $\psi$ is represented by a diagonal matrix. Observe that $\phi$ in this basis is represented by the identity matrix, so both matrices of $\phi$ and $\psi$ in $\mathcal{B}$ are diagonal.
\end{proofbox}

\begin{corollary}
	Let $A, B \in \mathcal{M}_n(\mathbb{R})$ (resp. $\mathcal{M}_n(\mathbb{C})$) are both symmetric (resp. Hermitian). Assume for all $x \neq 0$, $\overline{x}^{T} A x > 0$. Then, there exists $Q \in \mathcal{M}_n(\mathbb{R})$ (resp. $\mathcal{M}_n(\mathbb{C})$) invertible, such that both $Q^{T}AQ$ and $Q^{T}BQ$ (resp. $Q^{\dagger}AQ$ and $Q^{\dagger}BQ$) are diagonal.
\end{corollary}

This is a direct consequence of the simultaneous diagonalization of operators.

\newpage

\printindex

\end{document}
