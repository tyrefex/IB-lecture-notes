\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[a4paper]{geometry}
\usepackage{fancyhdr}
\usepackage{tikz}
\usetikzlibrary{cd}
\usetikzlibrary{babel}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{adjustbox}
\usepackage[shortlabels]{enumitem}
\usepackage{parskip}
\makeatletter
\newcommand{\@minipagerestore}{\setlength{\parskip}{\medskipamount}}
\makeatother
\usepackage{imakeidx}

\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\nullity}{null}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\Orb}{Orb}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\ccl}{ccl}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\Syl}{Syl}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Fit}{Fit}
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\epi}{epi}


\newcommand{\incfig}[1]{%
	\def\svgwidth{\columnwidth}
	\import{./figures/}{#1.pdf_tex}
}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}
\newcommand{\Diff}[1]{\mathop{}\!\mathrm{d}^{#1}}

\setlength\parindent{0pt}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\pagestyle{fancy}
\fancyhf{}
\rhead{\leftmark}
\lhead{Page \thepage}
\setlength{\headheight}{15pt}

\makeindex[intoc]

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\newcommand{\mapsfrom}{\mathrel{\reflectbox{\ensuremath{\mapsto}}}}

\begin{document}

\hypersetup{pageanchor=false}
\begin{titlepage}
	\begin{center}
		\vspace*{1em}
		\Huge
		\textbf{IB Linear Algebra}

		\vspace{1em}
		\large
		Ishan Nath, Michaelmas 2022

		\vspace{1.5em}

		\Large

		Based on Lectures by Prof. Pierre Raphael

		\vspace{1em}

		\large
		\today
	\end{center}
	
\end{titlepage}
\hypersetup{pageanchor=true}

\tableofcontents

\newpage

\section{Vector Spaces and Subspaces}%
\label{sec:vector_spaces_and_subspaces}

Let $F$ be an arbitrary field.

\begin{definition}[$F$ vector space]\index{vector space}
	A $F$ vector space is an abelian group $(V, +)$ equipped with a function
	\begin{align*}
		F \times V &\to V \\
		(\lambda, v) &\mapsto \lambda v
	\end{align*}
	such that
	\begin{itemize}
		\item $\lambda(v_1 + v_2) = \lambda v_1 + \lambda v_2$, 
		\item $(\lambda_1 + \lambda_2)v = \lambda_1 v + \lambda_2 v$,
		\item $\lambda(\mu v) = (\lambda \mu) v$,
		\item $1 \cdot v = v$.
	\end{itemize}
\end{definition}
We know how to
\begin{itemize}
	\item Sum two vectors
	\item Multiply a vector $v \in V$ by a scalar $\lambda \in F$.
\end{itemize}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\begin{example}
	\begin{enumerate}[(i)]
		\item[]
		\item Take $n \in \mathbb{N}$, then $F^{n}$ is the set of column vectors of length $n$ with elements in $F$. We have
			\[
			v \in F^{n}, v =
			\begin{pmatrix}
				x_1 \\
				\vdots \\
				x_n
			\end{pmatrix}
			, x_i \in F
			,\]
			\[
			v + w =
			\begin{pmatrix}
				v_1 \\
				\vdots \\
				v_n
			\end{pmatrix}
			+
			\begin{pmatrix}
				w_1 \\
				\vdots \\
				w_n
			\end{pmatrix}
			=
			\begin{pmatrix}
				v_1 + w_1 \\
				\vdots \\
				v_n + w_n
			\end{pmatrix}
			,\]
			\[
			\lambda v =
			\begin{pmatrix}
				\lambda v_1 \\
				\vdots \\
				\lambda v_n
			\end{pmatrix}
			.\]
			Then $F^{n}$ is a $F$ vector space.
	\end{enumerate}
	
\end{example}

\end{adjustbox}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\begin{enumerate}[(i)]
		\setcounter{enumi}{1}
		\item For any set $X$, take
			\[
				\mathbb{R}^{X} = \{f : X \to \mathbb{R}\}
			.\]
			Then $\mathbb{R}^{X}$ is an $\mathbb{R}$ vector space.
		\item Take $M_{n, m}(F)$, the set of $n \times m$ $F$ valued matrices. Then $M_{n, m}(F)$ is a $F$ vector space.
\end{enumerate}

\end{adjustbox}

\begin{remark}
	The axiom of scalar multiplication implies that for all $v \in V$, $0 \cdot v = \mathbf{0}$.
\end{remark}

\begin{definition}[Subspace]\index{subspace}
	Let $V$ be a vector space over $F$. A subset $U$ of $V$ is a vector subspace of $V$ (denoted $U \leq V$) if
	\begin{itemize}
		\item $0 \in U$,
		\item $(u_1, u_2) \in U \times U$ implies $u_1 + u_2 \in U$,
		\item $(\lambda, u) \in F \times U$ implies $\lambda u \in U$.
	\end{itemize}
\end{definition}
Note if $V$ is an $F$ vector space, and $U \leq V$, then $U$ is an $F$ vector space.

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\begin{example}
	\begin{enumerate}[(i)]
		\item[]
		\item Take $V = \mathbb{R}^{\mathbb{R}}$, the space of functions $f : \mathbb{R} \to \mathbb{R}$. Let $\mathcal{C}(\mathbb{R})$ be the space of continuous function $f : \mathbb{R} \to \mathbb{R}$. Then $\mathcal{C}(\mathbb{R}) \leq \mathbb{R}^{\mathbb{R}}$.
		\item Take the elements of $\mathbb{R}^3$ which sum up to $t$. This is a subspace if and only if $t = 0$.
	\end{enumerate}	
\end{example}

\end{adjustbox}

Note that the union of two subspaces is generally not a subspace, as it is usually not closed under addition.

\begin{proposition}
	Let $V$ be an $F$ vector space, and $U, W \leq V$. Then $U \cap W \leq V$.
\end{proposition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} Since $0 \in U, 0 \in W$, $0 \in U \cap W$. Now consider $(\lambda, \mu) \in F^2$, and $(v_1, v_2) \in (U \cap W)^2$. Take $\lambda_1 v_1 + \lambda_2 v_2$. Since $u_1,v_1 \in U$, this is in $U$. Similarly, it is in $W$. So it is in $U \cap W$, and $U \cap W \leq V$.
\end{adjustbox}

\begin{definition}[Sum of subspaces]\index{subspace sum}
	Let $V$ be an $F$ vector space. Let $U, W \leq V$. Then the \textbf{sum} of $U$ and $W$ is the set
	\[
		U + W = \left\{ u + w \mid (u, w) \in U \times W \right\}
	.\]
\end{definition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} Note $0 = 0 + 0 \in U + W$. Take $\lambda_1 f + \lambda_2 g$, where $f, g \in U + W$. Then we can write $f = f_1 + f_2, g = g_1 + g_2$, where $f_1, g_1 \in U$, $f_2, g_2 \in W$. Then 
\[
	\lambda_1 f + \lambda_2 g = \lambda_1 (f_1 + f_2) + \lambda_2(g_1 + g_2) = (\lambda_1 f_1 + \lambda_2 g_1) + (\lambda_1 f_2 + \lambda_2 g_2) \in U + W
.\]
\end{adjustbox}

\begin{remark}
	$U + W$ is the smallest subspace of $V$ which contains both $U$ and $W$.
\end{remark}

\subsection{Subspaces and Quotients}%
\label{sub:subspaces_and_quotients}

\begin{definition}[Quotient]\index{quotient}
	Let $V$ be an $F$ vector space. Let $U \leq V$. The quotient space $V / U$ is the abelian group $V/U$ equipped with the scalar product multiplication
	\begin{align*}
		F \times V/U &\to V/U \\
		(\lambda, v + U) &\mapsto \lambda v + U
	\end{align*}
\end{definition}

\begin{proposition}
	$V/U$ is an $F$ vector space.
\end{proposition}

\newpage

\section{Spans, Linear Independence and the Steinitz Exchange Lemma}%
\sectionmark{Spans, LI and the SEL}
\label{sec:spans_linear_independence_and_the_steinitz_exchange_lemma}

\begin{definition}[Span of a family of vectors]\index{span}
	Let $V$ be a $F$ vector space. Let $S \subset B$ be a subset. We define
	\begin{align*}
		\langle S \rangle &= \left\{ \text{finite linear combinations of elements of } S \right\} \\
				  &= \left\{ \sum_{\delta \in J} \lambda_{\delta} v_{\delta},  v_{\delta} \in S,  \lambda_{\delta} \in F, J \text{ finite} \right\}.
	\end{align*}
\end{definition}

By convention, we let $\langle \emptyset \rangle = \{0\}$.

\begin{remark}
	$\langle S' \rangle$ is the smallest vector subspace which contains $S$.
\end{remark}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\begin{example}
	Take $V = \mathbb{R}^3$, and
	\[
	S = \left\{
		\begin{pmatrix}
			1 \\
			0 \\
			0
		\end{pmatrix}
		,
		\begin{pmatrix}
			0 \\
			1\\
			2
		\end{pmatrix}
		,
		\begin{pmatrix}
			3 \\
			-2 \\
			4
		\end{pmatrix}
	\right\}
	.\]
	Then we have
	\[
		\langle S' \rangle = \left\{
			\begin{pmatrix}
				a \\
				b \\
				2b
			\end{pmatrix}
			,
		(a, b) \in \mathbb{R}^2\right\}
	.\]
	Take $V = \mathbb{R}^{n}$, and let $e_i$ be the $i$'th basis vector. Then $V = \langle e_1, \ldots, e_n \rangle$.

	Take $X$ a set, and $V = \mathbb{R}^{X}$. Let $S_x : X \to \mathbb{R}$, such that $y \mapsto 1$ if $x = y$, otherwise $y \mapsto 0$. Then
	\begin{align*}
		\langle (S_x)_{x \in X} \rangle &= \{f \in \mathbb{R}^{X} \mid f \text{ has finite support}\}.
	\end{align*}
\end{example}

\end{adjustbox}

\begin{definition}
	Let $V$ be a $F$ vector space. Let $S'$ be a subset of $V$. We may say that $S$ \textbf{spans} $V$ if $\langle S \rangle = V$.
\end{definition}

\begin{definition}[Finite dimension]\index{finite dimension}
	Let $V$ be a $F$ vector space. We say that $V$ is \textbf{finite dimensional} if it is spanned by a finite set.
\end{definition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\begin{example}
	Consider $P[x]$, the polynomials over $\mathbb{R}$, and $P_n[x]$, the polynomials over $\mathbb{R}$ with degree $\leq n$. Then since
	\[
		\langle 1, x, \ldots, x^{n}\rangle = P_n[x]
	,\]
	$P_n[x]$ is finite dimensional, however $P[x]$ is not.
\end{example}

\end{adjustbox}

\begin{definition}[Independence]\index{linear independence}
	We say that $(v_1, \ldots, v_n)$, elements of $V$ are \textbf{linearly independent} if
	\[
	\sum_{i = 1}^{n} \lambda_i v_i = 0 \implies \lambda_i = 0 \, \forall i
	.\]
\end{definition}

\begin{remark}
	\begin{enumerate}[1.]
		\item[]
		\item We also say that the family $(v_1, \ldots, v_n)$ is \textbf{free}.
		\item Equivalently, $(v_1, \ldots, v_n)$ are not linearly independent if one of these vectors is a linear combination of the remaining $(n-1)$.
		\item If $(v_i)$ is free, then $v_i = 0$ for all $i$.
	\end{enumerate}
	
\end{remark}

\begin{definition}[Basis]\index{basis}
	A subset $S$ of $V$ is a \textbf{basis} of $V$ if and only if
	\begin{enumerate}[(i)]
		\item $\langle S' \rangle = V$,
		\item $S$ is linearly independent.
	\end{enumerate}
	
\end{definition}

\begin{remark}
	A subset $S$ that generates $V$ is a generating family, so a basis $S$ is a free generating family.
\end{remark}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\begin{example}
	For $V = \mathbb{R}^{n}$, then $(e_i)$ is a basis of $V$.

	If $V = \mathbb{C}$, then for $F = \mathbb{C}$, $\{1\}$ is a basis.

	If $V = P[x]$, then $S = \{x^{n}, n \geq 0\}$ is a basis for $V$.
\end{example}

\end{adjustbox}

\begin{lemma}
	$V$ is a $F$ vector space. Then $(v_1, \ldots, v_n)$ is a basis of $V$ if and only if any vector $v \in V$ has a unique decomposition
	\[
	v = \sum_{i = 1}^{n} \lambda_i v_i
	.\]
\end{lemma}

\begin{remark}
	We call $(\lambda_1, \ldots, \lambda_n)$ the coordinates of $v$ in the basis $(v_1, \ldots, v_n)$.
\end{remark}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} Since $\langle v_1, \ldots, v_n \rangle = V$, we must have 
	\[
	v = \sum_{i = 1}^{n} \lambda_i v_i
	\]
	for some $\lambda_i$. Now assume
	\begin{align*}
		v = \sum_{i = 1}^{n}\lambda_i v_i = \sum_{i = 1}^{n}\lambda_i' v_i, \\
		\implies \sum_{i = 1}^{n}(\lambda_i - \lambda_i') v_i = 0.
	\end{align*}
	Since $v_i$ are free, $\lambda_i = \lambda_i'$.
\end{adjustbox}

\begin{lemma}
	If $(v_1, \ldots, v_n)$ spans $V$, then some subset of this family is a basis of $V$.
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} If $(v_1, \ldots, v_n)$ are linearly independent, we are done. Otherwise assume they are not independent, then by possibly reordering the vectors, we have
	\[
		v_n \in \langle v_1, \ldots, v_{n-1}\rangle
	.\]
	Then we have $V = \langle v_1, \ldots, v_n \rangle = \langle v_1, \ldots, v_{n-1}\rangle$.
	By iterating, we must eventually get to an independent set.
\end{adjustbox}

\begin{theorem}[Steinitz Exchange Lemma]\index{Steinitz exchange lemma}
	Let $V$ be a finite dimensional vector space over $F$. Take
	\begin{enumerate}[\normalfont(i)]
		\item $(v_1, \ldots, v_m)$ free,
		\item $(w_1, \ldots, w_n)$ generating.
	\end{enumerate}
	Then $m \leq n$, and up to reordering, $(v_1, \ldots, v_m, w_{m+1}, \ldots, w_n)$ spans $V$.
\end{theorem}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} Induction. Suppose that we have replaced $l$ of the $w_i$, reordering if necessary, so
\[
	\langle v_1, \ldots, v_{l}, w_{l+1}, \ldots, w_n \rangle = V
.\]
If $m = l$, we are done. Otherwise, $l < m$. Then since these vectors span $V$, we have

 
\end{adjustbox}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\[
v_{l+1} = \sum_{i \leq l}a_i v_i + \sum_{i > l}\beta_i w_i
.\]
	Since $(v_1, \ldots, v_{l+1})$ is free, some of the $\beta_i$ are non-zero. Upon reordering, we may let $\beta_{l+1} \neq 0$. Then,
\[
	w_{l+1} = \frac{1}{\beta_{l+1}} \left[ v_{l+1} - \sum_{i \leq l}\alpha_i v_i - \sum_{i > l+1} \beta_i w_i \right]
.\]
Hence, $V = \langle v_1, \ldots, v_l, w_{l+1}, \ldots, w_n \rangle = \langle v_1, \ldots, v_l, v_{l+1}, w_{l+1}, \ldots, w_n \rangle = \langle v_1, \ldots, v_{l+1}, w_{l+2}, \ldots, w_n \rangle$. Iterating this process, we eventually get $l = m$, which then proves $m \leq n$.
\end{adjustbox}

\newpage

\section{Basis, Dimension and Direct Sums}%
\label{sec:basis_dimension_and_direct_sums}

\begin{corollary}
	Let $V$ be a finite dimensional vector space over $F$. Then any two bases of $V$ have the same number of vectors, called the \textbf{dimension}\index{dimension} of $V$.
\end{corollary}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} take $(v_1, \ldots, v_n), (w_1, \ldots, w_m)$ bases of $V$.
	\begin{enumerate}[(i)]
		\item As $(v_i)$ is free and $(w_i)$ is generating, $n \leq m$.
		\item As $(w_i)$ is free and $(v_i)$ is generating, $m \leq n.$
	\end{enumerate}
	So $m = n$.
\end{adjustbox}

\begin{corollary}
	Let $V$ be a vector space over $F$ with dimension $n \in \mathbb{N}$.
	\begin{enumerate}[\normalfont(i)]
		\item Any set of independent vectors has at most $n$ elements, with equality if and only if it is a basis.
		\item Any spanning set of vectors has at least $n$ elements, with equality if and only if it is a basis.
	\end{enumerate}
	
\end{corollary}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} Exercise (fill this in).
\end{adjustbox}

\begin{proposition}
	Let $U, W$ be finite dimensional subspaces of $V$. If $U$ and $W$ are finite dimensional, then so is $U + W$, and
	\[
		\dim (U + W) = \dim U + \dim W - \dim(U \cap W)
	.\]
\end{proposition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} Pick $(v_1, \ldots, v_l)$ a basis of $U \cap W$. Extend this to a basis $(v_1, \ldots, v_l, u_1, \ldots, u_m)$ of $U$, and a basis $(v_1, \ldots, v_l, w_1, \ldots, w_n)$ of $W$. Then we show $(v_1, \ldots, v_l, u_1, \ldots, u_m, w_1, \ldots, w_n)$ is a basis of $U + W$.

	It is clearly a generating family, so we will show it is free. Suppose
	\[
		\sum_{i = 1}^{l} \alpha_i v_i + \sum_{ = 1}^{m} \beta_i u_i + \sum_{i = 1}^{n} \gamma_i w_i = 0
	.\]
	Then we get
	\[
	\sum_{i = 1}^{n} \gamma_i w_i \in U \cap W
	,\]
	implying that
	\[
	\sum_{i = 1}^{l} s_i v_i = \sum_{i = 1}^{n} \gamma_i w_i
	.\]
	But since $(v_1, \ldots, w_n)$ is a basis of $W$, we get $\gamma_i = 0$. Similarly, $\beta_i = 0$. Thus,
	\[
	\sum_{i = 1}^{l} \alpha_i v_i = 0
	.\]
	Since $(v_i)$ is a basis of $U \cap W$, $\alpha_i = 0$.
\end{adjustbox}

\begin{proposition}
	Let $V$ be a finite dimensional vector space over $F$. Let $U \leq V$. Then $U$ and $V/U$ are both finite dimensional and
	\[
		\dim V = \dim U + \dim(V/U)
	.\]
\end{proposition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} Let $(u_1, u_2, \ldots, u_l)$ be a basis of $U$. Extend this to a basis $(u_1, \ldots, u_l, w_{l+1}, \ldots, w_{n})$ of $V$. Then we show that $(w_{l+1} + U, \ldots, w_{n} + U)$ is a basis of $V/U$. (Fill this in).
\end{adjustbox}

\begin{remark}
	If $U \leq V$, then we say $U$ is proper if $U \neq V$. Then for finite dimensions, $U$ proper\index{proper subspace} implies $\dim U < \dim V$, as $\dim(V/U) > 0$.
\end{remark}

\begin{definition}[Direct sum]\index{direct sum}
	Let $V$ be a vector space over $F$, and $U, W \leq V$. We say $V = U \oplus W$ if and only if any element of $v \in V$ can be uniquely decomposed as $v = u + w$ for $u \in U, w \in W$.
\end{definition}

\begin{remark}
	If $V = U \oplus W$, we say that $W$ is a complement\index{complement} of $U$ in $V$. There is no uniqueness of such a complement.
\end{remark}

In the sequel, we use the following notation. Let $\mathcal{B}_1 = \{u_1, \ldots, u_l\}$ and $\mathcal{B}_2 = \{w_1, \ldots, w_m\}$ be collections of vectors. Then
\[
	\mathcal{B}_1 \cup \mathcal{B}_2 = \{u_1, \ldots, u_l, w_1, \ldots, w_m\}
\]
with the convention that $\{v\} \cup \{v\} = \{v, v\}$.

\begin{lemma}
	Let $U, W \leq V$. Then the following are equivalent:
	\begin{enumerate}[\normalfont(i)]
		\item $V = U \oplus W$;
		\item $V = U + W$ and $U \cap W = \{0\}$;
		\item For any basis $\mathcal{B}_1$ of $U$, $\mathcal{B}_2$ of $W$, the union $\mathcal{B} = \mathcal{B}_1 \cup \mathcal{B}_2$ is a basis of $V$.
	\end{enumerate}
\end{lemma}


\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} We show (ii) implies (i). Let $V = U + W$, then clearly $U, W$ generate $V$. We only need to show uniqueness. Suppose $u_1 + w_1 = u_2 + w_2$. Then
	\[
		u_1 - u_2 = w_2 - w_1 \in U \cap W = \{0\}
	.\]
	Hence $u_1 = u_2$ and $w_1 = w_2$, as required.

	Now we show (i) implies (iii). Let $\mathcal{B}_1$ be a basis of $U$, and $\mathcal{B}_2$ a basis of $W$. Then $\mathcal{B} = \mathcal{B}_1 \cup \mathcal{B}_2$ generates $U + W = V$, and $\mathcal{B}$ is free, as if $\sum \lambda_i v_i = u + w = 0$, then $0 = 0 + 0$ uniquely, so $u = 0, w = 0$, giving $\lambda_i = 0$ for all $i$.

	Finally, we show (iii) implies (ii). Let $\mathcal{B} = \mathcal{B}_1 \cup \mathcal{B}_2$. Then since $\mathcal{B}$ is a basis of $V$,
	\[
	v = \sum_{u_i \in \mathcal{B}_1}\lambda_i u_i + \sum_{w_i \in \mathcal{B}_2}\lambda_i w_i = u + w
	.\]
	Now if $v \in U \cap W$,
	\[
	v = \sum_{u \in \mathcal{B}_1} \lambda_u u = \sum_{w \in \mathcal{B}_2} \lambda_w w
	.\]
	This gives
	\[
	\sum_{u \in \mathcal{B}_1}\lambda_u u - \sum_{w \in \mathcal{B}_2} \lambda_w w = 0
	.\]
	Since $\mathcal{B}_1 \cup \mathcal{B}_2$ is free, we get $\lambda_u = \lambda_w = 0$, so $U \cap W = \{0\}$.
\end{adjustbox}

\begin{definition}
	Let $V$ be a vector space over $F$, and $V_1, \ldots, V_l \leq V$. Then
	\begin{enumerate}[\normalfont(i)]
		\item The sum of the subspaces is
			\[
				\sum_{i = 1}^{l} V_i = \{v_1 + \cdots + v_l \mid v_j \in V_J, 1 \leq j \leq l \}
			.\]
		\item The sum is direct:
			\[
			\sum_{i = 1}^{l} V_i = \bigoplus_{i = 1}^{l} V_i
			\]
			if and only if
			\[
			v_1 + \cdots + v_l = v_1' + \cdots + v_l' \implies v_1 = v_1', \ldots, v_l = v_l'
			.\]
	\end{enumerate}
	
\end{definition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} Exercise.
\end{adjustbox}

\begin{proposition}
	The following are equivalent:
	\begin{enumerate}[\normalfont(i)]
		\item
			\[
			\sum_{i = 1}^{l} V_i = \bigoplus_{i = 1}^{l} V_i
			,\]
		\item 
			\[
				\forall i, V_i \cap \left( \sum_{j < i} V_i \right) = \{0\}
			,\]
		\item For any basis $\mathcal{B}_i$ of $V_i$,
			\[
				\mathcal{B} = \bigcup_{i = 1}^{l} \mathcal{B}_i \text{ is a basis of } \sum_{i = 1}^{l} V_l
			.\]
	\end{enumerate}
	
\end{proposition}

\newpage

\section{Linear maps, Isomorphisms and the Rank-Nullity Theorem}%
\sectionmark{Linear maps, Isomorphism and Rank-Nullity}
\label{sec:linear_maps_isomorphism_and_the_rank_nullity_theorem}

\begin{definition}[Linear map]\index{linear map}
	Let $V, W$ be vector spaces over $F$. A map $\alpha : V \to W$ is \textbf{linear} if and only if for all $\lambda_1, \lambda_2 \in F$ and $v_1, v_2 \in V$, we have
	\[
		\alpha(\lambda_1 v_1 + \lambda_2 v_2) = \lambda_1 \alpha(v_1) + \lambda_2 \alpha(v_2)
	.\]
\end{definition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\begin{example}
	\begin{enumerate}[(i)]
		\item[]
		\item Take an $m \times n$ matrix $M$, Then we can take the linear map $\alpha : \mathbb{R}^{m} \to \mathbb{R}^{n}$ defined by $X \mapsto M X$.
		\item Take the linear map $\alpha : \mathcal{C}[0, 1] \to \mathcal{C}[0, 1]$ by
			\[
				f \mapsto \alpha(f)(x) = \int_{0}^{x}f(t)\diff t
			.\]
		\item Fix $x \in [a, b]$. Then we can take a linear map $\mathcal{C}[a, b] \to \mathbb{R}$ by $f \mapsto f(x)$.
	\end{enumerate}
	
\end{example}

\end{adjustbox}

\begin{remark}
	Let $U, V, W$ be $F$-vector spaces.
	\begin{enumerate}[(i)]
		\item The identity map $\id_V : V \to V$ by $x \mapsto x$ is a linear map.
		\item If $U \to V$ is $\beta$ linear, and $V \to W$ is $\alpha$ linear, then $U \to W$ is linear by $\alpha \circ \beta$.
	\end{enumerate}
\end{remark}

\begin{lemma}
	Let $V, W$ be $F$-vector spaces, and $\mathcal{B}$ a basis of $V$. Let $\alpha_0 : \mathcal{B} \to W$ be any map, then there is a unique linear map $\alpha : V \to W$ extending $\alpha_0$.
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} For $v \in V$, we can write
\[
v = \sum_{i = 1}^{n} \lambda_i v_i
,\]
where $\mathcal{B} = (v_1, \ldots, v_n)$. Then by linearity, we must have
\[
	\alpha(v) = \alpha \left( \sum_{i = 1}^{n} \lambda_i v_i \right) = \sum_{i = 1}^{}\lambda_i \alpha_0 (v_i)
.\]
This is unique as $\mathcal{B}$ is a basis.
\end{adjustbox}

\begin{remark}
	This is true in infinite dimensions as well.
\end{remark}

Often, to define a linear map, we define its value on a basis and extend by linearity. As a corollary, if $\alpha_1, \alpha_2 : V \to W$ are linear and agree on a basis of $V$, they are equal.

\begin{definition}[Isomorphism]\index{isomorphism}
	Let $V, W$ be vector spaces over $F$. A map $\alpha : V \to W$ is called an \textbf{isomorphism} if and only if $\alpha$ is linear and bijective. If such an $\alpha$ exists, we say $V \cong W$.
\end{definition}

\begin{remark}
	If $\alpha : V \to W$ is an isomorphism, then $\alpha^{-1} : W \to V$ is linear. Indeed, for $w_1, w_2 \in W \times W$, let $w_1 = \alpha(v_1), w_2 = \alpha(v_2)$. Then,
	\begin{align*}
		\alpha^{-1}(\lambda_1 w_1 + \lambda_2 w_2) &= \alpha^{-1}(\lambda_1 \alpha(v_1) + \lambda_2 \alpha(v_2)) \\
							   &= \alpha^{-1} ( \alpha(\lambda_1 v_1 + \lambda_2 v_2) ) \\
							   &= \lambda_1 v_1 + \lambda_2 v_2 \\
							   &= \lambda_1 \alpha^{-1}(v_1) + \lambda_2 \alpha^{-1}(v_2).
	\end{align*}
\end{remark}

\begin{lemma}
	Congruence is an equivalence relation on the class of all vector spaces of $F$:
	\begin{enumerate}[\normalfont(i)]
		\item $\id_V : V \to V$ is an isomorphism.
		\item $\alpha : V \to W$ is an isomorphism implies $\alpha^{-1} : W \to V$ is an isomorphism.
		\item If $\alpha : U \to V$ is an isomorphism, $\beta : V \to W$ is an isomorphism, then $\beta \circ \alpha : U \to W$ is an isomorphism.
	\end{enumerate}
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} Exercise.
\end{adjustbox}

\begin{theorem}
	If $V$ is a vector space over $F$ of dimension $n$, then $V \cong F^{n}$.
\end{theorem}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} Let $\mathcal{B} = (v_1, \ldots, v_n)$ be a basis of $V$. Then take
	\begin{align*}
		\alpha : V &\to F^{n} \\
		v = \sum_{i = 1}^{n} \lambda_i v_i &\mapsto 
		\begin{pmatrix}
			\lambda_1 \\
			\vdots \\
			\lambda_n
		\end{pmatrix}
	\end{align*}
	as an isomorphism.
\end{adjustbox}

\begin{remark}
	In this way, choosing a basis of $V$ is like choosing an isomorphism from $V$ to $F^{n}$.
\end{remark}

\begin{theorem}
	Let $V, W$ be vector spaces over $F$ with finite dimension. Then $V \cong W$ if and only if $\dim V = \dim W$.
\end{theorem}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} If $\dim V = \dim W$, then $V \cong F^{n} \cong W$, so $V \cong W$.

Otherwise, let $\alpha : V \to W$ be an isomorphism, and $\mathcal{B}$ a basis of $V$. Then we show $\alpha(\mathcal{B})$ is a basis of $W$.
\begin{itemize}
	\item $\alpha(\mathcal{B})$ spans $V$ from the surjectivity of $\alpha$.
	\item $\alpha(\mathcal{B})$ is free from the injectivity of $\alpha$.
\end{itemize}
Hence $\dim V = \dim W$.

\end{adjustbox}

\begin{definition}[Kernal and Image]\index{kernel}\index{image}
	Let $V, W$ be vector spaces over $F$. Let $\alpha : V \to W$ be a linear map. We define
	\begin{enumerate}[(i)]
	\item $\Ker \alpha = \{v \in V \mid \alpha(v) = 0\}$, the kernel of $\alpha$.
	\item  $\Img(\alpha = \{w \in W \mid \exists v \in V, \alpha(v) = w\}$, the image of $\alpha$.
	\end{enumerate}
\end{definition}

\begin{lemma}
	$\Ker \alpha$ is a subspace of $V$, and $\Img \alpha$ is a subspace of $W$.
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} Let $\lambda_1, \lambda_2 \in F$, and $v_1, v_2 \in \Ker \alpha$. Then
\[
	\alpha (\lambda_1 v_1 + \lambda_2 v_2) = \lambda_1 \alpha(v_1) + \lambda_2 \alpha(v_2) = 0
.\]
So $\lambda_1 v_1 + \lambda_2 v_2 \in \Ker \alpha$.

Now if $w_1 = \alpha(v_1), w_2 = \alpha (v_2)$, then
 \[
	 \lambda_1 w_1 + \lambda_2 w_2 = \lambda_1 \alpha(v_1) + \lambda_2 \alpha(v_2) = \alpha(\lambda_1 v_1 + \lambda_2 v_2)
.\]
Hence $\lambda_1 w_1 + \lambda_2 w_2 \in \Img \alpha$.
\end{adjustbox}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\begin{example}
	Consider $\alpha : \mathcal{C}^{\infty}(\mathbb{R}) \to \mathcal{C}^{\infty}(\mathbb{R}),$ given by
	\[
		f \mapsto \alpha(f) = f'' + f
	.\]
	Then $\alpha$ is linear, and
	\[
		\Ker \alpha = \{f \in \mathcal{C}^{\infty}(\mathbb{R}) \mid f'' + f = 0\} = \langle \sin t, \cos t \rangle
	.\]
\end{example}

\end{adjustbox}

\begin{remark}
	If $\alpha : V \to W$ is linear, then $\alpha$ is injective if and only if $\Ker \alpha = \{0\}$, as
	\[
		\alpha(v_1) = \alpha(v_2) \iff \alpha(v_1 - v_2) = 0
	.\]
\end{remark}

\begin{theorem}
	Let $V, W$ be vector spaces over $F$, and $\alpha : V \to W$ linear. Then
	\begin{align*}
		V/\Ker \alpha &\to \Img \alpha \\
		v + \Ker \alpha &\mapsto \alpha(v)
	\end{align*}
	is an isomorphism.
\end{theorem}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} We proceed in steps.
\begin{itemize}
	\item $\bar \alpha$ is well defined: Note if $v + \Ker \alpha = v' + \Ker \alpha$, then $v - v' \in \Ker \alpha$, so $\alpha(v - v') = 0$. Hence $\alpha(v) = \alpha(v')$.
	\item $\bar \alpha$ is linear: This follows from linearity of $\alpha$.
	\item $\bar \alpha$ is a bijection: First, if $\bar \alpha (v + \Ker \alpha) = 0$, then $\alpha(v) = 0$, so $v \in \Ker \alpha$, hence $v + \Ker \alpha = 0 + \Ker \alpha$, so $\alpha$ is injective. Then $\bar \alpha$ is surjective from the definition of the image.
\end{itemize}

\end{adjustbox}

\begin{definition}[Rank and Nullity]\index{rank}\index{nullity}
	We define the rank $r(\alpha) = \rank(\alpha) = \dim \Img \alpha$, and the nullity $n(\alpha) = \nullity(\alpha) = \dim \Ker \alpha$.
\end{definition}

\begin{theorem}[Rank-nullity theorem]\index{rank-nullity theorem}
	Let $U, V$ be vector spaces over $F$, with $\dim U < \infty$, and let $\alpha : U \to V$ be a lninear map. Then,
	\[
		\dim U = r(\alpha) + n(\alpha)
	.\]
\end{theorem}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} We have proven that $U/Ker \alpha \cong \Img \alpha$, but we have already proven $\dim U/Ker \alpha = \dim U - r(\alpha)$, which proves the theorem.
\end{adjustbox}

\begin{lemma}
	Let $V, W$ be vector spaces over $F$ of equal finite dimension. Let $\alpha : V \to W$ be a linear map. Then the following are equivalent:
	\begin{itemize}
		\item $\alpha$ is injective,
		\item $\alpha$ is surjective,
		\item $\alpha$ is an isomorphism.
	\end{itemize}
\end{lemma}

This follows immediately from the rank-nullity theorem.

\newpage

\section{Linear maps and Matrices}%
\label{sec:linear_maps_and_matrices}

\begin{definition}
	If $V, W$ are vector spaces over $F$, then
	\[
		L(V, W) = \{\alpha : V \to W \text{ linear}\}
	.\]
\end{definition}

\begin{proposition}
	$L(V, W)$ is a vector space over $F$ with
	\[
		(\alpha_1 + \alpha_2)(v) = \alpha_1(v) + \alpha_2(v)
	,\]
	\[
		(\lambda \alpha)(v) = \lambda \alpha 9v)
	.\]
	Moreover, if $V$ and $W$ are finite dimensional, then so is $L(V, W)$, and
	\[
		\dim L(V, W) = \dim V \dim W
	.\]
\end{proposition}

\begin{definition}
	An $m \times n$ matrix\index{matrix} over $F$ is an array with $m$ rows and $n$ columns with entries in $F$, $A = (a_{ij})$. Define
	\[
		M_{m, n}(F) = \{\text{set of } m \times n \text{ matrices over } F\}
	.\]
\end{definition}

\begin{proposition}
	$M_{m, n}(F)$ is a vector space over $F$, and $\dim M_{m, n}(F) = mn$
\end{proposition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} Let $E_{ij}$ be the matrix with $a_{xy} = \delta_{xi}\delta_{yj}$. Then $(E_{ij})$ is a basis of $M_{m, n}(F)$, as
	\[
		N = (a_{ij}) = \sum_{i, j}a_{ij} E_{ij}
	,\]
	and $(E_{ij})$ is free.
\end{adjustbox}

If $V, W$ are vector spaces over $F$, and $\alpha : V \to W$ is a linear map, we take a basis $\mathcal{B} = (v_1, \ldots, v_n)$ of $V$, and $\mathcal{C} = (w_1, \ldots, w_m)$ of $W$. Let $v \in V$, then
\[
v = \sum_{i = 1}^{n} \lambda_i v_i \sim
\begin{pmatrix}
	\lambda_1 \\
	\vdots \\
	\lambda_n
\end{pmatrix}
 \in F^{n}
.\]
We let this isomorphism from $V$ to $F^{n}$ be $[v]_{\mathcal{B}}$. Similarly, we can obtain $[w]_{\mathcal{B}}$ for $w \in W$.

\begin{definition}
	We define a matrix of $\alpha$ with respect to a basis $\mathcal{B}, \mathcal{C}$ as
	\[
		[\alpha]_{\mathcal{B}, \mathcal{C}} = ([\alpha(v_1)]_{\mathcal{C}}, [\alpha(v_2)]_{\mathcal{C}}, \ldots, [\alpha(v_n)]_{\mathcal{C}})
	.\]
\end{definition}

By definition, if $[\alpha]_{\mathcal{B}, \mathcal{C}} = (a_{ij})$, then
\[
	\alpha(v_j) = \sum_{i = 1}^{m} a_{ij} w_i
.\]

\begin{lemma}
	If $v \in V$, then
	\[
		[\alpha(v)]_{\mathcal{C}} = [\alpha]_{\mathcal{B}, \mathcal{C}} \cdot [v]_{\mathcal{B}}
	,\]
	or equivalently,
	\[
		(\alpha (v))_{i} = \sum_{j = 1}^{n} a_{ij} \lambda_j
	.\] 
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} Let $v \in V$, then
\[
v = \sum_{j = 1}^{n} \lambda_j v_j
.\]
Then
 \begin{align*}
	 \alpha(v) &= \alpha \left( \sum_{j = 1}^{n} \lambda_j v_j \right) = \sum_{j = 1}^{n} \lambda_j \alpha(v_j) \\
		   &= \sum_{j = 1}^{n} \lambda_j \sum_{i = 1}^{n} a_{ij} w_i = \sum_{i = 1}^{m} \left( \sum_{j = 1}^{n} a_{ij} \lambda_j \right) w_i.
\end{align*}

\end{adjustbox}

\begin{lemma}
	If $U \to V$ is linear under $\beta$, $V \to W$ linear under $\alpha$, then $U \to W$ is linear under $\alpha \to W$. Let $\mathcal{A}$ be a basis of $U$, $\mathcal{B}$ a basis of $V$, and $\mathcal{C}$ a basis of $W$. Then
	\[
		[\alpha \circ \beta]_{\mathcal{A}, \mathcal{C}} = [\alpha]_{\mathcal{B}, \mathcal{C}} \cdot [\beta]_{\mathcal{A}, \mathcal{B}}
	.\] 
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} Let $A = [\alpha]_{\mathcal{B}, \mathcal{C}}$, $B = [\beta]_{\mathcal{A}, \mathcal{B}}$. Pick $u_l \in A$. Then
	\begin{align*}
		(\alpha \circ \beta)(u_l) &= \alpha(\beta(u_l)) = \alpha\left( \sum_{j} b_{jl} v_j \right) \\
					  &= \sum_{j} b_{jl} \alpha(v_j) = \sum_{j}b_{jl}\sum_{i}a_{ij}w_i \\
					  &= \sum_{i}\left( \sum_{j} a_{ij} b_{jl} \right) w_i.
	\end{align*}
\end{adjustbox}

\begin{proposition}
If $V$ and $W$ are vector spaces over $F$, and $\dim V = n$, $\dim W = m$, then $L(V, W) \cong M_{m, n}(F)$, so $\dim L(V, W) = m \times n$.
\end{proposition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} Fix $\mathcal{B}, \mathcal{C}$ bases of $V$ and $W$. We show
\begin{align*}
	\theta : L(V, W) &\to M_{m, n}(F) \\
	\alpha &\mapsto [\alpha]_{\mathcal{B}, \mathcal{C}}
\end{align*}
is an isomorphism.
\begin{itemize}
	\item $\theta$ is linear:
		$[\lambda_1 \alpha_1 + \lambda_2 \alpha_2]_{\mathcal{B}, \mathcal{C}} = \lambda_1 [\alpha_1]_{\mathcal{B}, \mathcal{C}} + \lambda_2[\alpha_2]_{\mathcal{B}, \mathcal{C}}$.
	\item $\theta$ is surjective: Consider $A = (a_{ij})$. Consider the map
		\[
		\alpha : v_j \mapsto \sum_{i = 1}^{m} a_{ij}w_i
		.\]
		This can be extended by linearity, and $[\alpha]_{\mathcal{B}, \mathcal{C}} = A$.
	\item $\theta$ is injective: If $[\alpha]_{\mathcal{B}, \mathcal{C}} = 0$, then $\alpha = 0$ for all $v$.
\end{itemize}

\end{adjustbox}

\begin{remark}
	If $\mathcal{B}, \mathcal{C}$ are bases of $V, W$ and $\varepsilon_{\mathcal{B}} : v \mapsto [v]_{\mathcal{B}}$, $\varepsilon_{\mathcal{C}} : w \mapsto [w]_{\mathcal{C}}$, then the following diagram commutes:
	\[
		\begin{tikzcd}
			V \arrow[r, "\alpha"] \arrow[d, "\varepsilon_{\mathcal{B}}"] & W \arrow[d, "\varepsilon_{\mathcal{C}}"] \\
			F^{n} \arrow[r, "{[\alpha]_{\mathcal{B},\mathcal{C}}}"] & F^{m}
		\end{tikzcd}
	\] 
\end{remark}

\newpage

\section{Change of Basis and Equivalent Matrices}%
\label{sec:change_of_basis_and_equivalent_matrices}

Let $\alpha : V \to W$ with $\mathcal{B}$ and $\mathcal{C}$ bases of $V, W$. Then
\[
	[\alpha(v)]_{\mathcal{C}} = [\alpha]_{\mathcal{B}, \mathcal{C}} \cdot [v]_{\mathcal{B}}
.\]
If $Y \leq V$, we can take $\mathcal{B}$ a basis of $V$, such that $(v_1, \ldots, v_k, v_{k+1}, \ldots, v_n)$ is a basis of $V$, and $(v_1, \ldots, v_k)$ is a basis $\mathcal{B}'$ of $Y$, and $(v_{k+1}, \ldots, v_n)$ is a basis $\mathcal{B}''$.

Then if $Z \leq W$, we can take a basis $\mathcal{C}$ of $W$ $(w_1, \ldots, w_l, w_{l+1}, \ldots, w_m)$, such that $(w_1, \ldots, w_l)$ is a basis $\mathcal{C}'$ of $Z$, and $(w_{l+1}, \ldots, w_m)$ is a basis $\mathcal{C}''$. Then
\[
	[\alpha]_{\mathcal{B}, \mathcal{C}} =
	\begin{pmatrix}
		A & B \\
		0 & C
	\end{pmatrix}
.\]
Then we can show that
\[
	A = [\alpha|_{Y}]_{\mathcal{B}', \mathcal{C}'}
,\]
if $\alpha(Y) \leq Z$. Moreover, we can show $\alpha$ induces a homomorphism
\begin{align*}
	\bar \alpha : V / Y &\to W / Z \\
	v + Y &\mapsto \alpha(v) + Z
\end{align*}
This is well-defined as $\alpha(v) \in Z$ for $v \in Y$, and $[\bar \alpha]_{\mathcal{B}'', \mathcal{C}''} = C$.

\subsection{Change of Basis}%
\label{sub:change_of_basis}

Consider $\alpha : V \to W$, where $V$ has two bases $\mathcal{B} = \{v_1, \ldots, v_n\}$ and $\mathcal{B}' = \{v_1', \ldots, v_n'\}$ and $W$ has two bases $\mathcal{C} = \{w_1, \ldots, w_n\}$ and $\mathcal{C}' = \{w_1', \ldots, w_m'\}$. We aim to find the relation between $[\alpha]_{\mathcal{B}, \mathcal{C}}$ and $[\alpha]_{\mathcal{B}', \mathcal{C}'}$.

\begin{definition}
	The change of basis matrix\index{change of basis matrix} from $\mathcal{B}'$ to $\mathcal{B}$ is $P = (p_{ij})$ given by
	\[
		P = ([v_1']_{\mathcal{B}}, \ldots, [v_n']_{\mathcal{B}}) = [\id]_{\mathcal{B}', \mathcal{B}}
	.\]
\end{definition}

\begin{lemma}
	$[v]_{\mathcal{B}} = P[v]_{\mathcal{B}'}$.
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} In general $[\alpha(v)]_{\mathcal{C}} = [\alpha]_{\mathcal{B}, \mathcal{C}} [v]_{\mathcal{B}}$. If $P = [\id]_{\mathcal{B}', \mathcal{B}}$, then
	\[
		[v]_{\mathcal{B}} = [\id(v)]_{\mathcal{B}} = [\id]_{\mathcal{B}', \mathcal{B}} [v]_{\mathcal{B}'} = P[v]_{\mathcal{B}'}
	.\]
\end{adjustbox}

\begin{remark} 
	$P$ is an $n \times n$ invertible matrix, and $P^{-1}$ is the change of basis matrix from $B$ to $B'$. Indeed,
	\[
		[\id]_{\mathcal{B}, \mathcal{B}'} [\id]_{\mathcal{B}', \mathcal{B}} = [\id]_{\mathcal{B}', \mathcal{B}'} = \id
	,\]
	and similarly.
\end{remark}

Note while we know $[v]_{\mathcal{B}} = P [v]_{\mathcal{B}'}$, to compute a vector in $\mathcal{B}'$, we have $[v]_{\mathcal{B}'} = P^{-1}[v]_{\mathcal{B}}$. This is hard to do.

Similarly, we can also change basis $\mathcal{C}$ to $\mathcal{C}'$ in $W$. In this case, the change of basis matrix $Q = [\id]_{\mathcal{C}', \mathcal{C}}$ is $m \times m$ and invertible.

Now given $\alpha : V \to W$, we wish to find how $[\alpha]_{\mathcal{B}, \mathcal{C}}$ and $[\alpha]_{\mathcal{B}', \mathcal{C}'}$.

\begin{proposition}
	If $A = [\alpha]_{\mathcal{B}, \mathcal{C}}$, $A' = [\alpha]_{\mathcal{B}', \mathcal{C}'}$, $P = [\id]_{\mathcal{B}', \mathcal{B}}$, $Q = [\id]_{\mathcal{C}'}, \mathcal{C}$, then
	\[
	A' = Q^{-1}AP
	.\]
\end{proposition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} Combining the facts we know, we get
\[
	[\alpha(v)]_{\mathcal{C}} = Q[\alpha(v)]_{\mathcal{C}'} = Q[a]_{\mathcal{B}', \mathcal{C}'} [v]_{\mathcal{B}'} = Q A' [v]_{\mathcal{B}'}.
\]
But we also know
\[
	[\alpha(v)]_{\mathcal{C}} = [\alpha]_{\mathcal{B}, \mathcal{C}}[v]_{\mathcal{B}} = A P [v]_{\mathcal{B}'}
.\]
But since this is true for any $v \in V$, we get $QA' = AP$, so $A' = Q^{-1}AP$.
\end{adjustbox}

\begin{definition}[Equivalent matrices]\index{equivalent matrices}
	Two matrices $A, B \in M_{m, n}(F)$ are equivalent if $A' = Q^{-1}AP$, where $Q \in M_{m, m}$ and $P \in M_{n, n}$ are invertible.
\end{definition}

\begin{remark}
	This defines an equivalence relation on $M_{m, n}(F)$, as
	\begin{itemize}
		\item $A = I_m^{-1}AI_n$,
		\item If $A' = Q^{-1}AP$, then $A = (Q^{-1})^{-1}A' P^{-1}$,
		\item If $A' = Q^{-1}AP$, $A'' = (Q')^{-1}A'P'$, then $A'' = (QQ')^{-1}A(PP')$.
	\end{itemize}
	
\end{remark}

\begin{proposition}
	Let $V, W$ be vector spaces over $F$, with $\dim_F V = n$, $\dim_F W = m$. Let $\alpha : V \to W$ be a linear map. Then there exists $\mathcal{B}$, $\mathcal{C}$ bases of $V, W$ such that
	\[
		[\alpha]_{\mathcal{B}, \mathcal{C}} =
		\begin{pmatrix}
			I_r & 0 \\
			0 & 0
		\end{pmatrix}
	.\]
\end{proposition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} Choose $\mathcal{B}$ and $\mathcal{C}$ wisely. Fix $r \in \mathbb{N}$ such that $\dim \Ker \alpha = n - r$. Let $N(\alpha) = \Ker(\alpha) = \{x \in V \mid \alpha(x) = 0\}$. Fix any basis of $N(x)$, $(v_{r+1}, \ldots, v_n)$, and extend it to a basis $\mathcal{B} = (v_1, \ldots, v_r, v_{r+1}, \ldots, v_n)$.

	We claim that $(\alpha(v_1), \ldots, \alpha(v_r))$ is a basis of $\Img \alpha$.
	\begin{itemize}
		\item First, if $v = \sum \lambda_i v_i$, then
			\[
				\alpha(v) = \sum_{i = 1}^{n} \lambda_i \alpha(v_i) = \sum_{i = 1}^{r} \lambda_i \alpha(v_i)
			.\]
			Let $y \in \Img \alpha$, so then
			\[
				y = \sum_{i = 1}^{r} \lambda_i \alpha(v_i)
			.\]
			So $y \in \langle \alpha(v_1), \ldots, \alpha(v_r)\rangle$.
		\item Now, suppose that it is not free, so
			\[
				\sum_{i = 1}^{r} \lambda_i \alpha(v_i) = 0
			.\]
			Then we get
			\[
				\alpha\left( \sum_{i = 1}^{r} \lambda_i v_i \right) = 0
			,\]
			so
			\[
			\sum_{i = 1}^{r} \lambda_i v_i \in \Ker \alpha
			.\]
			Hence, we get that
			\[
			\sum_{i = 1}^{r} \lambda_i v_i = \sum_{i = 1}^{n} \mu_i v_i
			.\]
			But since $(v_1, \ldots, v_n)$ is a basis, $\lambda_i = \mu_i = 0$.
	\end{itemize}
	So we have $(\alpha(v_1), \ldots, \alpha(v_r))$ is a basis of $\Img \alpha$, and $(v_{r+1}, \ldots, v_{n})$ is a basis of $\Ker \alpha$. Let $\mathcal{C} = (\alpha(v_1), \ldots, \alpha(v_r), w_{r+1}, \ldots, w_{m})$. We get that
	\[
		[\alpha]_{\mathcal{B}, \mathcal{C}} = (\alpha(v_1), \ldots, \alpha(v_r), \alpha(v_{r+1}), \ldots, \alpha(v_n)) =
		\begin{pmatrix}
			I_r & 0 \\
			0 & 0
		\end{pmatrix}	
	.\]
\end{adjustbox}

\begin{remark}
	This proves another proof of the rank-nullity theorem: $r(\alpha) + n(\alpha) = n$.
\end{remark}

\begin{corollary}
	Any $m \times n$ matrix is equivalent to
	\[
	\begin{pmatrix}
		I_r & 0 \\
		0 & 0
	\end{pmatrix}
	,\]
	where $r = \rank(\alpha)$.
\end{corollary}

\begin{definition}
	For $a \in M_{m, n}(F)$, the column rank\index{column rank} $r_c(A)$ of $A$ is the dimension of the span of the column vectors of $A$ in $F^{m}$. Similarly, the row rank\index{row rank} is the column rank of $A^{T}$.
\end{definition}

\begin{remark}
	If $\alpha$ is a linear map represented by $A$ with respect to one basis, the column rank $A$ equals the rank of $\alpha$.
\end{remark}

\begin{proposition}
	Two matrices are equivalent if and only if $r_c(A) = r_c(A')$.
\end{proposition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} If $A$ and $A'$ are equivalent then they coorespond to the same linear map $\alpha$ except in two different bases.

Conversely, if $r_c(A) = r_c(A') = r$, then both $A$ and $A'$ are equivalent to
\[
\begin{pmatrix}
	I_r & 0 \\
	0 & 0
\end{pmatrix}
,\]
hence are equivalent.
\end{adjustbox}

\begin{theorem}
	$r_c(A) = r_c(A^{T})$, so column rank equals row rank.
\end{theorem}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} If $r = r_c(A)$, then
	\[
	Q^{-1}AP =
	\begin{pmatrix}
		I_r & 0 \\
		0 & 0
	\end{pmatrix}
	.\]
	Take the transpose, to get
	\[
		(Q^{-1}AP)^{T} = P^{T}A^{T}(Q^{-1})^{T} = P^{T}A^{T}(Q^{T})^{-1} =
		\begin{pmatrix}
			I_r & 0 \\
			0 & 0
		\end{pmatrix}
	.\]
	Hence $r_c(A^{T}) = r = r_c(A)$.
\end{adjustbox}

\newpage

\section{Elementary operations and Elementary Matrices}%
\label{sec:elementary_operations_and_elementary_matrices}

This is a special case of the change of basis formula, when $\alpha : V \to V$ is a map from a vector space to itself, called an endomorphism.\index{endomorphism} Suppose $\mathcal{B} = \mathcal{C}$ and $\mathcal{B}' = \mathcal{C}'$, and $P$ is the change of basis matrix from $\mathcal{B}'$ to $\mathcal{B}$. Then
\[
	[\alpha]_{\mathcal{B'},  \mathcal{B}'} = P^{-1} [\alpha]_{\mathcal{B}, \mathcal{B}} P
.\]

\begin{definition}
	Let $A, A'$ be $n \times n$ matrices. We say that $A$ and $A'$ are similar\index{similar matrices} if and only if $A' = P^{-1}AP$ for a square invertible matrix $P$.
\end{definition}

\begin{definition}
	The elementary column operations\index{elementary column operation} on an $m \times n$ matrix $A$ are:
	\begin{enumerate}[(i)]
		\item Swap columns $i$ and $j$;
		\item Replace column $i$ by $\lambda$ times column $i$;
		\item Add $\lambda$ times column $i$ to column $j$, for $i \neq j$.
	\end{enumerate}
	The elementary row operations are analogously defined\index{elementary row operation}.
\end{definition}

Note elementary operations are invertible, and all operations can be realized through the action of elementary matrices\index{elementary matrices}:
\begin{enumerate}[(i)]
	\item For swapping columns $i$ and $j$, we can take an identity matrix, but with $a_{ij} = a_{ji} = 1$, and $a_{ii} = a_{jj} = 0$.
	\item For multiplying column $i$ by $\lambda$, we can take an identity matrix but with $a_{ii} = \lambda$.
	\item For adding $\lambda$ times columns $i$ to column $j$, we can take an identity matrix but with $a_{ij} = \lambda$.
\end{enumerate}
An elementary columns (resp. row) operation can be done by multiplying $A$ by the corresponding elementary matrix from the right (resp. left).

We will now show that any $m \times n$ matrix is equivalent to
\[
\begin{pmatrix}
	I_r & 0 \\
	0 & 0
\end{pmatrix}
.\]
Start with a matrix $A$. If all entries are zero, we are done. Otherwise, pick $a_{ij} = \lambda \neq 0$. By swapping columns and rows, we can ensure $a_{11} = \lambda$. Multiplying column 1 by $1/\lambda$, we get $a_{11} = 1$. We can then clean out row 1 by subtracting a suitable multiply of column 1 from every row, and similarly from column 1. This gives us a matrix
\[
\begin{pmatrix}
	1 & 0 & \cdots & 0 \\
	0 & & & \\
	\vdots & & \tilde A & \\
	0 & & &
\end{pmatrix}
.\]
Iterating with $\tilde A$, a strictly smaller matrix, eventually gives
\[
\begin{pmatrix}
	I_r & 0 \\
	0 & 0
\end{pmatrix} = Q^{-1}AP
.\]
A variation of this is known as \textbf{Gauss' pivot algorithm}\index{Gauss' pivot algorithm}. If we only use row operations, we can reach the row-echelon form of the matrix\index{row echelon form}:
\begin{itemize}
	\item Assume that $a_{i1} \neq 0$ for some $i$.
	\item Swap rows $i$ and $1$.
	\item Divide first row by $\lambda = a_{i1}$.
	\item Use $1$ in $a_{11}$ to clean the first column.
	\item Iterate over all columns.
\end{itemize}
This procedure is what is usually done when solving a system of linear equations.

\subsection{Representation of Square Invertible Matrix}%
\label{sub:representation_of_square_invertible_matrix}

\begin{lemma}
	If $A$ is an $n \times n$ square invertible matrix, then we can obtain $I_n$ using either only row or column elementary operations.
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} We prove for column operations; row operations are analogous. We proceed by induction on the number of rows.
\begin{itemize}
	\item Suppose that we could write $A$ in the form
		\[
		\begin{pmatrix}
			I_h & 0 \\
			\ast & \ast
		\end{pmatrix}
		.\]
		Then we want to obtain the same structure as we go from $h$ to $h + 1$.
	\item We show there exists $j > h$ such that $\lambda = a_{h+1, j} \neq 0$. Otherwise, the row rank is less than $n$, as the first $h+1$ rows are linearly dependent. Hence $\rank A < n$.
	\item We swap columns $h + 1$ and $j$, so $\lambda = a_{h+1,h+1} \neq 0$, and then divide by $\lambda$.
	\item Finally, we can use the $1$ in $a_{h+1, h+1}$ to clear out the rest of the $(h+1)$'st row.
\end{itemize}

\end{adjustbox}

This gives $A E_1 \ldots E_c = I_n$, or $A^{-1} = E_1 \ldots E_c$. This is an algorithm for computing $A^{-1}$.

\begin{proposition}
	Any invertible square matrix is a product of elementary matrices.
\end{proposition}

\newpage

\section{Dual Spaces and Dual Maps}%
\label{sec:dual_spaces_and_dual_maps}

\begin{definition}
	$V$ is a $F$-vector space. We say $V^{\ast}$ is the dual of $V$ if
	\[
		V^{\ast} = L(V, F) = \{ \alpha : V \to F \text{ linear}\}
	.\]\index{dual space}
	If $\alpha : V \to F$ is linear, then we say $\alpha$ is a linear form.\index{linear form}
\end{definition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\begin{example}
	\begin{enumerate}[(i)]
		\item[]
		\item $\tr : M_{n, n}(F) \to F$ is a linear map, so $\tr \in M_{n,n}^{\ast}(F)$.
		\item Let $f : [0, 1] \to \mathbb{R}$ by $x \mapsto f(x)$, and $Tf : \mathcal{C}^{\infty}([0,1], \mathbb{R}) \to \mathbb{R}$ by
			\[
				\phi \mapsto \int_{0}^{1} f(x)\phi(x)\diff x
			.\]
			Then $Tf$ is a linear form.
	\end{enumerate}
\end{example}
\end{adjustbox}

\begin{lemma}
	Let $V$ be a vector space over $F$ with a finite basis $\mathcal{B} = \{e_1, \ldots, e_n\}$. Then there exists a basis for $V^{\ast}$ given by $\mathcal{B}^{\ast} = \{\varepsilon_1, \ldots, \varepsilon_n\}$, with
	\[
		\varepsilon_j \Biggl( \sum_{i = 1}^{n} a_i e_i \Biggr) = a_j
	.\]
	Then $\mathcal{B}^{\ast}$ is the dual basis of $\mathcal{B}$.\index{dual basis}
\end{lemma}

\begin{remark}
	If we define the Kronecker symbols
	\[
	\delta_{ij} =
	\begin{cases}
		1 & i = j, \\
		0 & \text{otherwise},
	\end{cases}
	\]
	then we can equivalently define
	\[
		\varepsilon_j \Biggl( \sum_{i = 1}^{n} a_i e_i \Biggr) = a_j \iff \varepsilon_j(e_i) = \delta_{ij}
	.\]
\end{remark}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} Let $(\varepsilon_1, \ldots, \varepsilon_n)$ be defined as above.

	We prove $(\varepsilon_i)$ are free. Indeed, suppose
	\[
		\sum_{j = 1}^{n} \lambda_j \varepsilon_j = 0 \implies \sum_{j = 1}^{n} \lambda_j e_j(e_i) = 0 \implies \lambda_i = 0
	.\]
\end{adjustbox}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	Now we show $(\varepsilon_i)$ generates $V^{\ast}$. Pick $\alpha \in V^{\ast}$, then for $x \in V$, we have
	\[
		\alpha(x) = \alpha \Biggl( \sum_{j = 1}^{n} \lambda_j e_j \Biggr) = \sum_{j = 1}^{n} \lambda_j \alpha(e_j)
	.\]
	On the other hand, consider the linear form
	\[
		\sum_{j = 1}^{n} \alpha(e_j) \varepsilon_j \in V^{\ast}
	.\]
	Then we have
	\begin{align*}
		\sum_{j = 1}^{n} \alpha(e_j) \varepsilon_j(x) &= \sum_{j = 1}^{n} \alpha(e_j) \varepsilon_j\Biggl( \sum_{k = 1}^{n} \lambda_k e_k \Biggr) = \sum_{j = 1}^{n} \alpha(e_j) \sum_{k = 1}^{n} \lambda_k \varepsilon_j (e_k) \\
							      &= \sum_{j = 1}^{n} \alpha(e_j) \lambda_j = \alpha(x).
	\end{align*}
	Hence $(\varepsilon_i)$ generates $V^{\ast}$.
\end{adjustbox}


\begin{corollary}
	If $V$ is finite dimensional, then $\dim V^{\ast} = \dim V$.
\end{corollary}

This is very different in infinite dimensions.

\begin{remark}
	It is sometimes convenient to think of $V^{\ast}$ as the space of row vector of length $n$ over $F$. If $(e_1, \ldots, e_n)$ is a basis of $v$ such that $x = \sum x_i e_i$ and $(\varepsilon_1, \ldots, \varepsilon_n)$ is a basis of $V^{\ast}$ such that $\alpha = \sum \alpha_i \varepsilon_i$, then
	\begin{align*}
		\alpha(x) &= \sum_{i = 1}^{n} \alpha_i \varepsilon_i \Biggl( \sum_{j = 1}^{n} x_j e_j \Biggr) = \sum_{i = 1}^{n} \alpha_i \sum_{j = 1}^{n} x_j \varepsilon_i(e_j) = \sum_{i = 1}^{n} \alpha_i x_i \\
			  &= 
			  \begin{pmatrix}
				  \alpha_1 & \cdots & \alpha_n
			  \end{pmatrix}
			  \begin{pmatrix}
			  	x_1 \\
				\vdots \\
				x_n
			  \end{pmatrix}.
	\end{align*}
	This gives a scalar product structure on $V^{\ast}$.
\end{remark}

\begin{definition}
	If $U \leq V$, we define the annihilator\index{annihilator} of $U$ by
	\[
		U^{\circ} = \{\alpha \in V^{\ast} \mid \alpha(u) = 0 \; \forall u \in U\}
	.\]
\end{definition}
\newpage

\begin{lemma}
	\begin{enumerate}[\normalfont(i)]
		\item $U^{\circ} \le V^{\ast}$.
		\item If $U \leq V$ and $\dim V < \infty$, then $\dim V = \dim U + \dim U^{\circ}$.
	\end{enumerate}
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} Suppose $\alpha, \alpha' \in U^{\circ}$. Then for all $u \in U$,
	\[
		(\alpha + \alpha')(u) = \alpha(u) + \alpha'(u) = 0
	,\]
	and for all $\lambda \in F$, $(\lambda \alpha)(u) = \lambda \alpha(u) = 0$. Hence $U^{\circ} \le V^{\ast}$.

	Now let $U \leq V$, and $\dim V = n$. Let $(e_1, \ldots, e_k)$ be a basis of $U$ and complete it to a basis $\mathcal{B} = (e_1, \ldots, e_k, e_{k+1}, \ldots, e_n)$ of $V$. Let $(\varepsilon_1, \ldots, \varepsilon_n)$ be the dual basis of $\mathcal{B}$. Then I claim $U^{\circ} = \langle \varepsilon_{k+1}, \ldots, \varepsilon_n\rangle$.

	Indeed, pick $i > k$, then $\varepsilon_i(e_k) = \delta_{ik} = 0$, so $\varepsilon_i \in U^{\circ}$. Now let $\alpha \in U^{\circ}$. Then $(\varepsilon_1, \ldots, \varepsilon_n)$ is a basis of $V^{\ast}$ implies $\alpha = \sum \alpha_i \varepsilon_i$. But $\alpha \in U^{\circ} \implies \alpha(e_i) = 0$, which gives $\alpha_i = 0$ for $i \leq k$. Hence $\alpha \in \langle \varepsilon_{k+1}, \ldots, \varepsilon_n\rangle$.
\end{adjustbox}

\begin{definition}
	Let $V, W$ be vector spaces over $F$, and let $\alpha \in L(V, W)$. Then the map
	\begin{align*}
		\alpha^{\ast} : W^{\ast} &\to V^{\ast} \\
		\varepsilon &\mapsto \varepsilon \circ \alpha
	\end{align*}
	is an element of $L(W^{\ast}, V^{\ast}$. This is known as the dual map of $\alpha$.\index{dual map}
\end{definition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} $\varepsilon \circ \alpha : V \to F$ is linear due to the linearity of $\varepsilon$ and $\alpha$. Hence $\varepsilon \circ \alpha \in V^{\ast}$.

We show $\alpha^{\ast}$ is linear. Let $\theta_1, \theta_2 \in W^{\ast}$. Then,
\[
	\alpha^{\ast}(\theta_1 + \theta_2) = (\theta_1 + \theta_2)(\alpha) = \theta_1 \circ \alpha + \theta_2 \circ \alpha = \alpha^{\ast} (\theta_1) + \alpha^{\ast}(\theta_2)
.\]
Similarly, if $\lambda \in F$, then
\[
	\alpha^{\ast}(\lambda \theta) = \lambda \alpha^{\ast}(\theta)
.\]
Hence $\alpha^{\ast} \in L(W^{\ast}, V^{\ast})$.
\end{adjustbox}

\begin{proposition}
	Let $V, W$ be finite dimensional spaces over $F$ with bases $\mathcal{B}, \mathcal{C}$. Let $\mathcal{B}^{\ast}, \mathcal{C}^{\ast}$ be the dual bases for $V^{\ast}, W^{\ast}$. Then
	\[
		[\alpha^{\ast}]_{\mathcal{C}^{\ast}, \mathcal{B}^{\ast}} = [\alpha]_{\mathcal{B},\mathcal{C}}^{T}
	.\]
\end{proposition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} Let $\mathcal{B} = (b_1, \ldots, b_n), \mathcal{C} = (c_1, \ldots, c_m)$, $\mathcal{B}^{\ast} = (\beta_1, \ldots, \beta_n), \mathcal{C}^{\ast} = (\gamma_1, \ldots, \gamma_m)$. Say $[\alpha]_{\mathcal{B}, \mathcal{C}} = A = (a_{ij})$. Recall $\alpha^{\ast} : W^{\ast} \to V^{\ast}$, so let us compute
	\begin{align*}
		\alpha^{\ast}(\gamma_r)(b_s) = \gamma_r \circ \alpha(b_s) = \gamma_r \Biggl( \sum_{t} a_{ts} c_t \Biggr) = \sum_{t} a_{ts} \gamma_r(c_t) = a_{rs}.
	\end{align*}
	Say that
	\[
		[\alpha^{\ast}]_{\mathcal{C}^{\ast}, \mathcal{B}^{\ast}} = 
		\begin{pmatrix}
			\alpha^{\ast}(\gamma_1) & \cdots & \alpha^{\ast}(\gamma_m)
		\end{pmatrix}
		= (m_{ij})
	.\]
	Then we can find that
	\[
		\alpha^{\ast}(\gamma_r) = \sum_{i = 1}^{n} m_{ir}\beta_i
	,\]
	so
	\[
		\alpha^{\ast}(\gamma_r)(b_s) = m_{sr}
	.\]
	This gives $a_{rs} = m_{sr}$, as desired.
\end{adjustbox}

\newpage

\section{Properties of the Dual Map}%
\label{sec:properties_of_the_dual_map}

Recall if $V, W$ are vector spaces over $F$, and $\alpha \in L(V, W)$, then we can construct a dual map
\begin{align*}
	\alpha^{\ast} : W^{\ast} &\to V^{\ast} \\
	\varepsilon &\mapsto \varepsilon \circ \alpha
\end{align*}
Moreover, if $\mathcal{B}, \mathcal{C}$ are bases of $V$ and $W$, and $\mathcal{B}^{\ast}$, $\mathcal{C}^{\ast}$ are the dual bases of $\mathcal{B}$ and $\mathcal{C}$ respectively, then
\[
	[\alpha^{\ast}]_{\mathcal{C}^{\ast}, \mathcal{B}^{\ast}} = [\alpha]_{\mathcal{B}, \mathcal{C}}^{T}
.\]
Now if $\mathcal{E} = (e_1, \ldots, e_n)$ is a basis of $V$ and $\mathcal{F} = (f_1, \ldots, f_n)$ is another basis of $V$, then consider the change of basis matrix
\[
	P = [\id]_{\mathcal{F}, \mathcal{E}}
.\]
Consider $\mathcal{E}^{\ast} = (\varepsilon_1, \ldots, \varepsilon_n)$ and $\mathcal{F}^{\ast} = (\eta_1, \ldots, \eta_n)$.

\begin{lemma}
	The change of basis matrix from $\mathcal{F}^{\ast}$ to $\mathcal{E}^{\ast}$ is
	\[
		(P^{-1})^{T}
	.\]
\end{lemma}
\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} We have
\[
	[\id]_{\mathcal{F}^{\ast}, \mathcal{E}^{\ast}} = [\id]_{\mathcal{E}, \mathcal{F}}^{T} = ([\id]_{\mathcal{F}, \mathcal{E}}^{-1})^{T}
.\]
\end{adjustbox}

\subsection{Properties of the Dual Map}%
\label{sub:properties_of_the_dual_map}

\begin{lemma}
	Let $V, W $ be vector spaces over $F$. Let $\alpha \in L(V, W)$ and $\alpha^{\ast} \in L(W^{\ast}, V^{\ast})$. Then
	\begin{enumerate}[\normalfont(i)]
		\item $\Ker(\alpha^{\ast}) = (\Img \alpha)^{\circ}$. Hence $\alpha^{\ast}$ is injective if and only if $\alpha$ is surjective.
		\item $\Img \alpha^{\ast} \leq (\Ker \alpha)^{\circ}$ with equality if $V, W$ are finite dimensional. Hence in this case, $\alpha^{\ast}$ is injective if and only if $\alpha$ is injective.
	\end{enumerate}
\end{lemma}

There are many problems where the understanding of $\alpha^{\ast}$ is simpler than the understanding of $\alpha$.

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:}
\begin{enumerate}[(i)]
	\item Let $\varepsilon \in W^{\ast}$. Then $\varepsilon \in \Ker \alpha^{\ast} \iff \alpha^{\ast} (\varepsilon) = 0$. But $\alpha^{\ast}(\varepsilon) = \varepsilon(\alpha)$, so for all $x$,
			\[
				\varepsilon(\alpha)(x) = \varepsilon(\alpha(x)) = 0
			.\]
			This holds if and only if $\varepsilon \in (\Img \alpha)^{\circ}$.
		\item We will first show that
			 \[
				 \Img \alpha^{\ast} \leq (\Ker \alpha)^{\circ}
			.\]
			Indeed, if $\varepsilon \in \Img \alpha^{\ast}$, then $\varepsilon = \alpha^{\ast}(\phi)$, so for all $u \in \Ker \alpha$,
			\[
				\varepsilon(u) = \alpha^{\ast}(\phi) (u) = \phi \circ \alpha(u) = \phi(0) = 0
			.\]
			Hence $\varepsilon \in (\Ker \alpha)^{\circ}$. In finite dimension, we can compare the dimension of $\Img \alpha^{\ast}$ and $(\Ker \alpha)^{\circ}$. Indeed,
			\[
				\dim (\Img \alpha^{\ast}) = r(\alpha^{\ast}) = r([\alpha^{\ast}]_{\mathcal{C}^{\ast}, \mathcal{B}^{\ast}}) = r([\alpha]_{\mathcal{B},\mathcal{C}}^{T}) = r([\alpha]_{\mathcal{B}, \mathcal{C}}) = r(\alpha)
			.\]
			Hence, we get
			\[
				\dim (\Img \alpha^{\ast}) = r(\alpha^{\ast}) = r(\alpha) = \dim V - \dim \Ker \alpha = \dim [(\Ker \alpha)^{\circ}]
			.\]
			Since the dimensions are the same, we get $\Img \alpha^{\ast} = (\Ker \alpha)^{\circ}$.
\end{enumerate}
\end{adjustbox}

\subsection{Double Dual}%
\label{sub:double_dual}

If $V$ is a vector space over $F$, then $V^{\ast} = L(V, F)$.

We define the \textbf{bidual}\index{bidual}\index{double dual} as
\[
	V^{\ast \ast} = (V^{\ast})^{\ast} = L(V^{\ast}, F)
.\]
This is a very important space in infinite dimension. In general, there is no obvious connection between $V$ and $V^{\ast}$. However, there is a large class of function spaces such that
\[
V \cong V^{\ast \ast}
.\]
This is known as a reflexive space\index{reflexive space}.

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\begin{example}
	For $p > 2$, define
	\[
		L^{p}(\mathbb{R}) = \biggl\{ f : \mathbb{R} \to \mathbb{R} \,\bigg\vert\, \int_{\mathbb{R}} |f(x)|^{p} \diff x < \infty \biggr\}
	.\]
	This is an example of a reflexive space.
\end{example}
\end{adjustbox}

In general, there is a canonical embedding of $V$ into $V^{\ast \ast}$. Indeed, pick $v \in V$. We define
\begin{align*}
	\hat v : V^{\ast} &\to F \\
	\varepsilon &\mapsto \varepsilon(v)
\end{align*}
Then this is linear, as
\[
	\hat v(\lambda_1 \varepsilon_1 + \lambda_2 \varepsilon_2) = (\lambda_1 \varepsilon_1 + \lambda_2 + \varepsilon_2)(v) = \lambda_1 \varepsilon_1(v) + \lambda_2 \varepsilon_2(v) = \lambda_1 \hat v(\varepsilon_1) + \lambda_2 \hat v(\varepsilon_2)
.\]

\begin{theorem}
	If $V$ is a finite dimensional vector space over $F$, then the hat map $v \mapsto \hat v$ is an isomorphism.
\end{theorem}

In infinite dimension, under certain assumption (e.g. Banach space) we can show that the hat map is injective.

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} If $V$ is finite dimensional, then first note that for $v \in V$, $\hat v \in V^{\ast \ast}$. We show the hat map is linear: for $v_1, v_2 \in V$, $\lambda_1, \lambda_2 \in F$ and $\varepsilon \in V^{\ast}$,
	\[
		\widehat{\lambda_1 v_1 + \lambda_2 v_2} (\varepsilon) = \varepsilon(\lambda_1 v_1 + \lambda_2 v_2) = \lambda_1 \varepsilon(v_1) + \lambda_1 \varepsilon_2(v_2) = \lambda_1 \hat v_1(\varepsilon) + \lambda_2 \hat v_2 (\varepsilon)
	.\]
	Now we show the hat map is injective. Let $e \in V \setminus \{0\}$. Then extend to a basis $(e, e_2, \ldots, e_n)$. Let $(\varepsilon, \varepsilon_2, \ldots, \varepsilon_n)$ be the dual basis. Then
	\[
		\hat e (\varepsilon) = \varepsilon(e) = 1
	.\]
	Hence $\hat e \neq \{0\}$, so the hat map is injective.

	Finally, we show the hat map is an isomorphism. We already know $\dim V = \dim V^{\ast}$, and as a result $\dim V^{\ast} = \dim V^{\ast \ast}$. Thus, since the hat map is injective, it is an isomorphism.
\end{adjustbox}

\begin{lemma}
	Let $V$ be a finite dimensional vector space over $K$, and let $U \leq V$. Then
	\[
	\hat U = U^{\circ \circ}
	.\]
	Hence after identification of $V$ and $V^{\ast \ast}$, we get
	\[
	U = U^{\circ \circ}
	.\]
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} We will show $U \leq U^{\circ \circ}$. Indeed, let $u \in U$. Then for all $\varepsilon \in U^{\circ}$, $\varepsilon(u) = 0$. So for all $\varepsilon \in U^{\circ}$, $\hat u(\varepsilon) = \varepsilon(u) = 0$. Hence $\hat u \in U^{\circ \circ}$, so $\hat U \subset U^{\circ \circ}$.

	But then we can compute dimension to find
	\[
	\dim U^{\circ \circ} = \dim V - \dim U^{\circ} = \dim U
	,\]
	proving this lemma.
\end{adjustbox}

\begin{remark}
	If $T \leq V^{\ast}$, then
	\[
		T^{\circ} = \{v \in V \mid \theta(v) = 0, \, \forall \theta \in T\}
	.\]
\end{remark}

\begin{lemma}
	Let $V$ be a finite dimensional vector space over $K$. Let $U_1, U_2 \leq V$. Then,
	\begin{enumerate}[\normalfont(i)]
		\item $(U_1 + U_2)^{\circ} = U_1^{\circ} \cap U_2^{\circ}$,
		\item $(U_1 \cap U_2)^{\circ} = U_1^{\circ} + U_2^{\circ}$.
	\end{enumerate}
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:}
\begin{enumerate}[(i)]
	\item Let $\theta \in V^{\ast}$, then
		\begin{align*}
			\theta \in (U_1 + U_2)^{\circ} &\iff \theta(u_1 + u_2) = 0 \iff \theta(u) = 0 \, \forall u \in U_1 \cup U_2 \\
						       &\iff \theta \in U_1^{\circ} \cap U_2^{\circ}.
		\end{align*}
		Hence $(U_1 + U_2)^{\circ} = U_1^{\circ} \cap U_2^{\circ}$.
	\item Looking at (i), we can take the annihilator of everything to get
		\[
			(U_1 \cap U_2)^{\circ} = (U_1^{\circ} + U_2^{\circ})^{\circ \circ} = U_1^{\circ} + U_2^{\circ}
		.\]
\end{enumerate}
\end{adjustbox}

\newpage

\section{Bilinear Forms}%
\label{sec:bilinear_forms}

\begin{definition}
	Let $U, V$ be vector spaces over $K$. Then
	\[
	\phi : U \times V \to K
	\]
	is a \textbf{bilinear form}\index{bilinear form} if it is linear in both components.
\end{definition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\begin{example}
	\begin{enumerate}[(i)]
		\item[]
		\item Take $V \times V^{\ast} \to K$ by $(v, \theta) \mapsto \theta(v)$.
		\item The scalar product\index{scalar product} on $U = V = \mathbb{R}^{n}$ is $\psi : \mathbb{R}^{n} \times \mathbb{R}^{n} \to \mathbb{R}$ by
			\[
				\left(
					\begin{pmatrix}
						x_1 \\
						\vdots \\
						x_n
					\end{pmatrix},
					\begin{pmatrix}
						y_1 \\
						\vdots \\
						y_n
					\end{pmatrix}
				\right)
				\mapsto  \sum_{i = 1}^{n} x_i y_i
			.\]
		\item If $U = V = \mathcal{C}([0, 1], \mathbb{R})$, then we can define
			\[
				\phi(f, g) = \int_{0}^{1}f(t)g(t) \diff t
			.\]
			This can be thought of as an infinite dimensional scalar product.
	\end{enumerate}
\end{example}
\end{adjustbox}

\begin{definition}
	Let $\mathcal{B} = (e_1, \ldots, e_m)$ be a basis of $U$, and $\mathcal{C} = (f_1, \ldots, f_n)$ be a basis of $V$. If $\phi : U \times V \to F$ is a bilinear form, then the matrix of $\phi$ with respect to $\mathcal{B}$ and $\mathcal{C}$\index{matrix of bilinear form} is
	\[
		[\phi]_{\mathcal{B}, \mathcal{C}} = (\phi(e_i, f_j))
	.\]
\end{definition}

\begin{lemma}
	\[
		\phi(u, v) = [u]_{\mathcal{B}}^{T} [\phi]_{\mathcal{B}, \mathcal{C}} [v]_{\mathcal{C}}
	.\]
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} Let
\[
u = \sum_{i = 1}^{m} \lambda_i e_i, \quad v = \sum_{j = 1}^{n} \mu_ij f_j
.\]
Since $\phi$ is a bilinear form,
\begin{align*}
	\phi(u, v) &= \phi \Biggl( \sum_{i = 1}^{m} \lambda_i e_i, \sum_{j = 1}^{n} \mu_j e_j \Biggr) = \sum_{i = 1}^{m}\sum_{j = 1}^{n} \lambda_i \mu_j \phi(e_i, f_j) \\
		   &= [u]_{\mathcal{B}}^{T}[\phi]_{\mathcal{B}, \mathcal{C}}[v]_{\mathcal{C}}.
\end{align*}
\end{adjustbox}

\begin{remark}
	$[\phi]_{\mathcal{B},\mathcal{C}}$ is the only matrix satisfying this property.
\end{remark}

\begin{definition}
	$\phi: U \times V \to K$ a bilinear form determines two linear maps:
	\begin{align*}
		\phi_L : U \to V^{\ast}& \\
		\phi_L(u) : V &\to K \\
			      v &\mapsto \phi(u, v)\\
			      \phi_R : V \to U^{\ast}& \\
			      \phi_R(v) : U &\to K \\
			      u &\mapsto \phi(u, v)
	\end{align*}
\end{definition}

\begin{lemma}
	Let $\mathcal{B} = (e_1, \ldots, e_m)$ a basis of $U$, and $\mathcal{B}^{\ast} = (\varepsilon_1, \ldots, \varepsilon_m)$ a dual basis of $U^{\ast}$, Similarly, let $\mathcal{C} = (f_1, \ldots, f_n)$ be a basis of $V$, and $\mathcal{C}^{\ast} (\eta_1, \ldots, \eta_n)$ a dual basis of $V^{\ast}$.

	Let $A = [\phi]_{\mathcal{B}, \mathcal{C}}$. Then,
	\begin{align*}
		[\phi_R]_{\mathcal{C}, \mathcal{B}^{\ast}} &= A, \\
		[\phi_L]_{\mathcal{B}, \mathcal{C}^{\ast}} &= A^{T}.
	\end{align*}
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} We have $\phi_L(e_i, f_j) = \phi(e_i, f_j) = A_{ij}$, and so
	\[
		\phi_L(e_i) = \sum A_{ij} \eta_j
	.\]
	Similarly, $\phi_R(f_j)(e_i) = \phi(e_i, f_j) = A_{ij}$, so
	\[
		\phi_R(f_{j}) = \sum A_{ij} \varepsilon_i
	.\]
	This naturally gives our result.
\end{adjustbox}

\begin{definition}
	Let $\Ker \phi_L$ be the left kernel of $\phi$, and $\Ker \phi_R$ be the right kernel of $\phi$.

	We say that $\phi$ is non-degenerate\index{non-degenerate bilinear form} if $\Ker \phi_L = \{0\}$ and $\Ker \phi_R = \{0\}$. Otherwise, we say that $\phi$ is degenerate.
\end{definition}

\begin{lemma}
	Let $U, V$ be finite dimensional, $\mathcal{B}, \mathcal{C}$ bases of $U$ and $V$, and $\phi: U \times V \to K$ a bilinear form. Let $A = [\phi]_{\mathcal{B}, \mathcal{C}}$.

	Then $\phi$ is non-degenerate if and only if $A$ is invertible.
\end{lemma}

\begin{corollary}
	If $\phi$ is non-degenerate, then $\dim U = \dim V$.
\end{corollary}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
	\textbf{Proof:} $\phi$ is non-degenerate if and only if $\Ker \phi_L = \{0\}$ and $\Ker \phi_R = \{0\}$. But this implies $\nullity (A^{T}) = 0$ and $\nullity (A) = 0$, hence by rank-nullity theorem, we must have $\rank (A^{T}) = \dim U$, and $\rank (A) = \dim V$. But this gives $A$ invertible and $\dim U = \dim V$.
\end{adjustbox}

\begin{remark}
	Taking $\phi : \mathbb{R}^{n} \times \mathbb{R}^{n} \to \mathbb{R}$ by the scalar product, then $\phi$ is non-degenerate, as in the standard basis $\mathcal{B}$,
	\[
		[\phi]_{\mathcal{B}, \mathcal{B}} = I_n
	.\]
\end{remark}

\begin{corollary}
	When $U$ and $V$ are finite dimensional, then choosing a non-degenerate bilinear form $\phi: U \times V \to K$ is equivalent to choosing an isomorphism $\phi_L : U \to V^{\ast}$.
\end{corollary}

\begin{definition}
	If $T \subset U$, we define
	\[
		T^{\perp} = \{v \in V \mid \phi(t, v) = 0 \, \forall t \in T\}
	.\]
	Similarly, if $S \subset V$, then
	\[
		^{\perp}S = \{u \in U \mid \phi(u, s) = 0 \, \forall s \in S\}
	.\]
\end{definition}

\begin{proposition}
	Let $\mathcal{B}, \mathcal{B}'$ be two bases of $U$, and $P = [\id]_{\mathcal{B}', \mathcal{B}}$, and $\mathcal{C}, \mathcal{C}'$ two bases of $V$, and $Q = [\id]_{\mathcal{C}', \mathcal{C}}$, then if $\phi: U \times V \to K$ is a bilinear form, then
	\[
		[\phi]_{\mathcal{B}', \mathcal{C}'} = P^{T} [\phi]_{\mathcal{B}, \mathcal{C}} Q
	.\]
\end{proposition}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:} We have
\[
	\phi(u, v) = [u]_{\mathcal{B}}^{T} [\phi]_{\mathcal{B}, \mathcal{C}}[v]_{\mathcal{C}} = (P[u]_{\mathcal{B}'})^{T}[\phi]_{\mathcal{B},\mathcal{C}}(Q[v]_{\mathcal{C}'}) = [u]_{\mathcal{B}'}^{T} (P^{T} [\phi]_{\mathcal{B}, \mathcal{C}} Q)[v]_{\mathcal{C}'}
,\]
which implies $P^{T}[\phi]_{\mathcal{B}, \mathcal{C}}Q = [\phi]_{\mathcal{B}', \mathcal{C}'}$.
\end{adjustbox}

\begin{definition}
	The rank of $\phi$ ($\rank \phi$) is the rank of any matrix representing $\phi$.\index{rank of bilinear form}
\end{definition}

This is true as $\rank (P^{T}AQ) = \rank A$, if $P$ and $Q$ are invertible.

Note we could have equivalently defined $\rank \phi = \rank \phi_L = \rank \phi_R$.

\newpage

\section{Determinant and Traces}%
\label{sec:determinant_and_traces}

\begin{definition}
	If $A \in M_{n}(K)$, we define the trace\index{trace} of $A$ as
	\[
	\tr A = \sum_{i = 1}^{n} A_{ii}
	.\]
\end{definition}

\begin{remark}
	The map $M_n(K) \to K$ by $A \mapsto \tr A$ is linear.
\end{remark}

\begin{lemma}
	$\tr (AB) = \tr (BA)$.
\end{lemma}

\begin{adjustbox}{minipage = \columnwidth - 25.5pt, margin=1em, frame=1pt, margin=0em}
\textbf{Proof:}
\[
	\tr (AB) = \sum_{i = 1}^{n} \Biggl( \sum_{j = 1}^{n} a_{ij} b_{ji} \Biggr) = \sum_{j = 1}^{n} \sum_{i = 1}^{n} b_{ji} a_{ij} = \tr (BA)
.\]
\end{adjustbox}


\newpage

\printindex

\end{document}
